{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_augs in updated_args['repeats']:\n",
    "    for mn, model_name_str in enumerate(models_to_run):\n",
    "        if model_name_str in skip_models:\n",
    "            continue\n",
    "\n",
    "        day_wer_dict, total_wer_dict = {}, {}\n",
    "\n",
    "        for seed in seeds_list:\n",
    "            if seed in skip_seeds:\n",
    "                continue\n",
    "\n",
    "            print(f\"Running model: {model_name_str}_seed_{seed}\")\n",
    "            day_wer_dict[seed] = []\n",
    "\n",
    "            modelPath = f\"{model_storage_path}{model_name_str}_seed_{seed}\"\n",
    "            output_file = (\n",
    "                f\"{shared_output_file}_seed_{seed}\"\n",
    "                if shared_output_file\n",
    "                else f\"{model_name_str}_seed_{seed}\"\n",
    "            )\n",
    "\n",
    "            # Load args\n",
    "            with open(f\"{modelPath}/args\", \"rb\") as handle:\n",
    "                args = pickle.load(handle)\n",
    " \n",
    "                model = BiT_Phoneme(\n",
    "                    patch_size=args['patch_size'], dim=args['dim'], dim_head=args['dim_head'],\n",
    "                    nClasses=args['nClasses'], depth=args['depth'], heads=args['heads'],\n",
    "                    mlp_dim_ratio=args['mlp_dim_ratio'], dropout=updated_args['dropout'], input_dropout=updated_args['input_dropout'],\n",
    "                    gaussianSmoothWidth=args['gaussianSmoothWidth'],\n",
    "                    T5_style_pos=args['T5_style_pos'], max_mask_pct=updated_args['max_mask_pct'],\n",
    "                    num_masks=updated_args['num_masks'], mask_token_zeros=args['mask_token_zero'], max_mask_channels=0,\n",
    "                    num_masks_channels=0, dist_dict_path=None\n",
    "                ).to(device)\n",
    "\n",
    "            if data_file is None:\n",
    "                data_file = args['datasetPath']\n",
    "\n",
    "            trainLoader, testLoaders, loadedData = getDatasetLoaders(data_file, 64)\n",
    "                    \n",
    "            args.setdefault('mask_token_zero', False)\n",
    "\n",
    "            model.load_state_dict(torch.load(f\"{modelPath}/modelWeights\", map_location=device), strict=True)\n",
    "\n",
    "            if tta_mode != 'baseline':\n",
    "                print(updated_args['learning_rate'][mn])\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=updated_args['learning_rate'][mn], \n",
    "                                            weight_decay=updated_args['l2_decay'],\n",
    "                                                betas=(args['beta1'], args['beta2']))\n",
    "\n",
    "                if updated_args['freeze_linear']:\n",
    "                    for name, p in model.named_parameters():\n",
    "                        p.requires_grad = name in {\n",
    "                            \"dayWeights\", \"dayBias\"\n",
    "                        }\n",
    "                        \n",
    "                if updated_args['freeze_patch']:\n",
    "                    for name, p in model.named_parameters():\n",
    "                        p.requires_grad = name in {\n",
    "                            \"to_patch_embedding.1.weight\", \"to_patch_embedding.1.bias\",\n",
    "                            \"to_patch_embedding.2.weight\", \"to_patch_embedding.2.bias\",\n",
    "                            \"to_patch_embedding.3.weight\", \"to_patch_embedding.3.bias\"\n",
    "                        }\n",
    "\n",
    "            testDayIdxs = np.arange(len(loadedData['test']))\n",
    "            print(len(testDayIdxs))\n",
    "            \n",
    "                \n",
    "            model_outputs = {\"logits\": [], \"logitLengths\": [], \"trueSeqs\": [], \"transcriptions\": []}\n",
    "          \n",
    "            decoded_list_all_days = []\n",
    "            transcripts_all_days = []\n",
    "            \n",
    "            for test_day_idx, testDayIdx in enumerate(testDayIdxs):\n",
    "                \n",
    "                print(\"day \", test_day_idx)\n",
    "            \n",
    "                val_ds = SpeechDataset([loadedData['test'][test_day_idx]], return_transcript=True)\n",
    "                data_loader = get_dataloader(val_ds)                        \n",
    "                transcriptions_list = []\n",
    "                decoded_list = []\n",
    "                \n",
    "                test_day_decoded_sents = []\n",
    "                \n",
    "                for trial_idx, (X, y, X_len, y_len, day_idx, transcript) in enumerate(data_loader):\n",
    "                                              \n",
    "                    total_start = time.time()\n",
    "                    \n",
    "                    X, y, X_len, y_len = map(lambda x: x.to(device), [X, y, X_len, y_len])\n",
    "                    \n",
    "                    if updated_args['max_day'] is not None:\n",
    "                        day_idx = torch.tensor([updated_args['max_day']], dtype=torch.int64).to(device)\n",
    "                    else:\n",
    "                        day_idx = torch.tensor([day_idx],  dtype=torch.int64).to(device)\n",
    "                        \n",
    "                    adjusted_len = model.compute_length(X_len)\n",
    "                    \n",
    "                    # obtain beam search + LM corrected outputs\n",
    "                    # do this before adaptation on that trial to make \n",
    "                    # sure results are compatabile with a streaming system \n",
    "                    torch.cuda.synchronize(device)\n",
    "                    torch.cuda.reset_peak_memory_stats(device)\n",
    "                    \n",
    "                    model.eval()\n",
    "                    logits_eval = model(X, X_len, day_idx)\n",
    "                    decoded, y_pseudo, y_len_pseudo = get_lm_outputs(logits_eval)\n",
    "                    \n",
    "                    if tta_mode != 'baseline':\n",
    "                    \n",
    "                        # generate multiple versions of the same input\n",
    "                        if n_augs > 0:\n",
    "                            \n",
    "                            X = X.repeat(n_augs, 1, 1)\n",
    "                            y = y.repeat(n_augs, 1)\n",
    "                            y_len = y_len.repeat(n_augs)\n",
    "                            X_len = X_len.repeat(n_augs)\n",
    "                            adjusted_len = adjusted_len.repeat(n_augs)\n",
    "                            y_pseudo = y_pseudo.unsqueeze(0).repeat(n_augs, 1).to(device) \n",
    "                            y_len_pseudo = y_len_pseudo.repeat(n_augs).to(device)\n",
    "                            \n",
    "                        \n",
    "                        # add white noise and baseline shift augmentations to each sample\n",
    "                        if updated_args['WN+BS'] == True:\n",
    "                            \n",
    "                            X += torch.randn(X.shape, \n",
    "                                        device=device) * updated_args['white_noise']\n",
    "                        \n",
    "                            X += (\n",
    "                                torch.randn([X.shape[0], 1, X.shape[2]], \n",
    "                                device=device)\n",
    "                                * updated_args['baseline_shift']\n",
    "                            )      \n",
    "                        \n",
    "                        model.train()\n",
    "                        \n",
    "                        for _ in range(updated_args['adaptation_steps']):\n",
    "                    \n",
    "                            logits = model(X, X_len, day_idx)\n",
    "                            \n",
    "                            corp_loss = forward_ctc(logits, adjusted_len, y_pseudo, y_len_pseudo)\n",
    "                            corp_loss_tracker.append(corp_loss.detach().cpu().numpy())\n",
    "                            optimizer.zero_grad()\n",
    "                            corp_loss.backward()\n",
    "                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                            optimizer.step()\n",
    "\n",
    "\n",
    "                    model.eval()\n",
    "                    \n",
    "                    decoded_list.append(decoded)\n",
    "                    transcriptions_list.append(clean_transcription(transcript[0]))\n",
    "                    \n",
    "                to_gib = 1024**3\n",
    "                torch.cuda.synchronize(device)\n",
    "                peak_alloc_gib = torch.cuda.max_memory_allocated(device) / to_gib      # tensors/activations\n",
    "                peak_res_gib  = torch.cuda.max_memory_reserved(device) / to_gib       # allocator footprint\n",
    "\n",
    "                print(f\"PyTorch peak allocated: {peak_alloc_gib:.3f} GiB\")\n",
    "                print(f\"PyTorch peak reserved : {peak_res_gib:.3f} GiB\")\n",
    "                \n",
    "                sys.exit()\n",
    "                        \n",
    "                _, wer = _cer_and_wer(decoded_list, transcriptions_list, outputType=\"speech\", returnCI=False)\n",
    "                print(\"DAY WER: \", wer)\n",
    "                day_wer_dict[seed].append(wer)\n",
    "                \n",
    "                decoded_list_all_days.extend(decoded_list)\n",
    "                transcripts_all_days.extend(transcriptions_list)\n",
    "                \n",
    "                torch.cuda.synchronize(device)\n",
    "\n",
    "                \n",
    "            _, wer_total = _cer_and_wer(decoded_list_all_days, transcripts_all_days, outputType=\"speech\", returnCI=False)\n",
    "            total_wer_dict[seed] = wer_total\n",
    "            print(\"WER ACROSS DAYS: \", wer_total)\n",
    "            \n",
    "        continue\n",
    "        if val_save_file:\n",
    "            \n",
    "            val_save_file_updated = val_save_file.replace(\"dietcorp\", f\"diet{n_augs}corp\")\n",
    "            \n",
    "            print(f\"SAVING VAL RESULTS FOR {model_name_str}\")\n",
    "            with open(f\"{saveFolder_data}{model_name_str}_{val_save_file_updated}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(day_wer_dict, f)\n",
    "            with= open(f\"{saveFolder_data}{model_name_str}_{val_save_file_updated}_all_days.pkl\", \"wb\") as f:\n",
    "                pickle.dump(total_wer_dict, f)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
