{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02167edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pickle1\n",
    "import numpy as np\n",
    "from llm_utils import cer_with_gpt2_decoder, gpt2_lm_decode\n",
    "import pandas as pd\n",
    "from brainaudio.inference.eval_metrics import _cer_and_wer\n",
    "from brainaudio.inference.eval_metrics import clean_string\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f07891b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"b2t_25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f62025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.10.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in 16-bit with automatic device placement...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccdbb48cd5043d18bb54e596e8b9dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is in the hands of the people\n",
      "\n",
      "The future of AI is in the hands of the people\n",
      "\n",
      "The future of AI is in the hands of the people\n",
      "\n",
      "The future of AI is in the hands of the people\n",
      "\n",
      "The future of\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/opt-6.7b\"\n",
    "\n",
    "# Load tokenizer\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if dataset == \"b2t_25\":\n",
    "\n",
    "    print(\"Loading model in 16-bit with automatic device placement...\")\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "elif dataset == \"b2t_24\":\n",
    "    \n",
    "    # Load model in 8-bit with automatic device placement\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "# Example: Generate from a prompt\n",
    "inputs = llm_tokenizer(\"The future of AI is\", return_tensors=\"pt\").to(llm.device)\n",
    "outputs = llm.generate(**inputs, max_new_tokens=50)\n",
    "print(llm_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fba6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest_path = \"/home/ebrahim/data2/brain2text/b2t_25/wfst_outputs/pretrained_RNN/nbest_wfst_rescore_True_acoustic_scale_0.325.pkl\"\n",
    "model_outputs_path = None\n",
    "\n",
    "with open(nbest_path, mode = 'rb') as f:\n",
    "    nbest = pickle.load(f)\n",
    "\n",
    "if dataset == 'b2t_25':\n",
    "    acoustic_scale = 0.325\n",
    "    ground_truth = pd.read_pickle(\"/home/ebrahim/data2/brain2text/b2t_25/transcripts_val_cleaned.pkl\")\n",
    "    llm_weight = 0.55\n",
    "else:\n",
    "    acoustic_scale = 0.5\n",
    "    llm_weight = 0.5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45cb304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest_nejm = pd.read_pickle(\"/home/ebrahim/data2/brain2text/b2t_25/wfst_outputs/t15_pretrained_rnn_baseline_val_nbest_formatted.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b8b409d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('not too controversial', -429.5383605957031, -22.741165161132812),\n",
       " ('not too crucial', -438.0696105957031, -26.61922264099121),\n",
       " ('not to controversial', -429.5383605957031, -30.495746612548828),\n",
       " ('not to crucial', -438.1420254516602, -28.84305947303772),\n",
       " ('not two controversial', -429.5383605957031, -31.660655975341797),\n",
       " ('not two crucial', -438.1420254516602, -29.43133870124817)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbest_nejm[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d93925bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1426"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nbest_nejm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6028b01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07326627845420858\n"
     ]
    }
   ],
   "source": [
    "lm_preds = [clean_string(nbest[i][0][0]) for i in range(len(nbest))]\n",
    "metrics = _cer_and_wer(lm_preds, ground_truth)\n",
    "print(metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6c2b2bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     23\u001b[39m best_hyp_all = []\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nbest_trial \u001b[38;5;129;01min\u001b[39;00m nbest_nejm:\n\u001b[32m     27\u001b[39m     best_hyp = gpt2_lm_decode(\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[43mllm\u001b[49m,\n\u001b[32m     29\u001b[39m         llm_tokenizer,\n\u001b[32m     30\u001b[39m         nbest_trial,\n\u001b[32m     31\u001b[39m         acoustic_scale,\n\u001b[32m     32\u001b[39m         lengthPenlaty=\u001b[32m0\u001b[39m,\n\u001b[32m     33\u001b[39m         alpha=llm_weight,\n\u001b[32m     34\u001b[39m         returnConfidence=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     37\u001b[39m     best_hyp_all.append(best_hyp)\n",
      "\u001b[31mNameError\u001b[39m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "if model_outputs_path is not None:\n",
    "    model_outputs = np.load(model_outputs_path, allow_pickle=True) \n",
    "    \n",
    "    for i in range(len(model_outputs['transcriptions'])):\n",
    "        new_trans = [ord(c) for c in model_outputs['transcriptions'][i]] + [0]\n",
    "        model_outputs['transcriptions'][i] = np.array(new_trans)\n",
    "        \n",
    "    # Rescore nbest outputs with LLM\n",
    "    llm_out = cer_with_gpt2_decoder(\n",
    "        llm,\n",
    "        llm_tokenizer,\n",
    "        nbest[:],\n",
    "        acoustic_scale,\n",
    "        model_outputs,\n",
    "        outputType=\"speech_sil\",\n",
    "        returnCI=True,\n",
    "        lengthPenalty=0,\n",
    "        alpha=llm_weight,\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    \n",
    "    best_hyp_all = []\n",
    "    \n",
    "    for nbest_trial in nbest_nejm:\n",
    "        \n",
    "        best_hyp = gpt2_lm_decode(\n",
    "            llm,\n",
    "            llm_tokenizer,\n",
    "            nbest_trial,\n",
    "            acoustic_scale,\n",
    "            lengthPenlaty=0,\n",
    "            alpha=llm_weight,\n",
    "            returnConfidence=False\n",
    "        )\n",
    "        \n",
    "        best_hyp_all.append(best_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12f72f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyp_all_cleaned = [clean_string(hyp) for hyp in best_hyp_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b07baaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06500794070937004\n"
     ]
    }
   ],
   "source": [
    "metrics = _cer_and_wer(best_hyp_all_cleaned, ground_truth)\n",
    "print(metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6d1f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd                                                                                                                                                                                                 \n",
    "                \n",
    "df = pd.DataFrame({                                                                                                                                                                                                 \n",
    "    'id': range(len(best_hyp_all_cleaned)),\n",
    "    'text': best_hyp_all_cleaned\n",
    "})\n",
    "\n",
    "df.to_csv('/home/ebrahim/brainaudio/results/best_hyp_all_cleaned_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae74e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
