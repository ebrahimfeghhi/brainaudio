{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02167edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pickle\n",
    "import numpy as np\n",
    "from llm_utils import cer_with_gpt2_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f62025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.10.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77cf908ae914b70a219cc012e79305b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is in the hands of the people\n",
      "\n",
      "The future of AI is in the hands of the people\n",
      "\n",
      "The future of AI is in the hands of the people\n",
      "\n",
      "The future of AI is in the hands of the people\n",
      "\n",
      "The future of\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/opt-6.7b\"\n",
    "\n",
    "# Load tokenizer\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model in 8-bit with automatic device placement\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Example: Generate from a prompt\n",
    "inputs = llm_tokenizer(\"The future of AI is\", return_tensors=\"pt\").to(llm.device)\n",
    "outputs = llm.generate(**inputs, max_new_tokens=50)\n",
    "print(llm_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12455693",
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_scale = 0.5\n",
    "llm_weight = 0.5\n",
    "seed_list = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248590b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3dc4fdf8474ecfbddf1067daee1d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbest_path = \"/home/ebrahim/data2/brain2text/b2t_24/model_outputs/bidirectional_gru/bidirectional_gru_seed_0_nbest_5gram.pkl\"\n",
    "model_outputs_path = \"/home/ebrahim/data2/brain2text/b2t_24/model_outputs/bidirectional_gru/bidirectional_gru_seed_0_model_outputs.pkl\"\n",
    "\n",
    "with open(model_outputs_path, mode = 'rb') as f:\n",
    "    model_outputs = pickle.load(f)\n",
    "    \n",
    "with open(nbest_path, mode = 'rb') as f:\n",
    "    nbest = pickle.load(f)\n",
    "\n",
    "for i in range(len(model_outputs['transcriptions'])):\n",
    "    new_trans = [ord(c) for c in model_outputs['transcriptions'][i]] + [0]\n",
    "    model_outputs['transcriptions'][i] = np.array(new_trans)\n",
    "    \n",
    "\n",
    "# Rescore nbest outputs with LLM\n",
    "llm_out = cer_with_gpt2_decoder(\n",
    "    llm,\n",
    "    llm_tokenizer,\n",
    "    nbest[:],\n",
    "    acoustic_scale,\n",
    "    model_outputs,\n",
    "    outputType=\"speech_sil\",\n",
    "    returnCI=True,\n",
    "    lengthPenalty=0,\n",
    "    alpha=llm_weight,\n",
    ")\n",
    "\n",
    "\n",
    "with open(\"/home/ebrahim/data2/brain2text/b2t_24/model_outputs/bidirectional_gru/bidirectional_gru_seed_0_llm_outs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(llm_out['decoded_transcripts'])+ \"\\n\")   # one line per LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9514ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b6130b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.15439170758319695),\n",
       " np.float64(0.14027706574962823),\n",
       " np.float64(0.16899384824954927))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out[\"wer\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf87f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
