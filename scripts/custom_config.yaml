# ===================================================================
#                       Shared Configuration
# This section contains parameters common to all models and experiments.
# ===================================================================
outputDir: '/data2/brain2text/b2t_25/outputs/' # where to save trained model
manifest_paths: ["/data2/brain2text/b2t_25/trial_level_data/manifest.json"] # neural dataset
participant_suffixes: ["_25"] # REQUIRED: aligns with manifest_paths (provide one entry per participant)
seeds: [0]
evaluate_wer: True 
evaluate_every_n_epochs: 5
modelName: "fully_chunked_25_trial_38"
modelType: 'transformer'
optimizer: 'AdamW'
learning_rate: 0.001
beta1: 0.9
beta2: 0.999
eps: 1.0e-8
learning_scheduler: 'cosine'
n_epochs: 350
learning_rate_decay_steps: 340 # in epochs
learning_rate_warmup_steps: 10 # in epochs
milestones: [None] 
gamma: None 
lr_scaling_factor: 0.1
batchSize: 64
nClasses: 40 
load_pretrained_model: ''
grad_norm_clip_value: 10

# Early stopping parameters
early_stopping_enabled: True
early_stopping_checkpoint: 100 # epoch at which to check WER threshold
early_stopping_wer_threshold: 0.16 # if WER hasn't reached this by checkpoint, stop training
early_stopping_no_improvement: 15 # number of val checks with no improvement to wait before stopping (val_checks occurs based on evaluate_every_n_epochs)

# time masking aug
num_masks: 20
max_mask_pct: 0.05

# original augs
whiteNoiseSD: 0.2
constantOffsetSD: 0.05
gaussianSmoothWidth: 2.0
smooth_kernel_size: 20
random_cut: 0
use_amp: true

# standard regularization techniques
l2_decay: 1e-5
input_dropout: 0.1
dropout: 0.4

# LM-related parameters
acoustic_scale: 0.6
lm_weight: 2.0
word_score: 0.1
beam_size: 30

# ===================================================================
#                    Model-Specific Configurations
# Each sub-section defines the architecture for a different model.
# ===================================================================
model:
  # --- GRU Model Arguments ---
  gru:
    name: 'GRU'
    year: '2024'
    nInputFeatures: 256
    nClasses: 40
    nUnits: 1024
    nLayers: 5
    bidirectional: True
    strideLen: 4
    kernelLen: 4
    nDays: 24

  # --- Transformer Model Arguments ---
  transformer:
    name: 'Transformer'
    features_list: [512] # For multiple dataset, order the feature list aligning with corresponding dataset
    samples_per_patch: 4 # 100ms
    n_heads: 6
    mlp_dim_ratio: 4 
    dim_head: 64
    depth: 5
    chunked_attention:
      chunkwise_prob: 1.0 # 1.0 = always chunk, 0.0 = never chunk
      # chunk size in terms of how many patches. 
      chunk_size_min: 1 # 100 ms
      chunk_size_max: 30 # 2.4 sec
      left_constrain_prob: 0.75 # 1.0 = always restrict left context, 0.0 = never restrict
      context_sec_min: 5 # 5 seconds of left context
      context_sec_max: 20 # 10 seconds of left context
      timestep_duration_sec: 0.08 # length of each patch (in sec)
      eval:
        chunk_size: 5 # 400 ms of look ahead. 
        context_chunks: 50 # 50 chunks (20 sec)

wandb:
  project: "nejm-brain-to-text" 
  entity: "lionelhu926-ucla"

  