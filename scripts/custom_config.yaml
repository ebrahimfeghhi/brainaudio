# ===================================================================
#                       Shared Configuration
# This section contains parameters common to all models and experiments.
# ===================================================================
outputDir: '/data2/brain2text/b2t_24/outputs' # where to save trained model
# '/data3/brain2text/b2t_25/outputs/'
datasetPath: ['/data2/brain2text/b2t_24/brain2text24'] # neural dataset

device: 'cuda:0'
modelName: "neurips_gru_nonoverlapping_4_4_768_seed_0"
modelType: 'gru'
optimizer: 'Adam'
learning_rate: 0.02
beta1: 0.9
beta2: 0.999
eps: 1.0e-8
learning_scheduler: 'multistep'
n_epochs: 600
milestones: [400] # need to convert to batches
gamma: 0.1 # multiply learning rate by this factor at milestones
learning_rate_min: 0.0001
learning_rate_decay_steps: 120000 # in batches
learning_rate_warmup_steps: 1000 # in batches
batchSize: 64
nClasses: 40 
seed: 0
load_pretrained_model: ''
grad_norm_clip_value: -1

# time masking aug
num_masks: 20
max_mask_pct: 0.075

# original augs
whiteNoiseSD: 0.2
constantOffsetSD: 0.05
gaussianSmoothWidth: 2.0
smooth_kernel_size: 20
random_cut: 0
use_amp: true

# standard regularization techniques
l2_decay: 1.0e-5
input_dropout: 0.2
dropout: 0.35

# ===================================================================
#                    Model-Specific Configurations
# Each sub-section defines the architecture for a different model.
# ===================================================================
model:
  # --- GRU Model Arguments ---
  gru:
    name: 'GRU'
    year: '2025'
    nInputFeatures: 256
    nClasses: 40
    nUnits: 768
    nLayers: 5
    bidirectional: false
    strideLen: 4
    kernelLen: 4
    nDays: 24

  # --- Transformer Model Arguments ---
  transformer:
    name: 'Transformer'
    features_list: [512, 256] # For multiple dataset, order the feature list aligning with corresponding dataset
    samples_per_patch: 4
    d_model: 384        
    n_heads: 6          
    mlp_dim_ratio: 4 
    embed_mlp_ratio: 1.5
    dim_head: 64 
    depth: 5 

wandb:
  project: "nejm-brain-to-text" 
  entity: "lionelhu926-ucla"



  