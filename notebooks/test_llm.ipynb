{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "776365fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'endswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading Tokenizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Using gpt2 as a proxy for OPT/Gemma/Falcon (space-sensitive)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate-spaces/mamba2-2.7b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m     71\u001b[39m test_cases = [\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# Case 1: The \"Royal\" Fix\u001b[39;00m\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# Context is lowercase \"royal\", but truecase should make it \"Royal\", \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mi live in\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnew york\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     83\u001b[39m ]\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m RUNNING TESTS \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brainaudio/.venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1175\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1172\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brainaudio/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2113\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2110\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2111\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brainaudio/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2359\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2357\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2361\u001b[39m     logger.info(\n\u001b[32m   2362\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2363\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2364\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brainaudio/.venv/lib/python3.12/site-packages/transformers/models/gpt_neox/tokenization_gpt_neox_fast.py:106\u001b[39m, in \u001b[36mGPTNeoXTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, merges_file, tokenizer_file, unk_token, bos_token, eos_token, pad_token, add_bos_token, add_eos_token, add_prefix_space, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     94\u001b[39m     vocab_file=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m     **kwargs,\n\u001b[32m    105\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmerges_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerges_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28mself\u001b[39m._add_bos_token = add_bos_token\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m._add_eos_token = add_eos_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brainaudio/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mvocab_file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/brainaudio/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1857\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1855\u001b[39m     converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[32m   1856\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m converter_class(transformer_tokenizer).converted()\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mtekken.json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1858\u001b[39m     transformer_tokenizer.original_tokenizer = transformer_tokenizer\n\u001b[32m   1859\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Mistral tekken.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'endswith'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import truecase\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_candidate_tokens(tokenizer, context: str, candidate: str):\n",
    "    \"\"\"\n",
    "    Truecases the full text and returns ONLY the token IDs belonging to the candidate word.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: HuggingFace tokenizer.\n",
    "        context: The context string (e.g., \"he is a member of the royal\").\n",
    "        candidate: The candidate word (e.g., \"heirs\").\n",
    "    \n",
    "    Returns:\n",
    "        candidate_ids: List[int] of token IDs for the candidate.\n",
    "        true_full_text: The string after truecasing (for debugging).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Construct the raw full text\n",
    "    # We enforce a space between context and candidate\n",
    "    raw_full_text = f\"{context} {candidate}\"\n",
    "    \n",
    "    # 2. Apply Truecase to the full text\n",
    "    # This fixes \"royal irish\" -> \"Royal Irish\"\n",
    "    true_full_text = truecase.get_true_case(raw_full_text)\n",
    "    \n",
    "    # 3. Determine the Split Point\n",
    "    # We need to find where the 'context' ends in the truecased version.\n",
    "    # We truecase the context separately to measure its length.\n",
    "    true_context = truecase.get_true_case(context)\n",
    "    \n",
    "    # The candidate generally starts immediately after the context.\n",
    "    # Note: truecase usually strips trailing spaces, so the split point is the length of true_context.\n",
    "    # Example: \"Royal\" (len 5). Full: \"Royal Irish\". \" Irish\" starts at index 5.\n",
    "    split_char_idx = len(true_context)\n",
    "\n",
    "    # 4. Tokenize with Offsets\n",
    "    # return_offsets_mapping gives (start_char, end_char) for each token\n",
    "    inputs = tokenizer(\n",
    "        true_full_text, \n",
    "        return_tensors=\"pt\", \n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=True \n",
    "    )\n",
    "    \n",
    "    input_ids = inputs.input_ids[0]\n",
    "    offsets = inputs.offset_mapping[0]\n",
    "    \n",
    "    candidate_ids = []\n",
    "    \n",
    "    for i, (start, end) in enumerate(offsets):\n",
    "        # Skip special tokens (like BOS/EOS which often have 0,0 offsets)\n",
    "        if start == end == 0:\n",
    "            continue\n",
    "            \n",
    "        # LOGIC: If the token starts AT or AFTER our context ended, it's part of the candidate.\n",
    "        if start >= split_char_idx:\n",
    "            candidate_ids.append(input_ids[i].item())\n",
    "            \n",
    "    return candidate_ids, true_full_text\n",
    "\n",
    "# ==========================================\n",
    "# TEST BLOCK\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    print(\"Loading Tokenizer...\")\n",
    "    # Using gpt2 as a proxy for OPT/Gemma/Falcon (space-sensitive)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba2-2.7b\") \n",
    "    \n",
    "    test_cases = [\n",
    "        # Case 1: The \"Royal\" Fix\n",
    "        # Context is lowercase \"royal\", but truecase should make it \"Royal\", \n",
    "        # which changes the probability of \"Irish\" vs \"heirs\".\n",
    "        (\"he is a member of the royal\", \"irish\"),\n",
    "        \n",
    "        # Case 2: Standard lowercase continuation\n",
    "        # Context implies common noun, truecase should keep it lowercase (mostly).\n",
    "        (\"the prince and his\", \"heirs\"),\n",
    "        \n",
    "        # Case 3: Proper Noun in candidate\n",
    "        (\"i live in\", \"new york\"),\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n{'='*10} RUNNING TESTS {'='*10}\")\n",
    "\n",
    "    for ctx, cand in test_cases:\n",
    "        ids, text = get_candidate_tokens(tokenizer, ctx, cand)\n",
    "        \n",
    "        print(f\"\\nInput:     '{ctx}' + '{cand}'\")\n",
    "        print(f\"Truecased: '{text}'\")\n",
    "        print(f\"Token IDs: {ids}\")\n",
    "        print(f\"Decoded:   {tokenizer.decode(ids)}\")\n",
    "        \n",
    "        # Verification Logic\n",
    "        # We check if the decoded tokens match the candidate (ignoring casing differences)\n",
    "        decoded = tokenizer.decode(ids).strip().lower()\n",
    "        if decoded == cand.lower():\n",
    "            print(\"✅ VERIFIED: Candidate extracted correctly.\")\n",
    "        else:\n",
    "            print(f\"❌ MISMATCH: Expected '{cand}', got '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6396625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d609be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  2209,   563,   992,   496,  4374,   529,   506, 19833]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"He is also a member of the royal\", return_tensors=\"pt\", add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2946677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "2, 2209,  563,  992,  496, 4374,  529,  506, 1759\n",
    "2, 2209,  563,  992,  496, 4374,  529,  506, 19833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e56eaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>Has such a high clay content<eos>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.tensor([2, 18047,  1288,   496,  1494, 23957,  3004, tokenizer.eos_token_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8278a8d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attention_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mattention_mask\u001b[49m.shape\n",
      "\u001b[31mNameError\u001b[39m: name 'attention_mask' is not defined"
     ]
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc645941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
