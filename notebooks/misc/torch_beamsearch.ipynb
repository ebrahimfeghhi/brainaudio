{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 92,
         "metadata": {},
         "outputs": [],
         "source": [
            "from torchaudio.models.decoder import ctc_decoder, cuda_ctc_decoder\n",
            "import numpy as np \n",
            "import torch\n",
            "from torch.nn.functional import softmax"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 49,
         "metadata": {},
         "outputs": [],
         "source": [
            "language_model_path = \"/data2/brain2text/lm/languageModel/\"\n",
            "lexicon_phonemes_file = f\"{language_model_path}lexicon_phonemes.txt\"\n",
            "units_txt_file_pytorch = f\"{language_model_path}units_pytorch.txt\"\n",
            "\n",
            "units_txt_file_pytorch_char = f\"{language_model_path}units_pytorch_character.txt\"\n",
            "lexicon_char_file= f\"{language_model_path}lexicon_char.txt\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 108,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "val_transcripts = pd.read_pickle(\"/data2/brain2text/b2t_24/transcripts_val.pkl\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 73,
         "metadata": {},
         "outputs": [],
         "source": [
            "decoder_phoneme = ctc_decoder(tokens=units_txt_file_pytorch, lexicon=lexicon_phonemes_file, \n",
            "                       beam_size=50, nbest=20, lm=None)\n",
            "decoder_char = ctc_decoder(tokens=units_txt_file_pytorch_char, lexicon=lexicon_char_file, \n",
            "                       beam_size=50, nbest=20, lm=None)\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 129,
         "metadata": {},
         "outputs": [],
         "source": [
            "from torchaudio.models.decoder import download_pretrained_files\n",
            "\n",
            "files = download_pretrained_files(\"librispeech-4-gram\")\n",
            "\n",
            "decoder_char_lm = ctc_decoder(tokens=units_txt_file_pytorch_char, lexicon=lexicon_char_file, \n",
            "                       beam_size=50, nbest=20, lm=files.lm, lm_weight=3, word_score=-2)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 131,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "i say that but yet i know too\n",
                  "aye sci that butt yet eye nau thuy\n",
                  "i see that bit yet i know too\n",
                  "i see that but yet i know too\n"
               ]
            }
         ],
         "source": [
            "model_logits_phone = np.load(\"/data2/brain2text/b2t_24/logits/tm_transformer_b2t_24+25_large_wide/logits_val.npz\")\n",
            "model_logits_char = np.load(\"/data2/brain2text/b2t_24/logits/tm_transformer_combined_lw_char/logits_val.npz\")\n",
            "\n",
            "idx = 200\n",
            "print(val_transcripts[idx])\n",
            "\n",
            "\n",
            "single_trial_logits_phone = torch.as_tensor(model_logits_phone[f'arr_{idx}']).float().unsqueeze(0)\n",
            "beam_search_phoneme = decoder_phoneme(single_trial_logits_phone)\n",
            "beam_search_transcript_phoneme= \" \".join(beam_search_phoneme[0][0].words).strip()\n",
            "print(beam_search_transcript_phoneme)\n",
            "\n",
            "single_trial_logits_char = torch.as_tensor(model_logits_char[f'arr_{idx}']).float().unsqueeze(0)\n",
            "beam_search_char = decoder_char(single_trial_logits_char)\n",
            "beam_search_transcript_char = \" \".join(beam_search_char[0][0].words).strip()\n",
            "print(beam_search_transcript_char)\n",
            "\n",
            "beam_search_char_lm = decoder_char_lm(single_trial_logits_char)\n",
            "beam_search_transcript_char_lm = \" \".join(beam_search_char_lm[0][0].words).strip()\n",
            "print(beam_search_transcript_char_lm)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 147,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Transcript:  let's see about getting parents more involved\n",
                  "\n",
                  "\n",
                  "that's seery about getting pinning mohr involved\n",
                  "that's seery about getting pinning morr involved\n",
                  "that's seery about getting pinning more involved\n",
                  "that's seery about getting pinning mor involved\n",
                  "that's seery about getting pinning mohr involve\n",
                  "that's seery about getting pinning morr involve\n",
                  "that's seery about getting pinning mor involve\n",
                  "that's seery about getting pinning more involve\n",
                  "that's seery about getting penning morr involved\n",
                  "that's seery about getting penning more involved\n",
                  "\n",
                  "\n",
                  "that's hey about get counting more involved\n",
                  "that's hey about gets counting more involved\n",
                  "that's se about get counting more involved\n",
                  "that's see about get counting more involved\n",
                  "that's se about gets counting more involved\n",
                  "that's hey about getts counting more involved\n",
                  "that's see about gets counting more involved\n",
                  "that's hey about get touting more involved\n",
                  "that's hey about get counting more involve d\n",
                  "that's hey about gets counting more involve d\n"
               ]
            }
         ],
         "source": [
            "model_logits_phone = np.load(\"/data2/brain2text/b2t_24/logits/tm_transformer_b2t_24+25_large_wide/logits_val.npz\")\n",
            "model_logits_char = np.load(\"/data2/brain2text/b2t_24/logits/tm_transformer_combined_lw_char/logits_val.npz\")\n",
            "\n",
            "idx = 250\n",
            "print(\"Transcript: \", val_transcripts[idx])\n",
            "\n",
            "\n",
            "print(\"\\n\")\n",
            "single_trial_logits_phone = torch.as_tensor(model_logits_phone[f'arr_{idx}']).float().unsqueeze(0)\n",
            "beam_search_phoneme = decoder_phoneme(single_trial_logits_phone)\n",
            "for i in range(20):\n",
            "    beam_search_transcript_phoneme= \" \".join(beam_search_phoneme[0][i].words).strip()\n",
            "    print(beam_search_transcript_phoneme)\n",
            "\n",
            "print(\"\\n\")\n",
            "single_trial_logits_char = torch.as_tensor(model_logits_char[f'arr_{idx}']).float().unsqueeze(0)\n",
            "beam_search_char = decoder_char(single_trial_logits_char)\n",
            "for i in range(20):\n",
            "    beam_search_transcript_char = \" \".join(beam_search_char[0][i].words).strip()\n",
            "    print(beam_search_transcript_char)\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 101,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "single_trial_logits_char = torch.as_tensor(model_logits_char[f'arr_{idx}']).float().unsqueeze(0)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 106,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[[CTCHypothesis(tokens=tensor([40, 27, 12,  3, 40, 40, 40]), words=['PERA'], score=682.6983127593994, timesteps=tensor([ 0, 13, 15, 16, 19, 37, 50], dtype=torch.int32))]]\n"
               ]
            }
         ],
         "source": [
            "start = time.time()\n",
            "\n",
            "end = time.time()\n",
            "print(end-start)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 103,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "''"
                  ]
               },
               "execution_count": 103,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "single_trial_logits.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40,  2, 14, 31, 12, 40]), words=['VOID', 'AFTER'], score=522.8263626098633, timesteps=tensor([ 0, 10, 12, 14, 17, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 18, 40,  2, 14, 31, 12, 40]), words=['VOID', 'E.', 'AFTER'], score=520.3375873565674, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 18, 40,  2, 14, 31, 12, 40]), words=['VOID', 'E', 'AFTER'], score=520.3375873565674, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 18, 40,  2, 14, 31, 12, 40]), words=['VOID', 'EE', 'AFTER'], score=520.3375873565674, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40,  2, 14, 31, 12, 23, 34, 23, 40]), words=['VOID', 'AFTERNOON'], score=519.0685338973999, timesteps=tensor([ 0, 10, 12, 14, 17, 22, 24, 25, 27, 28, 29, 30, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'EAU', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'OHH', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'AU', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'O.', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'EAUX', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40,  2, 14, 31, 12, 23, 34, 23, 38, 40]), words=['VOID', 'AFTERNOONS'], score=513.4793133735657, timesteps=tensor([ 0, 10, 12, 14, 17, 22, 24, 25, 27, 28, 29, 30, 31, 32],\n",
                  "       dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40,  2, 14, 31, 12, 23, 34, 23, 38, 40]), words=['VOID', \"AFTERNOON'S\"], score=513.4793133735657, timesteps=tensor([ 0, 10, 12, 14, 17, 22, 24, 25, 27, 28, 29, 30, 31, 32],\n",
                  "       dtype=torch.int32))]\n"
               ]
            }
         ],
         "source": [
            "decoder.decode_begin()\n",
            "for t in range(32):\n",
            "    decoder.decode_step(single_trial_logits[0, t:t + 1, :])\n",
            "    \n",
            "decoder.decode_end()\n",
            "beam_search_result_inc = decoder.get_final_hypothesis()\n",
            "print(beam_search_result_inc)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 59,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "(5, 12, 8, 16, 5, 13, 19, 7, 19, 3, 2, 4, 11, 19, 8, 14, 18, 2, 13, 17, 14, 18, 5, 1, 6, 9)\n",
                  "Score 97.032\n"
               ]
            }
         ],
         "source": [
            "import heapq\n",
            "\n",
            "def make_new_beam():\n",
            "  fn = lambda : (NEG_INF, NEG_INF)\n",
            "  return collections.defaultdict(fn)\n",
            "\n",
            "def logsumexp(*args):\n",
            "  \"\"\"\n",
            "  Stable log sum exp.\n",
            "  (Optimized version)\n",
            "  \"\"\"\n",
            "  if not args:\n",
            "      return NEG_INF\n",
            "\n",
            "  # <-- 2. Optimized logsumexp\n",
            "  # Find max in one pass\n",
            "  a_max = args[0]\n",
            "  for a in args[1:]:\n",
            "      if a > a_max:\n",
            "          a_max = a\n",
            "\n",
            "  if a_max == NEG_INF:\n",
            "      return NEG_INF\n",
            "\n",
            "  # Calculate sum in a second pass\n",
            "  lsp_sum = 0.0\n",
            "  for a in args:\n",
            "      lsp_sum += math.exp(a - a_max)\n",
            "      \n",
            "  return a_max + math.log(lsp_sum)\n",
            "\n",
            "def decode(probs, beam_size=100, blank=0):\n",
            "  \"\"\"\n",
            "  Performs inference for the given output probabilities.\n",
            "  Arguments:\n",
            "      probs: The output probabilities (e.g. post-softmax) for each\n",
            "        time step. Should be an array of shape (time x output dim).\n",
            "      beam_size (int): Size of the beam to use during inference.\n",
            "      blank (int): Index of the CTC blank label.\n",
            "  Returns the output label sequence and the corresponding negative\n",
            "  log-likelihood estimated by the decoder.\n",
            "  \"\"\"\n",
            "  T, S = probs.shape\n",
            "  # Convert to log-probabilities once at the beginning\n",
            "  log_probs = np.log(probs)\n",
            "\n",
            "  # Elements in the beam are (prefix, (p_blank, p_no_blank))\n",
            "  # Initialize the beam with the empty sequence, a probability of\n",
            "  # 1 for ending in blank and zero for ending in non-blank\n",
            "  # (in log space).\n",
            "  beam = [(tuple(), (0.0, NEG_INF))]\n",
            "\n",
            "  for t in range(T): # Loop over time\n",
            "\n",
            "    # A default dictionary to store the next step candidates.\n",
            "    next_beam = make_new_beam()\n",
            "\n",
            "    # <-- 3. Pre-slice probabilities for this time step\n",
            "    log_probs_t = log_probs[t, :]\n",
            "\n",
            "    for s in range(S): # Loop over vocab\n",
            "      p = log_probs_t[s] # Use the pre-sliced array\n",
            "\n",
            "      # The variables p_b and p_nb are respectively the\n",
            "      # probabilities for the prefix given that it ends in a\n",
            "      # blank and does not end in a blank at this time step.\n",
            "      for prefix, (p_b, p_nb) in beam: # Loop over beam\n",
            "\n",
            "        # If we propose a blank the prefix doesn't change.\n",
            "        # Only the probability of ending in blank gets updated.\n",
            "        if s == blank:\n",
            "          n_p_b, n_p_nb = next_beam[prefix]\n",
            "          n_p_b = logsumexp(n_p_b, p_b + p, p_nb + p)\n",
            "          next_beam[prefix] = (n_p_b, n_p_nb)\n",
            "          continue\n",
            "\n",
            "        # Extend the prefix by the new character s and add it to\n",
            "        # the beam. Only the probability of not ending in blank\n",
            "        # gets updated.\n",
            "        end_t = prefix[-1] if prefix else None\n",
            "        n_prefix = prefix + (s,)\n",
            "        n_p_b, n_p_nb = next_beam[n_prefix]\n",
            "        if s != end_t:\n",
            "          n_p_nb = logsumexp(n_p_nb, p_b + p, p_nb + p)\n",
            "        else:\n",
            "          # We don't include the previous probability of not ending\n",
            "          # in blank (p_nb) if s is repeated at the end. The CTC\n",
            "          # algorithm merges characters not separated by a blank.\n",
            "          n_p_nb = logsumexp(n_p_nb, p_b + p)\n",
            "          \n",
            "        next_beam[n_prefix] = (n_p_b, n_p_nb)\n",
            "\n",
            "        # If s is repeated at the end we also update the unchanged\n",
            "        # prefix. This is the merging case.\n",
            "        if s == end_t:\n",
            "          n_p_b, n_p_nb = next_beam[prefix]\n",
            "          n_p_nb = logsumexp(n_p_nb, p_nb + p)\n",
            "          next_beam[prefix] = (n_p_b, n_p_nb)\n",
            "\n",
            "    # <-- 4. MAJOR OPTIMIZATION: Use heapq.nlargest instead of full sort\n",
            "    # Original:\n",
            "    # beam = sorted(next_beam.items(),\n",
            "    #         key=lambda x : logsumexp(*x[1]),\n",
            "    #         reverse=True)\n",
            "    # beam = beam[:beam_size]\n",
            "    \n",
            "    # Optimized:\n",
            "    beam_items = next_beam.items()\n",
            "    beam = heapq.nlargest(beam_size,\n",
            "                          beam_items,\n",
            "                          key=lambda x: logsumexp(*x[1]))\n",
            "\n",
            "  # Final selection\n",
            "  best = beam[0]\n",
            "  return best[0], -logsumexp(*best[1])\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "  np.random.seed(3)\n",
            "\n",
            "  time = 50\n",
            "  output_dim = 20\n",
            "\n",
            "  probs = np.random.rand(time, output_dim)\n",
            "  probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
            "\n",
            "  labels, score = decode(probs)\n",
            "  print(labels)\n",
            "  print(\"Score {:.3f}\".format(score))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 63,
         "metadata": {},
         "outputs": [],
         "source": [
            "from scipy.special import softmax\n",
            "logits = softmax(model_logits[f'arr_750}'],axis=1)\n",
            "import time"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 65,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "0.6410787105560303\n"
               ]
            }
         ],
         "source": [
            "start = time.time()\n",
            "labels, probs = decode(logits)\n",
            "end = time.time()\n",
            "print(end-start)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 51,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "\n",
            "# Load model and tokenizer\n",
            "access_token = \"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\"\n",
            "model_name = \"google/gemma-3-270m\"\n",
            "\n",
            "device = \"cuda:2\"\n",
            "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
            "llm = AutoModelForCausalLM.from_pretrained(model_name, token=access_token)\n",
            "llm = llm.to(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from __future__ import annotations\n",
            "\n",
            "import itertools as it\n",
            "\n",
            "from abc import abstractmethod\n",
            "from collections import namedtuple\n",
            "from typing import Dict, List, NamedTuple, Optional, Tuple, Union\n",
            "\n",
            "import torch\n",
            "\n",
            "from flashlight.lib.text.decoder import (\n",
            "    CriterionType as _CriterionType,\n",
            "    LexiconDecoder as _LexiconDecoder,\n",
            "    LexiconDecoderOptions as _LexiconDecoderOptions,\n",
            "    LexiconFreeDecoder as _LexiconFreeDecoder,\n",
            "    LexiconFreeDecoderOptions as _LexiconFreeDecoderOptions,\n",
            "    LM as _LM,\n",
            "    LMState as _LMState,\n",
            "    SmearingMode as _SmearingMode,\n",
            "    Trie as _Trie,\n",
            "    ZeroLM as _ZeroLM,\n",
            ")\n",
            "from flashlight.lib.text.dictionary import (\n",
            "    create_word_dict as _create_word_dict,\n",
            "    Dictionary as _Dictionary,\n",
            "    load_words as _load_words,\n",
            ")\n",
            "from torchaudio.utils import download_asset\n",
            "\n",
            "try:\n",
            "    from flashlight.lib.text.decoder.kenlm import KenLM as _KenLM\n",
            "except Exception:\n",
            "    try:\n",
            "        from flashlight.lib.text.decoder import KenLM as _KenLM\n",
            "    except Exception:\n",
            "        _KenLM = None\n",
            "\n",
            "__all__ = [\n",
            "    \"CTCHypothesis\",\n",
            "    \"CTCDecoder\",\n",
            "    \"CTCDecoderLM\",\n",
            "    \"CTCDecoderLMState\",\n",
            "    \"ctc_decoder\",\n",
            "    \"download_pretrained_files\",\n",
            "]\n",
            "\n",
            "_PretrainedFiles = namedtuple(\"PretrainedFiles\", [\"lexicon\", \"tokens\", \"lm\"])\n",
            "\n",
            "\n",
            "def _construct_trie(tokens_dict, word_dict, lexicon, lm, silence):\n",
            "    vocab_size = tokens_dict.index_size()\n",
            "    trie = _Trie(vocab_size, silence)\n",
            "    start_state = lm.start(False)\n",
            "\n",
            "    for word, spellings in lexicon.items():\n",
            "        word_idx = word_dict.get_index(word)\n",
            "        _, score = lm.score(start_state, word_idx)\n",
            "        for spelling in spellings:\n",
            "            spelling_idx = [tokens_dict.get_index(token) for token in spelling]\n",
            "            trie.insert(spelling_idx, word_idx, score)\n",
            "    trie.smear(_SmearingMode.MAX)\n",
            "    return trie\n",
            "\n",
            "\n",
            "def _get_word_dict(lexicon, lm, lm_dict, tokens_dict, unk_word):\n",
            "    word_dict = None\n",
            "    if lm_dict is not None:\n",
            "        word_dict = _Dictionary(lm_dict)\n",
            "\n",
            "    if lexicon and word_dict is None:\n",
            "        word_dict = _create_word_dict(lexicon)\n",
            "    elif not lexicon and word_dict is None and type(lm) == str:\n",
            "        d = {tokens_dict.get_entry(i): [[tokens_dict.get_entry(i)]] for i in range(tokens_dict.index_size())}\n",
            "        d[unk_word] = [[unk_word]]\n",
            "        word_dict = _create_word_dict(d)\n",
            "\n",
            "    return word_dict\n",
            "\n",
            "\n",
            "class CTCHypothesis(NamedTuple):\n",
            "    \"\"\"Represents hypothesis generated by CTC beam search decoder :class:`CTCDecoder`.\"\"\"\n",
            "    tokens: torch.LongTensor\n",
            "    \"\"\"Predicted sequence of token IDs. Shape `(L, )`, where `L` is the length of the output sequence\"\"\"\n",
            "\n",
            "    words: List[str]\n",
            "    \"\"\"List of predicted words.\n",
            "\n",
            "    Note:\n",
            "        This attribute is only applicable if a lexicon is provided to the decoder. If\n",
            "        decoding without a lexicon, it will be blank. Please refer to :attr:`tokens` and\n",
            "        :func:`~torchaudio.models.decoder.CTCDecoder.idxs_to_tokens` instead.\n",
            "    \"\"\"\n",
            "\n",
            "    score: float\n",
            "    \"\"\"Score corresponding to hypothesis\"\"\"\n",
            "\n",
            "    timesteps: torch.IntTensor\n",
            "    \"\"\"Timesteps corresponding to the tokens. Shape `(L, )`, where `L` is the length of the output sequence\"\"\"\n",
            "\n",
            "\n",
            "\n",
            "class CTCDecoderLMState(_LMState):\n",
            "    \"\"\"Language model state.\"\"\"\n",
            "\n",
            "    @property\n",
            "    def children(self) -> Dict[int, CTCDecoderLMState]:\n",
            "        \"\"\"Map of indices to LM states\"\"\"\n",
            "        return super().children\n",
            "\n",
            "    def child(self, usr_index: int) -> CTCDecoderLMState:\n",
            "        \"\"\"Returns child corresponding to usr_index, or creates and returns a new state if input index\n",
            "        is not found.\n",
            "\n",
            "        Args:\n",
            "            usr_index (int): index corresponding to child state\n",
            "\n",
            "        Returns:\n",
            "            CTCDecoderLMState: child state corresponding to usr_index\n",
            "        \"\"\"\n",
            "        return super().child(usr_index)\n",
            "\n",
            "\n",
            "    def compare(self, state: CTCDecoderLMState) -> CTCDecoderLMState:\n",
            "        \"\"\"Compare two language model states.\n",
            "\n",
            "        Args:\n",
            "            state (CTCDecoderLMState): LM state to compare against\n",
            "\n",
            "        Returns:\n",
            "            int: 0 if the states are the same, -1 if self is less, +1 if self is greater.\n",
            "        \"\"\"\n",
            "        pass\n",
            "\n",
            "\n",
            "\n",
            "    class CTCDecoderLM(_LM):\n",
            "        \"\"\"Language model base class for creating custom language models to use with the decoder.\"\"\"\n",
            "\n",
            "        @abstractmethod\n",
            "        def start(self, start_with_nothing: bool) -> CTCDecoderLMState:\n",
            "            \"\"\"Initialize or reset the language model.\n",
            "\n",
            "            Args:\n",
            "                start_with_nothing (bool): whether or not to start sentence with sil token.\n",
            "\n",
            "            Returns:\n",
            "                CTCDecoderLMState: starting state\n",
            "            \"\"\"\n",
            "            raise NotImplementedError\n",
            "\n",
            "\n",
            "        @abstractmethod\n",
            "        def score(self, state: CTCDecoderLMState, usr_token_idx: int) -> Tuple[CTCDecoderLMState, float]:\n",
            "            \"\"\"Evaluate the language model based on the current LM state and new word.\n",
            "\n",
            "            Args:\n",
            "                state (CTCDecoderLMState): current LM state\n",
            "                usr_token_idx (int): index of the word\n",
            "\n",
            "            Returns:\n",
            "                (CTCDecoderLMState, float)\n",
            "                    CTCDecoderLMState:\n",
            "                        new LM state\n",
            "                    float:\n",
            "                        score\n",
            "            \"\"\"\n",
            "            raise NotImplementedError\n",
            "\n",
            "\n",
            "        @abstractmethod\n",
            "        def finish(self, state: CTCDecoderLMState) -> Tuple[CTCDecoderLMState, float]:\n",
            "            \"\"\"Evaluate end for language model based on current LM state.\n",
            "\n",
            "            Args:\n",
            "                state (CTCDecoderLMState): current LM state\n",
            "\n",
            "            Returns:\n",
            "                (CTCDecoderLMState, float)\n",
            "                    CTCDecoderLMState:\n",
            "                        new LM state\n",
            "                    float:\n",
            "                        score\n",
            "            \"\"\"\n",
            "            raise NotImplementedError\n",
            "\n",
            "\n",
            "\n",
            "class CTCDecoder:\n",
            "    \"\"\"CTC beam search decoder from *Flashlight* :cite:`kahn2022flashlight`.\n",
            "\n",
            "    .. devices:: CPU\n",
            "\n",
            "    Note:\n",
            "        To build the decoder, please use the factory function :func:`ctc_decoder`.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        nbest: int,\n",
            "        lexicon: Optional[Dict],\n",
            "        word_dict: _Dictionary,\n",
            "        tokens_dict: _Dictionary,\n",
            "        lm: CTCDecoderLM,\n",
            "        decoder_options: Union[_LexiconDecoderOptions, _LexiconFreeDecoderOptions],\n",
            "        blank_token: str,\n",
            "        sil_token: str,\n",
            "        unk_word: str,\n",
            "    ) -> None:\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            nbest (int): number of best decodings to return\n",
            "            lexicon (Dict or None): lexicon mapping of words to spellings, or None for lexicon-free decoder\n",
            "            word_dict (_Dictionary): dictionary of words\n",
            "            tokens_dict (_Dictionary): dictionary of tokens\n",
            "            lm (CTCDecoderLM): language model. If using a lexicon, only word level LMs are currently supported\n",
            "            decoder_options (_LexiconDecoderOptions or _LexiconFreeDecoderOptions):\n",
            "                parameters used for beam search decoding\n",
            "            blank_token (str): token corresopnding to blank\n",
            "            sil_token (str): token corresponding to silence\n",
            "            unk_word (str): word corresponding to unknown\n",
            "        \"\"\"\n",
            "\n",
            "        self.nbest = nbest\n",
            "        self.word_dict = word_dict\n",
            "        self.tokens_dict = tokens_dict\n",
            "        self.blank = self.tokens_dict.get_index(blank_token)\n",
            "        silence = self.tokens_dict.get_index(sil_token)\n",
            "        transitions = []\n",
            "\n",
            "        if lexicon:\n",
            "            trie = _construct_trie(tokens_dict, word_dict, lexicon, lm, silence)\n",
            "            unk_word = word_dict.get_index(unk_word)\n",
            "            token_lm = False  # use word level LM\n",
            "\n",
            "            self.decoder = _LexiconDecoder(\n",
            "                decoder_options,\n",
            "                trie,\n",
            "                lm,\n",
            "                silence,\n",
            "                self.blank,\n",
            "                unk_word,\n",
            "                transitions,\n",
            "                token_lm,\n",
            "            )\n",
            "        else:\n",
            "            self.decoder = _LexiconFreeDecoder(decoder_options, lm, silence, self.blank, transitions)\n",
            "        # https://github.com/pytorch/audio/issues/3218\n",
            "        # If lm is passed like rvalue reference, the lm object gets garbage collected,\n",
            "        # and later call to the lm fails.\n",
            "        # This ensures that lm object is not deleted as long as the decoder is alive.\n",
            "        # https://github.com/pybind/pybind11/discussions/4013\n",
            "        self.lm = lm\n",
            "\n",
            "    def _get_tokens(self, idxs: torch.IntTensor) -> torch.LongTensor:\n",
            "        idxs = (g[0] for g in it.groupby(idxs))\n",
            "        idxs = filter(lambda x: x != self.blank, idxs)\n",
            "        return torch.LongTensor(list(idxs))\n",
            "\n",
            "    def _get_timesteps(self, idxs: torch.IntTensor) -> torch.IntTensor:\n",
            "        \"\"\"Returns frame numbers corresponding to non-blank tokens.\"\"\"\n",
            "\n",
            "        timesteps = []\n",
            "        for i, idx in enumerate(idxs):\n",
            "            if idx == self.blank:\n",
            "                continue\n",
            "            if i == 0 or idx != idxs[i - 1]:\n",
            "                timesteps.append(i)\n",
            "        return torch.IntTensor(timesteps)\n",
            "\n",
            "    def decode_begin(self):\n",
            "        \"\"\"Initialize the internal state of the decoder.\n",
            "\n",
            "        See :py:meth:`decode_step` for the usage.\n",
            "\n",
            "        .. note::\n",
            "\n",
            "           This method is required only when performing online decoding.\n",
            "           It is not necessary when performing batch decoding with :py:meth:`__call__`.\n",
            "        \"\"\"\n",
            "        self.decoder.decode_begin()\n",
            "\n",
            "\n",
            "    def decode_end(self):\n",
            "        \"\"\"Finalize the internal state of the decoder.\n",
            "\n",
            "        See :py:meth:`decode_step` for the usage.\n",
            "\n",
            "        .. note::\n",
            "\n",
            "           This method is required only when performing online decoding.\n",
            "           It is not necessary when performing batch decoding with :py:meth:`__call__`.\n",
            "        \"\"\"\n",
            "        self.decoder.decode_end()\n",
            "\n",
            "\n",
            "    def decode_step(self, emissions: torch.FloatTensor):\n",
            "        \"\"\"Perform incremental decoding on top of the curent internal state.\n",
            "\n",
            "        .. note::\n",
            "\n",
            "           This method is required only when performing online decoding.\n",
            "           It is not necessary when performing batch decoding with :py:meth:`__call__`.\n",
            "\n",
            "        Args:\n",
            "            emissions (torch.FloatTensor): CPU tensor of shape `(frame, num_tokens)` storing sequences of\n",
            "                probability distribution over labels; output of acoustic model.\n",
            "\n",
            "        Example:\n",
            "            >>> decoder = torchaudio.models.decoder.ctc_decoder(...)\n",
            "            >>> decoder.decode_begin()\n",
            "            >>> decoder.decode_step(emission1)\n",
            "            >>> decoder.decode_step(emission2)\n",
            "            >>> decoder.decode_end()\n",
            "            >>> result = decoder.get_final_hypothesis()\n",
            "        \"\"\"\n",
            "        if emissions.dtype != torch.float32:\n",
            "            raise ValueError(\"emissions must be float32.\")\n",
            "\n",
            "        if not emissions.is_cpu:\n",
            "            raise RuntimeError(\"emissions must be a CPU tensor.\")\n",
            "\n",
            "        if not emissions.is_contiguous():\n",
            "            raise RuntimeError(\"emissions must be contiguous.\")\n",
            "\n",
            "        if emissions.ndim != 2:\n",
            "            raise RuntimeError(f\"emissions must be 2D. Found {emissions.shape}\")\n",
            "\n",
            "        T, N = emissions.size()\n",
            "        self.decoder.decode_step(emissions.data_ptr(), T, N)\n",
            "\n",
            "\n",
            "    def _to_hypo(self, results) -> List[CTCHypothesis]:\n",
            "        return [\n",
            "            CTCHypothesis(\n",
            "                tokens=self._get_tokens(result.tokens),\n",
            "                words=[self.word_dict.get_entry(x) for x in result.words if x >= 0],\n",
            "                score=result.score,\n",
            "                timesteps=self._get_timesteps(result.tokens),\n",
            "            )\n",
            "            for result in results\n",
            "        ]\n",
            "\n",
            "    def get_final_hypothesis(self) -> List[CTCHypothesis]:\n",
            "        \"\"\"Get the final hypothesis\n",
            "\n",
            "        Returns:\n",
            "            List[CTCHypothesis]:\n",
            "                List of sorted best hypotheses.\n",
            "\n",
            "        .. note::\n",
            "\n",
            "           This method is required only when performing online decoding.\n",
            "           It is not necessary when performing batch decoding with :py:meth:`__call__`.\n",
            "        \"\"\"\n",
            "        results = self.decoder.get_all_final_hypothesis()\n",
            "        return self._to_hypo(results[: self.nbest])\n",
            "\n",
            "\n",
            "    def __call__(\n",
            "        self, emissions: torch.FloatTensor, lengths: Optional[torch.Tensor] = None\n",
            "    ) -> List[List[CTCHypothesis]]:\n",
            "        \"\"\"\n",
            "        Performs batched offline decoding.\n",
            "\n",
            "        .. note::\n",
            "\n",
            "           This method performs offline decoding in one go. To perform incremental decoding,\n",
            "           please refer to :py:meth:`decode_step`.\n",
            "\n",
            "        Args:\n",
            "            emissions (torch.FloatTensor): CPU tensor of shape `(batch, frame, num_tokens)` storing sequences of\n",
            "                probability distribution over labels; output of acoustic model.\n",
            "            lengths (Tensor or None, optional): CPU tensor of shape `(batch, )` storing the valid length of\n",
            "                in time axis of the output Tensor in each batch.\n",
            "\n",
            "        Returns:\n",
            "            List[List[CTCHypothesis]]:\n",
            "                List of sorted best hypotheses for each audio sequence in the batch.\n",
            "        \"\"\"\n",
            "\n",
            "        if emissions.dtype != torch.float32:\n",
            "            raise ValueError(\"emissions must be float32.\")\n",
            "\n",
            "        if not emissions.is_cpu:\n",
            "            raise RuntimeError(\"emissions must be a CPU tensor.\")\n",
            "\n",
            "        if not emissions.is_contiguous():\n",
            "            raise RuntimeError(\"emissions must be contiguous.\")\n",
            "\n",
            "        if emissions.ndim != 3:\n",
            "            raise RuntimeError(f\"emissions must be 3D. Found {emissions.shape}\")\n",
            "\n",
            "        if lengths is not None and not lengths.is_cpu:\n",
            "            raise RuntimeError(\"lengths must be a CPU tensor.\")\n",
            "\n",
            "        B, T, N = emissions.size()\n",
            "        if lengths is None:\n",
            "            lengths = torch.full((B,), T)\n",
            "\n",
            "        float_bytes = 4\n",
            "        hypos = []\n",
            "\n",
            "        for b in range(B):\n",
            "            emissions_ptr = emissions.data_ptr() + float_bytes * b * emissions.stride(0)\n",
            "            results = self.decoder.decode(emissions_ptr, lengths[b], N)\n",
            "            hypos.append(self._to_hypo(results[: self.nbest]))\n",
            "        return hypos\n",
            "\n",
            "\n",
            "    def idxs_to_tokens(self, idxs: torch.LongTensor) -> List:\n",
            "        \"\"\"\n",
            "        Map raw token IDs into corresponding tokens\n",
            "\n",
            "        Args:\n",
            "            idxs (LongTensor): raw token IDs generated from decoder\n",
            "\n",
            "        Returns:\n",
            "            List: tokens corresponding to the input IDs\n",
            "        \"\"\"\n",
            "        return [self.tokens_dict.get_entry(idx.item()) for idx in idxs]\n",
            "\n",
            "\n",
            "\n",
            "def ctc_decoder(\n",
            "    lexicon: Optional[str],\n",
            "    tokens: Union[str, List[str]],\n",
            "    lm: Union[str, CTCDecoderLM] = None,\n",
            "    lm_dict: Optional[str] = None,\n",
            "    nbest: int = 1,\n",
            "    beam_size: int = 50,\n",
            "    beam_size_token: Optional[int] = None,\n",
            "    beam_threshold: float = 50,\n",
            "    lm_weight: float = 2,\n",
            "    word_score: float = 0,\n",
            "    unk_score: float = float(\"-inf\"),\n",
            "    sil_score: float = 0,\n",
            "    log_add: bool = False,\n",
            "    blank_token: str = \"-\",\n",
            "    sil_token: str = \"|\",\n",
            "    unk_word: str = \"<unk>\",\n",
            ") -> CTCDecoder:\n",
            "    \"\"\"Builds an instance of :class:`CTCDecoder`.\n",
            "\n",
            "    Args:\n",
            "        lexicon (str or None): lexicon file containing the possible words and corresponding spellings.\n",
            "            Each line consists of a word and its space separated spelling. If `None`, uses lexicon-free\n",
            "            decoding.\n",
            "        tokens (str or List[str]): file or list containing valid tokens. If using a file, the expected\n",
            "            format is for tokens mapping to the same index to be on the same line\n",
            "        lm (str, CTCDecoderLM, or None, optional): either a path containing KenLM language model,\n",
            "            custom language model of type `CTCDecoderLM`, or `None` if not using a language model\n",
            "        lm_dict (str or None, optional): file consisting of the dictionary used for the LM, with a word\n",
            "            per line sorted by LM index. If decoding with a lexicon, entries in lm_dict must also occur\n",
            "            in the lexicon file. If `None`, dictionary for LM is constructed using the lexicon file.\n",
            "            (Default: None)\n",
            "        nbest (int, optional): number of best decodings to return (Default: 1)\n",
            "        beam_size (int, optional): max number of hypos to hold after each decode step (Default: 50)\n",
            "        beam_size_token (int, optional): max number of tokens to consider at each decode step.\n",
            "            If `None`, it is set to the total number of tokens (Default: None)\n",
            "        beam_threshold (float, optional): threshold for pruning hypothesis (Default: 50)\n",
            "        lm_weight (float, optional): weight of language model (Default: 2)\n",
            "        word_score (float, optional): word insertion score (Default: 0)\n",
            "        unk_score (float, optional): unknown word insertion score (Default: -inf)\n",
            "        sil_score (float, optional): silence insertion score (Default: 0)\n",
            "        log_add (bool, optional): whether or not to use logadd when merging hypotheses (Default: False)\n",
            "        blank_token (str, optional): token corresponding to blank (Default: \"-\")\n",
            "        sil_token (str, optional): token corresponding to silence (Default: \"|\")\n",
            "        unk_word (str, optional): word corresponding to unknown (Default: \"<unk>\")\n",
            "\n",
            "    Returns:\n",
            "        CTCDecoder: decoder\n",
            "\n",
            "    Example\n",
            "        >>> decoder = ctc_decoder(\n",
            "        >>>     lexicon=\"lexicon.txt\",\n",
            "        >>>     tokens=\"tokens.txt\",\n",
            "        >>>     lm=\"kenlm.bin\",\n",
            "        >>> )\n",
            "        >>> results = decoder(emissions) # List of shape (B, nbest) of Hypotheses\n",
            "    \"\"\"\n",
            "    if lm_dict is not None and type(lm_dict) is not str:\n",
            "        raise ValueError(\"lm_dict must be None or str type.\")\n",
            "\n",
            "    tokens_dict = _Dictionary(tokens)\n",
            "\n",
            "    # decoder options\n",
            "    if lexicon:\n",
            "        lexicon = _load_words(lexicon)\n",
            "        decoder_options = _LexiconDecoderOptions(\n",
            "            beam_size=beam_size,\n",
            "            beam_size_token=beam_size_token or tokens_dict.index_size(),\n",
            "            beam_threshold=beam_threshold,\n",
            "            lm_weight=lm_weight,\n",
            "            word_score=word_score,\n",
            "            unk_score=unk_score,\n",
            "            sil_score=sil_score,\n",
            "            log_add=log_add,\n",
            "            criterion_type=_CriterionType.CTC,\n",
            "        )\n",
            "    else:\n",
            "        decoder_options = _LexiconFreeDecoderOptions(\n",
            "            beam_size=beam_size,\n",
            "            beam_size_token=beam_size_token or tokens_dict.index_size(),\n",
            "            beam_threshold=beam_threshold,\n",
            "            lm_weight=lm_weight,\n",
            "            sil_score=sil_score,\n",
            "            log_add=log_add,\n",
            "            criterion_type=_CriterionType.CTC,\n",
            "        )\n",
            "\n",
            "    # construct word dict and language model\n",
            "    word_dict = _get_word_dict(lexicon, lm, lm_dict, tokens_dict, unk_word)\n",
            "\n",
            "    if type(lm) == str:\n",
            "        if _KenLM is None:\n",
            "            raise RuntimeError(\n",
            "                \"flashlight-text is installed, but KenLM is not installed. \"\n",
            "                \"Please refer to https://github.com/kpu/kenlm#python-module for how to install it.\"\n",
            "            )\n",
            "        lm = _KenLM(lm, word_dict)\n",
            "    elif lm is None:\n",
            "        lm = _ZeroLM()\n",
            "\n",
            "    return CTCDecoder(\n",
            "        nbest=nbest,\n",
            "        lexicon=lexicon,\n",
            "        word_dict=word_dict,\n",
            "        tokens_dict=tokens_dict,\n",
            "        lm=lm,\n",
            "        decoder_options=decoder_options,\n",
            "        blank_token=blank_token,\n",
            "        sil_token=sil_token,\n",
            "        unk_word=unk_word,\n",
            "    )\n",
            "\n",
            "\n",
            "\n",
            "def _get_filenames(model: str) -> _PretrainedFiles:\n",
            "    if model not in [\"librispeech\", \"librispeech-3-gram\", \"librispeech-4-gram\"]:\n",
            "        raise ValueError(\n",
            "            f\"{model} not supported. Must be one of ['librispeech-3-gram', 'librispeech-4-gram', 'librispeech']\"\n",
            "        )\n",
            "\n",
            "    prefix = f\"decoder-assets/{model}\"\n",
            "    return _PretrainedFiles(\n",
            "        lexicon=f\"{prefix}/lexicon.txt\",\n",
            "        tokens=f\"{prefix}/tokens.txt\",\n",
            "        lm=f\"{prefix}/lm.bin\" if model != \"librispeech\" else None,\n",
            "    )\n",
            "\n",
            "\n",
            "def download_pretrained_files(model: str) -> _PretrainedFiles:\n",
            "    \"\"\"\n",
            "    Retrieves pretrained data files used for :func:`ctc_decoder`.\n",
            "\n",
            "    Args:\n",
            "        model (str): pretrained language model to download.\n",
            "            Valid values are: ``\"librispeech-3-gram\"``, ``\"librispeech-4-gram\"`` and ``\"librispeech\"``.\n",
            "\n",
            "    Returns:\n",
            "        Object with the following attributes\n",
            "\n",
            "            * ``lm``: path corresponding to downloaded language model,\n",
            "              or ``None`` if the model is not associated with an lm\n",
            "            * ``lexicon``: path corresponding to downloaded lexicon file\n",
            "            * ``tokens``: path corresponding to downloaded tokens file\n",
            "    \"\"\"\n",
            "\n",
            "    files = _get_filenames(model)\n",
            "    lexicon_file = download_asset(files.lexicon)\n",
            "    tokens_file = download_asset(files.tokens)\n",
            "    if files.lm is not None:\n",
            "        lm_file = download_asset(files.lm)\n",
            "    else:\n",
            "        lm_file = None\n",
            "\n",
            "    return _PretrainedFiles(\n",
            "        lexicon=lexicon_file,\n",
            "        tokens=tokens_file,\n",
            "        lm=lm_file,\n",
            "    )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "import math\n",
            "import collections\n",
            "\n",
            "NEG_INF = -float(\"inf\")\n",
            "\n",
            "def make_new_beam():\n",
            "    '''\n",
            "        A helper that creates a dictionary. If you ask for a prefix that doesn't exist, it gives you (NEG_INF, NEG_INF) \n",
            "        (log-probability of 0), which is a safe starting point.\n",
            "    '''\n",
            "    # A beam element looks like this: (prefix, (p_b, p_nb)) Example: (('C', 'A', 'T'), (-2.3, -4.5))\n",
            "    # Store two probabilities at each node: \n",
            "    # if the underlying phoneme prefix end with blank, extend the text sequence\n",
            "    # if the underlying phoneme prefix end with a rep char, do not extend the text sequence\n",
            "    fn = lambda : (NEG_INF, NEG_INF)\n",
            "    return collections.defaultdict(fn)\n",
            "\n",
            "def logsumexp(*args):\n",
            "    \"\"\"\n",
            "    Stable log sum exp. logsumexp(a, b) is equivalent to log(exp(a) + exp(b)).\n",
            "    \"\"\"\n",
            "    if all(a == NEG_INF for a in args):\n",
            "        return NEG_INF\n",
            "    a_max = max(args)\n",
            "    lsp = math.log(sum(math.exp(a - a_max)\n",
            "                        for a in args))\n",
            "    return a_max + lsp\n",
            "\n",
            "def decode(probs, beam_size=100, blank=0):\n",
            "    \"\"\"\n",
            "    Performs inference for the given output probabilities.\n",
            "    Arguments:\n",
            "        probs: The output probabilities (e.g. post-softmax) for each\n",
            "          time step. Should be an array of shape (time x output dim).\n",
            "        beam_size (int): Size of the beam to use during inference.\n",
            "        blank (int): Index of the CTC blank label.\n",
            "    Returns the output label sequence and the corresponding negative\n",
            "    log-likelihood estimated by the decoder.\n",
            "    \"\"\"\n",
            "    T, S = probs.shape\n",
            "    probs = np.log(probs)\n",
            "\n",
            "    # Elements in the beam are (prefix, (p_blank, p_no_blank))\n",
            "    # Initialize the beam with the empty sequence, a probability of\n",
            "    # 1 for ending in blank and zero for ending in non-blank\n",
            "    # (in log space).\n",
            "    beam = [(tuple(), (0.0, NEG_INF))]\n",
            "\n",
            "    for t in range(T): # Loop over time\n",
            "\n",
            "      # A default dictionary to store the next step candidates.\n",
            "      next_beam = make_new_beam()\n",
            "\n",
            "      for s in range(S): # Loop over vocab\n",
            "        p = probs[t, s]\n",
            "\n",
            "        # The variables p_b and p_nb are respectively the\n",
            "        # probabilities for the prefix given that it ends in a\n",
            "        # blank and does not end in a blank at this time step.\n",
            "        for prefix, (p_b, p_nb) in beam: # Loop over beam\n",
            "\n",
            "          # If we propose a blank the prefix doesn't change.\n",
            "          # Only the probability of ending in blank gets updated.\n",
            "          if s == blank:\n",
            "            n_p_b, n_p_nb = next_beam[prefix]\n",
            "            n_p_b = logsumexp(n_p_b, p_b + p, p_nb + p)\n",
            "            next_beam[prefix] = (n_p_b, n_p_nb)\n",
            "            continue\n",
            "\n",
            "          # Extend the prefix by the new character s and add it to\n",
            "          # the beam. Only the probability of not ending in blank\n",
            "          # gets updated.\n",
            "          end_t = prefix[-1] if prefix else None\n",
            "          n_prefix = prefix + (s,)\n",
            "          n_p_b, n_p_nb = next_beam[n_prefix]\n",
            "          if s != end_t:\n",
            "            n_p_nb = logsumexp(n_p_nb, p_b + p, p_nb + p)\n",
            "          else:\n",
            "            # We don't include the previous probability of not ending\n",
            "            # in blank (p_nb) if s is repeated at the end. The CTC\n",
            "            # algorithm merges characters not separated by a blank.\n",
            "            n_p_nb = logsumexp(n_p_nb, p_b + p)\n",
            "            \n",
            "          # *NB* this would be a good place to include an LM score.\n",
            "          next_beam[n_prefix] = (n_p_b, n_p_nb)\n",
            "\n",
            "          # If s is repeated at the end we also update the unchanged\n",
            "          # prefix. This is the merging case.\n",
            "          if s == end_t:\n",
            "            n_p_b, n_p_nb = next_beam[prefix]\n",
            "            n_p_nb = logsumexp(n_p_nb, p_nb + p)\n",
            "            next_beam[prefix] = (n_p_b, n_p_nb)\n",
            "\n",
            "      # Sort and trim the beam before moving on to the\n",
            "      # next time-step.\n",
            "      beam = sorted(next_beam.items(),\n",
            "              key=lambda x : logsumexp(*x[1]),\n",
            "              reverse=True)\n",
            "      beam = beam[:beam_size]\n",
            "\n",
            "    best = beam[0]\n",
            "    return best[0], -logsumexp(*best[1])\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "  np.random.seed(3)\n",
            "\n",
            "  time = 50\n",
            "  output_dim = 20\n",
            "\n",
            "  probs = np.random.rand(time, output_dim)\n",
            "  probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
            "\n",
            "  labels, score = decode(probs)\n",
            "  print(\"Score {:.3f}\".format(score))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "import torch.nn as nn\n",
            "\n",
            "def llm_scoring(input, llm, tokenizer):\n",
            "    '''\n",
            "        Takes the input of current word beams and apply llm to get language model scoring.\n",
            "        Args:\n",
            "            input (list[list[str] | str]) : a list of list of words, representing the current word beams.\n",
            "            llm (nn.Module) : the HuggingFace large language model.\n",
            "            tokenizer : the corresponding HuggingFace tokenizer.\n",
            "        Return:\n",
            "            scores (list[float]): A list of scores (total log-probability), one for each input beam.\n",
            "    '''\n",
            "    standardized_batch = []\n",
            "    for item in input:\n",
            "        if isinstance(item, list):\n",
            "            # If it's a list, join it with spaces\n",
            "            standardized_batch.append(\" \".join(item))\n",
            "        elif isinstance(item, str):\n",
            "            # If it's already a string, just append it\n",
            "            standardized_batch.append(item)\n",
            "\n",
            "    tokenized_input = tokenizer(standardized_batch, padding=True, return_tensors=\"pt\")\n",
            "\n",
            "    # Move tensors to the same device as the model\n",
            "    input_ids = tokenized_input['input_ids'].to(llm.device) #  (Beam_size, T_max, embed_dim)\n",
            "    attention_mask = tokenized_input['attention_mask'].to(llm.device)\n",
            "\n",
            "    # 3. Get logits from the LLM\n",
            "    with torch.no_grad():\n",
            "        # Pass both input_ids and attention_mask\n",
            "        outputs = llm(input_ids=input_ids, attention_mask=attention_mask) \n",
            "        logits = outputs.logits # (Beam_size, T_max, vocab_size)\n",
            "\n",
            "    # 4. Calculate the total log-probability for each sequence\n",
            "    # We need to compute the loss (NLL) of each token based on the *previous* token's logit. So, we shift the logits and labels.\n",
            "    \n",
            "    # Shift logits: Shape (B, T-1, V)\n",
            "    # We drop the last logit (which predicts nothing)\n",
            "    shift_logits = logits[..., :-1, :].contiguous()\n",
            "    \n",
            "    # Shift labels: Shape (B, T-1)\n",
            "    # We drop the first token (BOS) since it's not predicted\n",
            "    shift_labels = input_ids[..., 1:].contiguous()\n",
            "\n",
            "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
            "    \n",
            "    # Flatten so (B, T-1, V) -> (B*(T-1), V) and (B, T-1) -> (B*(T-1),)\n",
            "    per_token_nll = loss_fct(\n",
            "        shift_logits.view(-1, shift_logits.size(-1)), \n",
            "        shift_labels.view(-1)\n",
            "    )\n",
            "\n",
            "    # Reshape back to (B, T-1)\n",
            "    per_token_nll = per_token_nll.view(input_ids.size(0), -1)\n",
            "\n",
            "    # 6. Mask out the NLL of padding tokens\n",
            "    # We need to shift the attention mask just like the labels\n",
            "    shift_mask = attention_mask[..., 1:].contiguous()\n",
            "    \n",
            "    # Multiply by the mask. NLL for padding tokens becomes 0.\n",
            "    masked_nll = per_token_nll * shift_mask\n",
            "    \n",
            "    # 7. Get total NLL per sentence by summing\n",
            "    # This is the total Negative Log-Likelihood\n",
            "    sentence_nll = masked_nll.sum(dim=1) # Shape (B,)\n",
            "\n",
            "    # 8. Convert to score\n",
            "    # The score for beam search is the log-probability, which is -NLL.\n",
            "    # A *higher* score is better.\n",
            "    scores_tensor = -sentence_nll\n",
            "    \n",
            "    # Return a simple list of floats\n",
            "    return scores_tensor.cpu().tolist()\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.11"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}