{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import ctc_decoder, cuda_ctc_decoder\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_path = \"/data2/brain2text/lm/languageModel/\"\n",
    "lexicon_phonemes_file = f\"{language_model_path}lexicon_phonemes.txt\"\n",
    "units_txt_file_pytorch = f\"{language_model_path}units_pytorch.txt\"\n",
    "\n",
    "units_txt_file_pytorch_char = f\"{language_model_path}units_pytorch_character.txt\"\n",
    "lexicon_char_file= f\"{language_model_path}lexicon_char.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "val_transcripts = pd.read_pickle(\"/data2/brain2text/b2t_24/transcripts_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n",
      "[Trie] Trie label number reached limit: 6\n"
     ]
    }
   ],
   "source": [
    "decoder_phoneme = ctc_decoder(tokens=units_txt_file_pytorch, lexicon=lexicon_phonemes_file, \n",
    "                       beam_size=50, nbest=20, lm=None)\n",
    "decoder_char = ctc_decoder(tokens=units_txt_file_pytorch_char, lexicon=lexicon_char_file, \n",
    "                       beam_size=50, nbest=20, lm=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import download_pretrained_files\n",
    "\n",
    "files = download_pretrained_files(\"librispeech-4-gram\")\n",
    "\n",
    "decoder_char_lm = ctc_decoder(tokens=units_txt_file_pytorch_char, lexicon=lexicon_char_file, \n",
    "                       beam_size=50, nbest=20, lm=files.lm, lm_weight=3, word_score=-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i say that but yet i know too\n",
      "aye sci that butt yet eye nau thuy\n",
      "i see that bit yet i know too\n",
      "i see that but yet i know too\n"
     ]
    }
   ],
   "source": [
    "model_logits_phone = np.load(\"/data2/brain2text/b2t_24/logits/tm_transformer_b2t_24+25_large_wide/logits_val.npz\")\n",
    "model_logits_char = np.load(\"/data2/brain2text/b2t_24/logits/tm_transformer_combined_lw_char/logits_val.npz\")\n",
    "\n",
    "idx = 200\n",
    "print(val_transcripts[idx])\n",
    "\n",
    "\n",
    "single_trial_logits_phone = torch.as_tensor(model_logits_phone[f'arr_{idx}']).float().unsqueeze(0)\n",
    "beam_search_phoneme = decoder_phoneme(single_trial_logits_phone)\n",
    "beam_search_transcript_phoneme= \" \".join(beam_search_phoneme[0][0].words).strip()\n",
    "print(beam_search_transcript_phoneme)\n",
    "\n",
    "single_trial_logits_char = torch.as_tensor(model_logits_char[f'arr_{idx}']).float().unsqueeze(0)\n",
    "beam_search_char = decoder_char(single_trial_logits_char)\n",
    "beam_search_transcript_char = \" \".join(beam_search_char[0][0].words).strip()\n",
    "print(beam_search_transcript_char)\n",
    "\n",
    "beam_search_char_lm = decoder_char_lm(single_trial_logits_char)\n",
    "beam_search_transcript_char_lm = \" \".join(beam_search_char_lm[0][0].words).strip()\n",
    "print(beam_search_transcript_char_lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:  let's see about getting parents more involved\n",
      "\n",
      "\n",
      "that's seery about getting pinning mohr involved\n",
      "that's seery about getting pinning morr involved\n",
      "that's seery about getting pinning more involved\n",
      "that's seery about getting pinning mor involved\n",
      "that's seery about getting pinning mohr involve\n",
      "that's seery about getting pinning morr involve\n",
      "that's seery about getting pinning mor involve\n",
      "that's seery about getting pinning more involve\n",
      "that's seery about getting penning morr involved\n",
      "that's seery about getting penning more involved\n",
      "\n",
      "\n",
      "that's hey about get counting more involved\n",
      "that's hey about gets counting more involved\n",
      "that's se about get counting more involved\n",
      "that's see about get counting more involved\n",
      "that's se about gets counting more involved\n",
      "that's hey about getts counting more involved\n",
      "that's see about gets counting more involved\n",
      "that's hey about get touting more involved\n",
      "that's hey about get counting more involve d\n",
      "that's hey about gets counting more involve d\n"
     ]
    }
   ],
   "source": [
    "model_logits_phone = np.load(\"/data2/brain2text/b2t_24/logits/tm_transformer_b2t_24+25_large_wide/logits_val.npz\")\n",
    "model_logits_char = np.load(\"/data2/brain2text/b2t_24/logits/tm_transformer_combined_lw_char/logits_val.npz\")\n",
    "\n",
    "idx = 250\n",
    "print(\"Transcript: \", val_transcripts[idx])\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "single_trial_logits_phone = torch.as_tensor(model_logits_phone[f'arr_{idx}']).float().unsqueeze(0)\n",
    "beam_search_phoneme = decoder_phoneme(single_trial_logits_phone)\n",
    "for i in range(20):\n",
    "    beam_search_transcript_phoneme= \" \".join(beam_search_phoneme[0][i].words).strip()\n",
    "    print(beam_search_transcript_phoneme)\n",
    "\n",
    "print(\"\\n\")\n",
    "single_trial_logits_char = torch.as_tensor(model_logits_char[f'arr_{idx}']).float().unsqueeze(0)\n",
    "beam_search_char = decoder_char(single_trial_logits_char)\n",
    "for i in range(20):\n",
    "    beam_search_transcript_char = \" \".join(beam_search_char[0][i].words).strip()\n",
    "    print(beam_search_transcript_char)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "single_trial_logits_char = torch.as_tensor(model_logits_char[f'arr_{idx}']).float().unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013104915618896484\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 61, 41])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_trial_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40,  2, 14, 31, 12, 40]), words=['VOID', 'AFTER'], score=522.8263626098633, timesteps=tensor([ 0, 10, 12, 14, 17, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 18, 40,  2, 14, 31, 12, 40]), words=['VOID', 'E.', 'AFTER'], score=520.3375873565674, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 18, 40,  2, 14, 31, 12, 40]), words=['VOID', 'E', 'AFTER'], score=520.3375873565674, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 18, 40,  2, 14, 31, 12, 40]), words=['VOID', 'EE', 'AFTER'], score=520.3375873565674, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40,  2, 14, 31, 12, 23, 34, 23, 40]), words=['VOID', 'AFTERNOON'], score=519.0685338973999, timesteps=tensor([ 0, 10, 12, 14, 17, 22, 24, 25, 27, 28, 29, 30, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'EAU', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'OHH', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'AU', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'O.', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'EAUX', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40,  2, 14, 31, 12, 23, 34, 23, 38, 40]), words=['VOID', 'AFTERNOONS'], score=513.4793133735657, timesteps=tensor([ 0, 10, 12, 14, 17, 22, 24, 25, 27, 28, 29, 30, 31, 32],\n",
      "       dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40,  2, 14, 31, 12, 23, 34, 23, 38, 40]), words=['VOID', \"AFTERNOON'S\"], score=513.4793133735657, timesteps=tensor([ 0, 10, 12, 14, 17, 22, 24, 25, 27, 28, 29, 30, 31, 32],\n",
      "       dtype=torch.int32))]\n"
     ]
    }
   ],
   "source": [
    "decoder.decode_begin()\n",
    "for t in range(32):\n",
    "    decoder.decode_step(single_trial_logits[0, t:t + 1, :])\n",
    "    \n",
    "decoder.decode_end()\n",
    "beam_search_result_inc = decoder.get_final_hypothesis()\n",
    "print(beam_search_result_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 12, 8, 16, 5, 13, 19, 7, 19, 3, 2, 4, 11, 19, 8, 14, 18, 2, 13, 17, 14, 18, 5, 1, 6, 9)\n",
      "Score 97.032\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def make_new_beam():\n",
    "  fn = lambda : (NEG_INF, NEG_INF)\n",
    "  return collections.defaultdict(fn)\n",
    "\n",
    "def logsumexp(*args):\n",
    "  \"\"\"\n",
    "  Stable log sum exp.\n",
    "  (Optimized version)\n",
    "  \"\"\"\n",
    "  if not args:\n",
    "      return NEG_INF\n",
    "\n",
    "  # <-- 2. Optimized logsumexp\n",
    "  # Find max in one pass\n",
    "  a_max = args[0]\n",
    "  for a in args[1:]:\n",
    "      if a > a_max:\n",
    "          a_max = a\n",
    "\n",
    "  if a_max == NEG_INF:\n",
    "      return NEG_INF\n",
    "\n",
    "  # Calculate sum in a second pass\n",
    "  lsp_sum = 0.0\n",
    "  for a in args:\n",
    "      lsp_sum += math.exp(a - a_max)\n",
    "      \n",
    "  return a_max + math.log(lsp_sum)\n",
    "\n",
    "def decode(probs, beam_size=100, blank=0):\n",
    "  \"\"\"\n",
    "  Performs inference for the given output probabilities.\n",
    "  Arguments:\n",
    "      probs: The output probabilities (e.g. post-softmax) for each\n",
    "        time step. Should be an array of shape (time x output dim).\n",
    "      beam_size (int): Size of the beam to use during inference.\n",
    "      blank (int): Index of the CTC blank label.\n",
    "  Returns the output label sequence and the corresponding negative\n",
    "  log-likelihood estimated by the decoder.\n",
    "  \"\"\"\n",
    "  T, S = probs.shape\n",
    "  # Convert to log-probabilities once at the beginning\n",
    "  log_probs = np.log(probs)\n",
    "\n",
    "  # Elements in the beam are (prefix, (p_blank, p_no_blank))\n",
    "  # Initialize the beam with the empty sequence, a probability of\n",
    "  # 1 for ending in blank and zero for ending in non-blank\n",
    "  # (in log space).\n",
    "  beam = [(tuple(), (0.0, NEG_INF))]\n",
    "\n",
    "  for t in range(T): # Loop over time\n",
    "\n",
    "    # A default dictionary to store the next step candidates.\n",
    "    next_beam = make_new_beam()\n",
    "\n",
    "    # <-- 3. Pre-slice probabilities for this time step\n",
    "    log_probs_t = log_probs[t, :]\n",
    "\n",
    "    for s in range(S): # Loop over vocab\n",
    "      p = log_probs_t[s] # Use the pre-sliced array\n",
    "\n",
    "      # The variables p_b and p_nb are respectively the\n",
    "      # probabilities for the prefix given that it ends in a\n",
    "      # blank and does not end in a blank at this time step.\n",
    "      for prefix, (p_b, p_nb) in beam: # Loop over beam\n",
    "\n",
    "        # If we propose a blank the prefix doesn't change.\n",
    "        # Only the probability of ending in blank gets updated.\n",
    "        if s == blank:\n",
    "          n_p_b, n_p_nb = next_beam[prefix]\n",
    "          n_p_b = logsumexp(n_p_b, p_b + p, p_nb + p)\n",
    "          next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "          continue\n",
    "\n",
    "        # Extend the prefix by the new character s and add it to\n",
    "        # the beam. Only the probability of not ending in blank\n",
    "        # gets updated.\n",
    "        end_t = prefix[-1] if prefix else None\n",
    "        n_prefix = prefix + (s,)\n",
    "        n_p_b, n_p_nb = next_beam[n_prefix]\n",
    "        if s != end_t:\n",
    "          n_p_nb = logsumexp(n_p_nb, p_b + p, p_nb + p)\n",
    "        else:\n",
    "          # We don't include the previous probability of not ending\n",
    "          # in blank (p_nb) if s is repeated at the end. The CTC\n",
    "          # algorithm merges characters not separated by a blank.\n",
    "          n_p_nb = logsumexp(n_p_nb, p_b + p)\n",
    "          \n",
    "        next_beam[n_prefix] = (n_p_b, n_p_nb)\n",
    "\n",
    "        # If s is repeated at the end we also update the unchanged\n",
    "        # prefix. This is the merging case.\n",
    "        if s == end_t:\n",
    "          n_p_b, n_p_nb = next_beam[prefix]\n",
    "          n_p_nb = logsumexp(n_p_nb, p_nb + p)\n",
    "          next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "\n",
    "    # <-- 4. MAJOR OPTIMIZATION: Use heapq.nlargest instead of full sort\n",
    "    # Original:\n",
    "    # beam = sorted(next_beam.items(),\n",
    "    #         key=lambda x : logsumexp(*x[1]),\n",
    "    #         reverse=True)\n",
    "    # beam = beam[:beam_size]\n",
    "    \n",
    "    # Optimized:\n",
    "    beam_items = next_beam.items()\n",
    "    beam = heapq.nlargest(beam_size,\n",
    "                          beam_items,\n",
    "                          key=lambda x: logsumexp(*x[1]))\n",
    "\n",
    "  # Final selection\n",
    "  best = beam[0]\n",
    "  return best[0], -logsumexp(*best[1])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  np.random.seed(3)\n",
    "\n",
    "  time = 50\n",
    "  output_dim = 20\n",
    "\n",
    "  probs = np.random.rand(time, output_dim)\n",
    "  probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "\n",
    "  labels, score = decode(probs)\n",
    "  print(labels)\n",
    "  print(\"Score {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "logits = softmax(model_logits[f'arr_750}'],axis=1)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6410787105560303\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "labels, probs = decode(logits)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
