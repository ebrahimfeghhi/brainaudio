{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.decoder import ctc_decoder, cuda_ctc_decoder\n",
    "import numpy as np \n",
    "import torch\n",
    "from torch.nn.functional import softmax, log_softmax\n",
    "from brainaudio.inference.inference_utils import _cer_and_wer\n",
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_path = \"/data2/brain2text/lm/languageModel/\"\n",
    "lexicon_phonemes_file = f\"{language_model_path}lexicon_phonemes.txt\"\n",
    "units_txt_file_pytorch = f\"{language_model_path}units_pytorch.txt\"\n",
    "\n",
    "units_txt_file_pytorch_char = f\"{language_model_path}units_pytorch_character.txt\"\n",
    "lexicon_char_file= f\"{language_model_path}lexicon_char.txt\"\n",
    "\n",
    "imagineville_vocab = \"/data2/brain2text/lm/vocab_lower_100k_pytorch.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(units_txt_file_pytorch_char, 'r') as file:\n",
    "    character_units = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "val_transcripts = pd.read_pickle(\"/data2/brain2text/b2t_24/transcripts_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home3/ebrahim2/brainaudio/.venv/lib/python3.12/site-packages/torchaudio/models/decoder/_ctc_decoder.py:557: UserWarning: torchaudio.utils.download.download_asset has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  lexicon_file = download_asset(files.lexicon)\n",
      "/home3/ebrahim2/brainaudio/.venv/lib/python3.12/site-packages/torchaudio/models/decoder/_ctc_decoder.py:558: UserWarning: torchaudio.utils.download.download_asset has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  tokens_file = download_asset(files.tokens)\n",
      "/home3/ebrahim2/brainaudio/.venv/lib/python3.12/site-packages/torchaudio/models/decoder/_ctc_decoder.py:560: UserWarning: torchaudio.utils.download.download_asset has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  lm_file = download_asset(files.lm)\n"
     ]
    }
   ],
   "source": [
    "from torchaudio.models.decoder import download_pretrained_files\n",
    "\n",
    "files = download_pretrained_files(\"librispeech-4-gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_char = ctc_decoder(tokens=units_txt_file_pytorch_char, lexicon=imagineville_vocab, \n",
    "                       beam_size=100, nbest=1, lm=\"/data2/brain2text/lm/lm_dec19_huge_4gram.arpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logits_char = np.load(\"/data2/brain2text/b2t_24/logits/tm_transformer_combined_lw_char/logits_val.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:  friday afternoon at five thirty\n",
      "\n",
      "\n",
      "vitt atria fave\n",
      "vitt eternit fave\n",
      "vitt afferent fave\n",
      "vitt ator fave\n",
      "vitt aren fave\n",
      "vitt datron fave\n",
      "vidor ator fave\n",
      "vidor afferent fave\n",
      "ahranat fave\n",
      "vitt ator a fave\n",
      "vitt afferent a fave\n",
      "vitt aren a fave\n",
      "vitt datron a fave\n",
      "vitt atria five\n",
      "vitt eternit five\n",
      "videotron at five\n",
      "ahranat five\n",
      "vitt afferent five\n",
      "vitt ator five\n",
      "vitt aren five\n",
      "vitt datron five\n",
      "vitt ator athie\n",
      "ator at five\n",
      "vidor ator five\n",
      "vitt afferent athie\n",
      "vitt ator attie\n",
      "afferent at five\n",
      "vidor afferent five\n",
      "vitt atria frie\n",
      "vitt aren athie\n",
      "vitt datron athie\n",
      "aren at five\n",
      "vitt afferent attie\n",
      "datron at five\n",
      "vitt eternit frie\n",
      "vitt aren attie\n",
      "vitt datron attie\n",
      "vidor ator athie\n",
      "videos ator five\n",
      "videotron five\n",
      "vidor afferent athie\n",
      "vidor ator attie\n",
      "vitt ator at five\n",
      "videos afferent five\n",
      "atria five\n",
      "vidor afferent attie\n",
      "vitt afferent at five\n",
      "eternit five\n",
      "vitt afferent frie\n",
      "vitt aren at five\n",
      "videos ator athie\n",
      "vitt datron at five\n",
      "vitt ator frie\n",
      "videos afferent athie\n",
      "videos ator attie\n",
      "vidor ator at five\n",
      "therat five\n",
      "vitt aren frie\n",
      "videos afferent attie\n",
      "vitt datron frie\n",
      "afferent five\n",
      "vidor afferent at five\n",
      "treon at five\n",
      "ator five\n",
      "arnn at five\n",
      "aerien at five\n",
      "videos ator at five\n",
      "efron at five\n",
      "aren five\n",
      "videos afferent at five\n",
      "datron five\n",
      "toren at five\n",
      "teran at five\n",
      "atria at five\n",
      "arns at five\n",
      "vitt arnatt five\n",
      "ahren at five\n",
      "tran at five\n",
      "daffern at five\n",
      "ahranat frie\n",
      "i ator at five\n",
      "arons at five\n",
      "freon at five\n",
      "i afferent at five\n",
      "vitt ator a frie\n",
      "aro at five\n",
      "i aren at five\n",
      "i datron at five\n",
      "hefter at five\n",
      "efferent at five\n",
      "wiater at five\n",
      "arant at five\n",
      "treon five\n",
      "areta five\n",
      "aretha five\n",
      "arnn five\n",
      "effron at five\n",
      "aerien five\n",
      "armon at five\n",
      "haren at five\n",
      "trnka five\n",
      "aherin at five\n",
      "erena five\n",
      "aerts at five\n",
      "efron five\n",
      "eatery at five\n",
      "arone at five\n",
      "after five\n",
      "troha five\n",
      "eron at five\n",
      "aeriens at five\n",
      "arora five\n",
      "vidartes at five\n",
      "toran at five\n",
      "ferra five\n",
      "daren at five\n",
      "arnst five\n",
      "toren five\n",
      "teran five\n",
      "arwen at five\n",
      "arns five\n",
      "sater at five\n",
      "vidmar at five\n",
      "ferm at five\n",
      "ahren five\n",
      "i treon at five\n",
      "aretta five\n",
      "tran five\n",
      "altron at five\n",
      "daffron at five\n",
      "daffern five\n",
      "arant five\n",
      "atari at five\n",
      "i arnn at five\n",
      "satre at five\n",
      "armenta five\n",
      "arons five\n",
      "frein at five\n",
      "efferent five\n",
      "atorino at five\n",
      "vietor at five\n",
      "i aerien at five\n",
      "vidor at five\n",
      "freon five\n",
      "after a five\n",
      "yater at five\n",
      "i efron at five\n",
      "arner at five\n",
      "aro five\n",
      "arnatt five\n",
      "hefter five\n",
      "ferenc at five\n",
      "foran at five\n",
      "fermenta five\n",
      "freons at five\n",
      "traina five\n",
      "arman at five\n",
      "teri at five\n",
      "i toren at five\n",
      "arnst at five\n",
      "i teran at five\n",
      "deater at five\n",
      "i atria at five\n",
      "i arns at five\n",
      "i ahren at five\n",
      "i tran at five\n",
      "i daffern at five\n",
      "i arons at five\n",
      "then at five\n",
      "i freon at five\n",
      "i aro at five\n",
      "i hefter at five\n",
      "i efferent at five\n",
      "at five\n",
      "i aerts at five\n",
      "iida ator five\n",
      "after a fave\n",
      "after fave\n",
      "vitt ator a five\n",
      "vitt afferent a five\n",
      "vitt aren a five\n",
      "vitt datron a five\n",
      "vidor ator a five\n",
      "vidor afferent a five\n",
      "and after five\n",
      "videos ator a five\n",
      "videos afferent a five\n",
      "vitt after a five\n",
      "videotron a five\n",
      "vidor after a five\n",
      "videotron at five p\n",
      "for five\n",
      "videos after a five\n",
      "and after a five\n",
      "ator at five p\n",
      "iida after a five\n",
      "afferent at five p\n",
      "aren at five p\n",
      "datron at five p\n",
      "vitt ator at five p\n",
      "vitt afferent at five p\n",
      "vitt aren at five p\n",
      "vitt datron at five p\n",
      "vidor ator at five p\n",
      "vidor afferent at five p\n",
      "treon at five p\n",
      "arnn at five p\n",
      "aerien at five p\n",
      "videos ator at five p\n",
      "efron at five p\n",
      "videos afferent at five p\n",
      "toren at five p\n",
      "teran at five p\n",
      "atria at five p\n",
      "arns at five p\n",
      "at five p\n",
      "ahren at five p\n",
      "tran at five p\n",
      "daffern at five p\n",
      "i ator at five p\n",
      "arons at five p\n",
      "freon at five p\n",
      "i afferent at five p\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx = 400\n",
    "T = 5\n",
    "print(\"Transcript: \", val_transcripts[idx])\n",
    "print(\"\\n\")\n",
    "\n",
    "# 1. Get the original logits\n",
    "single_trial_logits_char = torch.as_tensor(model_logits_char[f'arr_{idx}']).float().unsqueeze(0)\n",
    "\n",
    "# 4. Perform beam search on the NOISY logits (and apply temperature)\n",
    "beam_search_char = decoder_char(single_trial_logits_char[0:1, :50] / T)\n",
    "\n",
    "for i in range(len(beam_search_char[0])):\n",
    "    beam_search_transcript_char = \" \".join(beam_search_char[0][i].words).strip()\n",
    "    print(beam_search_transcript_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_units[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1672609/1849404952.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax(single_trial_logits_char[0:1, 8]/100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0342, 0.0301, 0.0239, 0.0240, 0.0240, 0.0239, 0.0279, 0.0303, 0.0326,\n",
       "         0.0309, 0.0321, 0.0303, 0.0341, 0.0301, 0.0307, 0.0314, 0.0294, 0.0300,\n",
       "         0.0314, 0.0319, 0.0309, 0.0297, 0.0314, 0.0285, 0.0334, 0.0308, 0.0308,\n",
       "         0.0289, 0.0380, 0.0347, 0.0286, 0.0300, 0.0309]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 100\n",
    "from torch.nn.functional import softmax\n",
    "softmax(single_trial_logits_char[0:1, 8]/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_arr = []\n",
    "pred_arr = []\n",
    "for idx in range(880):\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)\n",
    "    single_trial_logits_char = torch.as_tensor(model_logits_char[f'arr_{idx}']).float().unsqueeze(0)\n",
    "    beam_search_char = decoder_char(single_trial_logits_char)\n",
    "    beam_search_transcript_char = \" \".join(beam_search_char[0][0].words).strip()\n",
    "    ground_truth_sentence = val_transcripts[idx]\n",
    "    ground_truth_arr.append(ground_truth_sentence)\n",
    "    pred_arr.append(beam_search_transcript_char)\n",
    "    \n",
    "cer, wer, wer_sent = _cer_and_wer(pred_arr, ground_truth_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['-', '|', 'i', 'w', 'a']\n",
      "1 ['-', '|', 'i', 'w', 'a']\n",
      "2 ['-', '|', 'i', 'w', 'a']\n",
      "3 ['-', '|', 'i', 'a', 'w']\n",
      "4 ['-', 'a', '|', 'i', 'o']\n",
      "5 ['-', 'a', 'i', '|', 'o']\n",
      "6 ['-', 'a', 'i', '|', 'o']\n",
      "7 ['-', 'a', 'i', 'o', '|']\n",
      "8 ['v', 'w', '-', 'f', 'r']\n",
      "9 ['-', 'v', 'i', 'w', 'r']\n",
      "10 ['i', 'e', 'a', 'o', '-']\n",
      "11 ['-', 'i', 'd', 'e', 'r']\n",
      "12 ['d', '-', 't', 'e', 'i']\n",
      "13 ['d', 'e', '-', 't', 'i']\n",
      "14 ['-', 'd', 'e', 't', 'a']\n",
      "15 ['a', '-', 'o', 't', 'y']\n",
      "16 ['-', 'a', 'y', '|', 'o']\n",
      "17 ['|', '-', 'a', 'y', 's']\n",
      "18 ['|', 'a', '-', 's', 'y']\n",
      "19 ['-', '|', 'h', 'd', 'r']\n",
      "20 ['a', 'e', '-', 'f', 'h']\n",
      "21 ['f', 'a', '-', 'e', 't']\n",
      "22 ['f', '-', 't', 'v', 'c']\n",
      "23 ['t', '-', 'f', 'h', 'v']\n",
      "24 ['t', 'e', '-', 'o', 'f']\n",
      "25 ['e', 'r', 'o', '-', 'a']\n",
      "26 ['r', 'n', '-', 'o', 'e']\n",
      "27 ['r', '-', 'e', 'o', 'n']\n",
      "28 ['e', '-', 'o', 'n', 'a']\n",
      "29 ['n', '-', 'e', 'o', 't']\n",
      "30 ['n', '-', 't', 's', 'k']\n",
      "31 ['|', '-', 't', 's', 'n']\n",
      "32 ['|', '-', 'h', 't', 's']\n",
      "33 ['-', '|', 'h', 'a', 't']\n",
      "34 ['a', 'o', 'i', 'u', 'e']\n",
      "35 ['t', '-', 'a', 's', 'd']\n",
      "36 ['t', '-', '|', 'd', 's']\n",
      "37 ['|', '-', 't', 'd', 's']\n",
      "38 ['|', '-', 'a', 'f', 'i']\n",
      "39 ['f', 'h', 'p', 't', 'l']\n",
      "40 ['-', 'f', 'i', 'r', 'w']\n",
      "41 ['i', '-', 'a', 'e', 'u']\n",
      "42 ['i', '-', 'v', 'a', 'e']\n",
      "43 ['v', '-', 'l', 'e', 'f']\n",
      "44 ['e', '-', 'v', 'o', 'l']\n",
      "45 ['e', '|', '-', 'l', 'r']\n",
      "46 ['|', 'e', '-', 'a', 'r']\n",
      "47 ['-', '|', 'p', 'e', 's']\n",
      "48 ['p', 'f', 'c', 'l', 't']\n",
      "49 ['l', '-', 'h', 'e', 'r']\n",
      "50 ['l', '-', 'a', 'e', 'o']\n",
      "51 ['a', 'o', 'e', '-', 'y']\n",
      "52 ['y', '-', 'a', 'c', 'e']\n",
      "53 ['y', '-', 'e', 'c', 't']\n",
      "54 ['y', 'e', '-', 'a', 'r']\n",
      "55 ['y', '-', '|', 'r', 's']\n",
      "56 ['|', 's', '-', 'y', 'r']\n",
      "57 ['|', 's', 'a', '-', 'y']\n",
      "58 ['|', '-', 'a', 's', 'e']\n",
      "59 ['-', '|', 'e', 'h', 'a']\n",
      "60 ['-', '|', 't', 'h', 'e']\n"
     ]
    }
   ],
   "source": [
    "values, indices = torch.topk(single_trial_logits_char.squeeze(), dim=-1, k=5)\n",
    "for time, index in enumerate(indices):\n",
    "    print(time, [character_units[i] for i in index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.1258)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(single_trial_logits_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sil score: -0.1, lm weight: 2.0, word penalty: 0, beam size: 50, wer: 0.26004728132387706\n",
      "sil score: -0.2, lm weight: 2.0, word penalty: 0, beam size: 50, wer: 0.259683578832515\n",
      "sil score: -0.3, lm weight: 2.0, word penalty: 0, beam size: 50, wer: 0.26041098381523914\n"
     ]
    }
   ],
   "source": [
    "lm_weights = [2.0]   \n",
    "word_penalty = [0]\n",
    "sil_score = [-0.1, -0.2, -0.3]\n",
    "beam_size = [50]\n",
    "\n",
    "wer_dict = {}\n",
    "wer_dict['sil'] = []\n",
    "wer_dict['lmw'] = []\n",
    "wer_dict['wp'] = []\n",
    "wer_dict['bs'] = []\n",
    "wer_dict['wer'] = []\n",
    "\n",
    "\n",
    "for sil in sil_score:\n",
    "    for wp in word_penalty:\n",
    "      for lmw in lm_weights:\n",
    "          for bs in beam_size:\n",
    "          \n",
    "            decoder_char_lm = ctc_decoder(tokens=units_txt_file_pytorch_char, lexicon=lexicon_char_file, \n",
    "                                beam_size=bs, nbest=50, lm=files.lm, lm_weight=lmw, word_score=wp, sil_score=sil)\n",
    "\n",
    "            ground_truth_arr = []\n",
    "            pred_arr = []\n",
    "        \n",
    "            for idx in range(880):\n",
    "                #if idx % 100 == 0:\n",
    "                #    print(idx)\n",
    "                single_trial_logits_char = torch.as_tensor(model_logits_char[f'arr_{idx}']).float().unsqueeze(0)\n",
    "                beam_search_char = decoder_char_lm(single_trial_logits_char)\n",
    "                beam_search_transcript_char = \" \".join(beam_search_char[0][0].words).strip()\n",
    "                ground_truth_sentence = val_transcripts[idx]\n",
    "                ground_truth_arr.append(ground_truth_sentence)\n",
    "                pred_arr.append(beam_search_transcript_char)\n",
    "                \n",
    "            cer, wer, wer_sent = _cer_and_wer(pred_arr, ground_truth_arr)\n",
    "            \n",
    "            \n",
    "            wer_dict['sil'].append(sil)\n",
    "            wer_dict['lmw'].append(lmw)\n",
    "            wer_dict['wp'].append(wp)\n",
    "            wer_dict['bs'].append(bs)\n",
    "            wer_dict['wer'].append(wer)\n",
    "            \n",
    "            \n",
    "            print(f\"sil score: {sil}, lm weight: {lmw}, word penalty: {wp}, beam size: {bs}, wer: {wer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['void after at five flurry', 'void after at five played', 'void after at five plead', 'void after at five platy', 'void after at five purdie', 'void after at five purdy', 'void after at five ploughed', 'void after at five plowed', 'void after at five lady', 'void after at five leedy', 'void after at five leidy', 'void after at fly purdie', 'void after at fly purdy', 'void after at flye purdie', 'void after at flye purdy', 'void after at flye ploughed', 'void after at flye plowed', 'void after at fly plowed', 'void after at fly ploughed', 'void after at five play', 'void after at five plea', 'void e after at five purdy', 'void ee after at five purdie', 'void e. after at five purdy', 'void e after at five purdie', 'void ee after at five purdy', 'void e. after at five purdie', 'void after at fly lady', 'void after at flye lady', 'void e. after at five plowed', 'void e. after at five ploughed', 'void e after at five plowed', 'void ee after at five plowed', 'void ee after at five ploughed', 'void e after at five ploughed', \"void after at five play's\", 'void after at five plays', 'void after at fly leedy', 'void after at flye leidy', 'void after at flye leedy', 'void after at fly leidy', 'void after at five lowy', 'void after at five loewy', 'void after at five lowey', 'void after at five pleas', 'void after at five please', 'void after at five plese', 'void after at five plow', 'void after at five plough', 'void after at flye play', 'void after at fly play', 'void after at fly plea', 'void after at flye plea', 'void after at five flow', 'void after at five flowe', 'void after at five floe', 'void after at five flo', 'void after at five fluhr', 'void after at five fleur', 'void after at five ploughs', 'void after at five plows', 'void after at flye plays', \"void after at flye play's\", \"void after at fly play's\", 'void after at fly plays', 'void e. after at five play', 'void ee after at five play', 'void e after at five play', 'void after at five ply', 'void after at fly loewy', 'void after at fly lowey', 'void after at flye loewy', 'void after at flye lowey', 'void after at fly lowy', 'void after at flye lowy', 'void after at fly please', 'void after at flye please', 'void after at fly plese', 'void after at fly pleas', 'void after at flye pleas', 'void after at flye plese', 'void e. after at five plea', 'void ee after at five plea', 'void e after at five plea', 'void after at five flood', 'void after at five fludd', 'void after at five flud', 'void after at five plate', 'void after at five plait', 'void after at fly plough', 'void after at flye plow', 'void after at fly plow', 'void after at flye plough', 'void after at five flay', 'void after at five flee', 'void after at five flea', 'void e. after at five plays', 'void ee after at five plays', 'void e after at five plays', \"void e. after at five play's\", \"void e after at five play's\", \"void ee after at five play's\", 'void after at live play', 'void e after at five lowey', 'void ee after at five lowey', 'void ee after at five lowy', 'void ee after at five loewy', 'void e after at five loewy', 'void e after at five lowy', 'void e. after at five lowy', 'void e. after at five loewy', 'void e. after at five lowey', 'void after at five load', 'void after at five lode', 'void ee after at five please', 'void ee after at five pleas', 'void e after at five please', 'void e after at five pleas', 'void e. after at five please', 'void ee after at five plese', 'void e after at five plese', 'void e. after at five plese', 'void e. after at five pleas', 'void after at fly flo', 'void after at fly floe', 'void after at flye flo', 'void after at fly flowe', 'void after at fly flow', 'void after at flye floe', 'void after at flye flow', 'void after at flye flowe', 'void after at live plea', 'void afternoon at five play', 'void afternoon at five plea', 'void ee after at fly play', 'void ee after at flye play', 'void e after at flye play', 'void e after at fly play', 'void e. after at flye play', 'void e. after at fly play', 'void after at five lloyd', 'void after at five loyd', 'void e. after at flye plea', 'void e after at flye plea', 'void ee after at flye plea', 'void e. after at fly plea', 'void e after at fly plea', 'void ee after at fly plea', 'void after at flye flud', 'void after at flye fludd', 'void after at fly fludd', 'void after at fly flud', 'void after at flye flood', 'void after at fly flood', 'void after at live lowy', 'void after at live loewy', 'void after at live lowey', 'void e after at five floe', 'void e. after at five floe', 'void ee after at five flo', 'void ee after at five floe', 'void e. after at five flow', 'void ee after at five flow', 'void e. after at five flo', 'void ee after at five flowe', 'void e. after at five flowe', 'void e after at five flowe', 'void e after at five flo', 'void e after at five flow', 'void after at live pleas', 'void after at live please', 'void after at live plese', 'void after at five lade', 'void after at five laid', 'void afternoon at five lowey', 'void afternoon at five lowy', 'void afternoon at five loewy', 'void afternoon at five pleas', 'void afternoon at five plese', 'void afternoon at five please', 'void after at five leed', 'void after at fly flee', 'void after at flye flee', 'void after at flye flea', 'void after at fly flea', 'void after at five player']\n"
     ]
    }
   ],
   "source": [
    "phoneme_n_best_list = []\n",
    "for i in range(len(beam_search_phoneme[0])):\n",
    "    beam_search_transcript_phoneme= \" \".join(beam_search_phoneme[0][i].words).strip()\n",
    "    phoneme_n_best_list.append(beam_search_transcript_phoneme)\n",
    "    \n",
    "print(phoneme_n_best_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vida after at five play', 'vide after at five play', 'vida ater at five play', 'vida after at five plays', 'video after at five play', 'vide ater at five play', 'vida after at five player', 'vide after at five plays', 'vida after en at five play', 'vida after at five ploy', 'vida after n at five play', 'vida after at five pla', 'vida after on at five play', 'vida ator at five play', 'vida after at five playa', 'vida after hat five play', 'vide after at five player', 'vida afferent at five play', 'vida ater at five plays', 'vida atra at five play', 'video ater at five play', 'vida aren at five play', 'vida arn at five play', 'vida datron at five play', 'veda after at five play', 'vide after en at five play', 'vide after at five ploy', 'vide after n at five play', 'video after at five plays', 'vida aron at five play', 'vida tern at five play', 'vida after at five flay', 'vida fern at five play', 'vida aten at five play', 'vide after at five pla', 'vide after on at five play', 'vida ater at five player', 'vide ator at five play', 'vide after at five playa', 'vida after ent at five play', 'vida after at five platy', 'vide after hat five play', 'via after at five play', 'vie after at five play', 'vide afferent at five play', 'vied after at five play', 'vida rafter at five play', 'vide ater at five plays', 'vide atra at five play', 'vida ater en at five play', 'vida ater at five ploy', 'vide aren at five play', 'video after at five player', 'vida ater n at five play', 'vide arn at five play', 'vida aer at five play', 'vide datron at five play', 'vida after an at five play', 'vida eater at five play', 'vida tron at five play', 'vida afro at five play', 'vi after at five play', 'vida after at five ply', 'vida after at five clay', 'vida ater at five pla', 'vida after en at five plays', 'vida after at five ploys', 'vida ater on at five play', 'vida after n at five plays', 'vide aron at five play', 'video after en at five play', 'video after at five ploy', 'vida aero at five play', 'vide tern at five play', 'vida after in at five play', 'video after n at five play', 'vida ater at five playa', 'vida arent at five play', 'vide after at five flay', 'vida after at five place', 'vida after at five play s', 'vida after at five lay', 'vide fern at five play', 'vide aten at five play', 'vida treon at five play', 'vida ater hat five play', 'vita after at five play', 'vida after eon at five play', 'vida hater at five play', 'vide ater at five player', 'vida arnn at five play', 'vide after ent at five play', 'vide after at five platy', 'vida after at five plas', 'vida ahern at five play', 'vida after at five plate', 'viet after at five play', 'vida after on at five plays', 'video after at five pla', 'vida aerien at five play', 'vida ator at five plays', 'vida afternoon at five play', 'video ator at five play', 'video after at five playa', 'vida after e at five play', 'vida affront at five play', 'vida after hat five plays', 'vida after ons at five play', 'video after hat five play', 'vide rafter at five play', 'veda ater at five play', 'ida after at five play', 'vide ater at five ploy', 'vida afferent at five plays', 'vida after en at five player', 'vida after a five play', 'video afferent at five play', 'vide aer at five play', 'vida after n at five player', 'wide after at five play', 'vida after ast five play', 'vide eater at five play', 'vida atra at five plays', 'vida after at five play y', 'video ater at five plays', 'video atra at five play', 'vida aren at five plays', 'vida trent at five play', 'vida arn at five plays', 'vida datron at five plays', 'vida after o at five play', 'vide afro at five play', 'vida ater at five flay', 'vide after at five ply', 'vida after orn at five play', 'veda after at five plays', 'vida atria at five play', 'vida arens at five play', 'vide after at five clay', 'vide ater at five pla', 'vida after at five clayey', 'vida arns at five play', 'vide after en at five plays', 'vide after at five ploys', 'vide after n at five plays', 'vide aero at five play', 'vida after en at five ploy', 'vida after on at five player', 'vide ater at five playa', 'vida after n at five ploy', 'vida after et at five play', 'idea after at five play', 'vida ater ent at five play', 'vida ator at five player', 'vide arent at five play', 'vida after ot five play', 'vida ater at five platy', 'vide after at five place', 'vide after at five play s', 'vida after t at five play', 'vide after at five lay', 'id after at five play', 'vide ater hat five play', 'vida after ant at five play', 'vida after at five laye', 'vida after hat five player', 'vide after eon at five play', 'vida aron at five plays', 'via ater at five play', 'vie ater at five play', 'vida tern at five plays', 'vide hater at five play', 'vida after er at five play', 'vida after it five play', 'vida after at five players', 'vida ater at five play s', 'video after at five play s', 'vida after at five play r', 'vide after at five play y', 'vide after at five players', 'vide ater at five play s', 'vida ater at five play y', 'vida after en at five play s', 'vida after at five ploy s', 'vida after n at five play s', 'vide after at five play r', 'vida fern at five plays', 'vida aten at five plays', 'vide after at five plas', 'vida after at five pla s', 'vide after on at five plays', 'video after at five play y', 'vida ater at five players', 'vide ator at five plays', 'vida after on at five play s', 'vida after ent at five plays', 'vida ator at five play s', 'vida after at five playa s', 'vide after hat five plays', 'vida after hat five play s', 'via after at five plays', 'vie after at five plays', 'vide afferent at five plays', 'vida afferent at five play s', 'vied after at five plays', 'vida rafter at five plays', 'vide atra at five plays', 'vida after at five play a', 'vida ater en at five plays', 'vida ater at five ploys', 'vide aren at five plays', 'vida atra at five play s', 'video after at five players', 'vida ater n at five plays', 'video ater at five play s', 'vide arn at five plays', 'vida aer at five plays', 'vide datron at five plays', 'vida aren at five play s', 'vida after an at five plays', 'vida arn at five play s', 'vida datron at five play s', 'vida eater at five plays', 'vida tron at five plays', 'vida ater at five play r', 'vide ater at five play y', 'veda after at five play s', 'vida after at five plays s', 'vide after en at five play s', 'vida afro at five plays', 'vide after at five ploy s', 'vide after n at five play s', 'vi after at five plays', 'vida after at five clays', 'vida ater at five plas', \"vida after at five play's\", 'vida ater on at five plays', 'vide aron at five plays', 'video after en at five plays', 'video after at five ploys', 'vida aero at five plays', 'vide tern at five plays', 'vida after in at five plays', 'video after n at five plays', 'vida aron at five play s', 'vida arent at five plays', 'vida after at five places', 'vida tern at five play s', 'vida after at five lays', 'vide fern at five plays', 'vide aten at five plays', 'vida treon at five plays', 'vida ater hat five plays', 'vita after at five plays', 'vida after at five flay s', 'vida fern at five play s', 'vida aten at five play s', 'vida after eon at five plays', 'vide after at five pla s', 'video after at five play r', 'vida after en at five play y', 'vida after at five ploy y', 'vida hater at five plays', 'vide ater at five players', 'vida after n at five play y', 'vide after on at five play s', 'vida arnn at five plays', 'vide after ent at five plays', 'vide ator at five play s', 'vida ahern at five plays', 'vide after at five playa s', 'vida after ent at five play s', 'vida after at five plates', 'vida after at five platy s', 'viet after at five plays', 'video after at five plas', 'vida aerien at five plays', 'vide after hat five play s', 'via after at five play s', 'vida afternoon at five plays', 'vie after at five play s', 'video ator at five plays', 'vida after e at five plays', 'vida affront at five plays', 'vide afferent at five play s', 'vida after ons at five plays', 'video after hat five plays', 'vida after at five pla y', 'vide rafter at five plays', 'veda ater at five plays', 'vied after at five play s', 'vide after at five play a', 'vida rafter at five play s', 'ida after at five plays', 'vida after on at five play y', 'vide ater at five ploys', 'vide atra at five play s', 'vida after en at five players', 'vida after a five plays', 'vida ator at five play y', 'video afferent at five plays', 'vida ater en at five play s', 'vide aer at five plays', 'vida ater at five ploy s', 'vida after at five playa y', 'vide aren at five play s', 'vida after n at five players', 'vida ater n at five play s', 'vida after at five player s', 'vide after at five plays s', 'vide after at five player s', 'vida ater at five play a', 'vida ater at five plays s', 'vida after at five play e', 'vida after at five plays a', 'video after at five play a', 'video after at five plays s', 'vida ater at five player s']\n"
     ]
    }
   ],
   "source": [
    "char_n_best_list = []\n",
    "for i in range(len(beam_search_char_lm[0])):\n",
    "    beam_search_transcript_char = \" \".join(beam_search_char_lm[0][i].words).strip()\n",
    "    char_n_best_list.append(beam_search_transcript_char)\n",
    "    \n",
    "print(char_n_best_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "single_trial_logits_char = torch.as_tensor(model_logits_char[f'arr_{idx}']).float().unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013104915618896484\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 61, 41])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_trial_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40,  2, 14, 31, 12, 40]), words=['VOID', 'AFTER'], score=522.8263626098633, timesteps=tensor([ 0, 10, 12, 14, 17, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 18, 40,  2, 14, 31, 12, 40]), words=['VOID', 'E.', 'AFTER'], score=520.3375873565674, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 18, 40,  2, 14, 31, 12, 40]), words=['VOID', 'E', 'AFTER'], score=520.3375873565674, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 18, 40,  2, 14, 31, 12, 40]), words=['VOID', 'EE', 'AFTER'], score=520.3375873565674, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40,  2, 14, 31, 12, 23, 34, 23, 40]), words=['VOID', 'AFTERNOON'], score=519.0685338973999, timesteps=tensor([ 0, 10, 12, 14, 17, 22, 24, 25, 27, 28, 29, 30, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'EAU', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'OHH', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'AU', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'O.', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40, 25, 40,  2, 14, 31, 12, 40]), words=['VOID', 'EAUX', 'AFTER'], score=517.1776781082153, timesteps=tensor([ 0, 10, 12, 14, 16, 17, 18, 22, 24, 25, 27, 31], dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40,  2, 14, 31, 12, 23, 34, 23, 38, 40]), words=['VOID', 'AFTERNOONS'], score=513.4793133735657, timesteps=tensor([ 0, 10, 12, 14, 17, 22, 24, 25, 27, 28, 29, 30, 31, 32],\n",
      "       dtype=torch.int32)), CTCHypothesis(tokens=tensor([40, 35, 26,  9, 40,  2, 14, 31, 12, 23, 34, 23, 38, 40]), words=['VOID', \"AFTERNOON'S\"], score=513.4793133735657, timesteps=tensor([ 0, 10, 12, 14, 17, 22, 24, 25, 27, 28, 29, 30, 31, 32],\n",
      "       dtype=torch.int32))]\n"
     ]
    }
   ],
   "source": [
    "decoder.decode_begin()\n",
    "for t in range(32):\n",
    "    decoder.decode_step(single_trial_logits[0, t:t + 1, :])\n",
    "    \n",
    "decoder.decode_end()\n",
    "beam_search_result_inc = decoder.get_final_hypothesis()\n",
    "print(beam_search_result_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 12, 8, 16, 5, 13, 19, 7, 19, 3, 2, 4, 11, 19, 8, 14, 18, 2, 13, 17, 14, 18, 5, 1, 6, 9)\n",
      "Score 97.032\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def make_new_beam():\n",
    "  fn = lambda : (NEG_INF, NEG_INF)\n",
    "  return collections.defaultdict(fn)\n",
    "\n",
    "def logsumexp(*args):\n",
    "  \"\"\"\n",
    "  Stable log sum exp.\n",
    "  (Optimized version)\n",
    "  \"\"\"\n",
    "  if not args:\n",
    "      return NEG_INF\n",
    "\n",
    "  # <-- 2. Optimized logsumexp\n",
    "  # Find max in one pass\n",
    "  a_max = args[0]\n",
    "  for a in args[1:]:\n",
    "      if a > a_max:\n",
    "          a_max = a\n",
    "\n",
    "  if a_max == NEG_INF:\n",
    "      return NEG_INF\n",
    "\n",
    "  # Calculate sum in a second pass\n",
    "  lsp_sum = 0.0\n",
    "  for a in args:\n",
    "      lsp_sum += math.exp(a - a_max)\n",
    "      \n",
    "  return a_max + math.log(lsp_sum)\n",
    "\n",
    "def decode(probs, beam_size=100, blank=0):\n",
    "  \"\"\"\n",
    "  Performs inference for the given output probabilities.\n",
    "  Arguments:\n",
    "      probs: The output probabilities (e.g. post-softmax) for each\n",
    "        time step. Should be an array of shape (time x output dim).\n",
    "      beam_size (int): Size of the beam to use during inference.\n",
    "      blank (int): Index of the CTC blank label.\n",
    "  Returns the output label sequence and the corresponding negative\n",
    "  log-likelihood estimated by the decoder.\n",
    "  \"\"\"\n",
    "  T, S = probs.shape\n",
    "  # Convert to log-probabilities once at the beginning\n",
    "  log_probs = np.log(probs)\n",
    "\n",
    "  # Elements in the beam are (prefix, (p_blank, p_no_blank))\n",
    "  # Initialize the beam with the empty sequence, a probability of\n",
    "  # 1 for ending in blank and zero for ending in non-blank\n",
    "  # (in log space).\n",
    "  beam = [(tuple(), (0.0, NEG_INF))]\n",
    "\n",
    "  for t in range(T): # Loop over time\n",
    "\n",
    "    # A default dictionary to store the next step candidates.\n",
    "    next_beam = make_new_beam()\n",
    "\n",
    "    # <-- 3. Pre-slice probabilities for this time step\n",
    "    log_probs_t = log_probs[t, :]\n",
    "\n",
    "    for s in range(S): # Loop over vocab\n",
    "      p = log_probs_t[s] # Use the pre-sliced array\n",
    "\n",
    "      # The variables p_b and p_nb are respectively the\n",
    "      # probabilities for the prefix given that it ends in a\n",
    "      # blank and does not end in a blank at this time step.\n",
    "      for prefix, (p_b, p_nb) in beam: # Loop over beam\n",
    "\n",
    "        # If we propose a blank the prefix doesn't change.\n",
    "        # Only the probability of ending in blank gets updated.\n",
    "        if s == blank:\n",
    "          n_p_b, n_p_nb = next_beam[prefix]\n",
    "          n_p_b = logsumexp(n_p_b, p_b + p, p_nb + p)\n",
    "          next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "          continue\n",
    "\n",
    "        # Extend the prefix by the new character s and add it to\n",
    "        # the beam. Only the probability of not ending in blank\n",
    "        # gets updated.\n",
    "        end_t = prefix[-1] if prefix else None\n",
    "        n_prefix = prefix + (s,)\n",
    "        n_p_b, n_p_nb = next_beam[n_prefix]\n",
    "        if s != end_t:\n",
    "          n_p_nb = logsumexp(n_p_nb, p_b + p, p_nb + p)\n",
    "        else:\n",
    "          # We don't include the previous probability of not ending\n",
    "          # in blank (p_nb) if s is repeated at the end. The CTC\n",
    "          # algorithm merges characters not separated by a blank.\n",
    "          n_p_nb = logsumexp(n_p_nb, p_b + p)\n",
    "          \n",
    "        next_beam[n_prefix] = (n_p_b, n_p_nb)\n",
    "\n",
    "        # If s is repeated at the end we also update the unchanged\n",
    "        # prefix. This is the merging case.\n",
    "        if s == end_t:\n",
    "          n_p_b, n_p_nb = next_beam[prefix]\n",
    "          n_p_nb = logsumexp(n_p_nb, p_nb + p)\n",
    "          next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "\n",
    "    # <-- 4. MAJOR OPTIMIZATION: Use heapq.nlargest instead of full sort\n",
    "    # Original:\n",
    "    # beam = sorted(next_beam.items(),\n",
    "    #         key=lambda x : logsumexp(*x[1]),\n",
    "    #         reverse=True)\n",
    "    # beam = beam[:beam_size]\n",
    "    \n",
    "    # Optimized:\n",
    "    beam_items = next_beam.items()\n",
    "    beam = heapq.nlargest(beam_size,\n",
    "                          beam_items,\n",
    "                          key=lambda x: logsumexp(*x[1]))\n",
    "\n",
    "  # Final selection\n",
    "  best = beam[0]\n",
    "  return best[0], -logsumexp(*best[1])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  np.random.seed(3)\n",
    "\n",
    "  time = 50\n",
    "  output_dim = 20\n",
    "\n",
    "  probs = np.random.rand(time, output_dim)\n",
    "  probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "\n",
    "  labels, score = decode(probs)\n",
    "  print(labels)\n",
    "  print(\"Score {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "logits = softmax(model_logits[f'arr_750}'],axis=1)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6410787105560303\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "labels, probs = decode(logits)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
