{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3627338409423828\n",
      "Score 97.032\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "import heapq\n",
    "import time\n",
    "\n",
    "NEG_INF = -float(\"inf\")\n",
    "\n",
    "def make_new_beam():\n",
    "  fn = lambda : (NEG_INF, NEG_INF)\n",
    "  return collections.defaultdict(fn)\n",
    "\n",
    "def logsumexp(*args):\n",
    "  \"\"\"\n",
    "  Stable log sum exp.\n",
    "  \"\"\"\n",
    "  if all(a == NEG_INF for a in args):\n",
    "      return NEG_INF\n",
    "  a_max = max(args)\n",
    "  lsp = math.log(sum(math.exp(a - a_max)\n",
    "                      for a in args))\n",
    "  return a_max + lsp\n",
    "\n",
    "def decode(probs, beam_size=100, blank=0):\n",
    "  \"\"\"\n",
    "  Performs inference for the given output probabilities.\n",
    "  Arguments:\n",
    "      probs: The output probabilities (e.g. post-softmax) for each\n",
    "        time step. Should be an array of shape (time x output dim).\n",
    "      beam_size (int): Size of the beam to use during inference.\n",
    "      blank (int): Index of the CTC blank label.\n",
    "  Returns the output label sequence and the corresponding negative\n",
    "  log-likelihood estimated by the decoder.\n",
    "  \"\"\"\n",
    "  T, S = probs.shape\n",
    "  probs = np.log(probs)\n",
    "\n",
    "  # Elements in the beam are (prefix, (p_blank, p_no_blank))\n",
    "  # Initialize the beam with the empty sequence, a probability of\n",
    "  # 1 for ending in blank and zero for ending in non-blank\n",
    "  # (in log space).\n",
    "  beam = [(tuple(), (0.0, NEG_INF))]\n",
    "\n",
    "  for t in range(T): # Loop over time\n",
    "\n",
    "    # A default dictionary to store the next step candidates.\n",
    "    next_beam = make_new_beam()\n",
    "\n",
    "    for s in range(S): # Loop over vocab\n",
    "      p = probs[t, s]\n",
    "\n",
    "      # The variables p_b and p_nb are respectively the\n",
    "      # probabilities for the prefix given that it ends in a\n",
    "      # blank and does not end in a blank at this time step.\n",
    "      for prefix, (p_b, p_nb) in beam: # Loop over beam\n",
    "\n",
    "        # If we propose a blank the prefix doesn't change.\n",
    "        # Only the probability of ending in blank gets updated.\n",
    "        if s == blank:\n",
    "          n_p_b, n_p_nb = next_beam[prefix]\n",
    "          n_p_b = logsumexp(n_p_b, p_b + p, p_nb + p)\n",
    "          next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "          continue\n",
    "\n",
    "        # Extend the prefix by the new character s and add it to\n",
    "        # the beam. Only the probability of not ending in blank\n",
    "        # gets updated.\n",
    "        end_t = prefix[-1] if prefix else None\n",
    "        n_prefix = prefix + (s,)\n",
    "        n_p_b, n_p_nb = next_beam[n_prefix]\n",
    "        if s != end_t:\n",
    "          n_p_nb = logsumexp(n_p_nb, p_b + p, p_nb + p)\n",
    "        else:\n",
    "          # We don't include the previous probability of not ending\n",
    "          # in blank (p_nb) if s is repeated at the end. The CTC\n",
    "          # algorithm merges characters not separated by a blank.\n",
    "          n_p_nb = logsumexp(n_p_nb, p_b + p)\n",
    "          \n",
    "        # *NB* this would be a good place to include an LM score.\n",
    "        next_beam[n_prefix] = (n_p_b, n_p_nb)\n",
    "\n",
    "        # If s is repeated at the end we also update the unchanged\n",
    "        # prefix. This is the merging case.\n",
    "        if s == end_t:\n",
    "          n_p_b, n_p_nb = next_beam[prefix]\n",
    "          n_p_nb = logsumexp(n_p_nb, p_nb + p)\n",
    "          next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "\n",
    "    # Sort and trim the beam before moving on to the\n",
    "    # next time-step.\n",
    "    beam = sorted(next_beam.items(),\n",
    "            key=lambda x : logsumexp(*x[1]),\n",
    "            reverse=True)\n",
    "    beam = beam[:beam_size]\n",
    "\n",
    "  best = beam[0]\n",
    "  return best[0], -logsumexp(*best[1])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  np.random.seed(3)\n",
    "\n",
    "  seq_len = 50\n",
    "  output_dim = 20\n",
    "\n",
    "  probs = np.random.rand(seq_len, output_dim)\n",
    "  probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "\n",
    "  start = time.time()\n",
    "  labels, score = decode(probs)\n",
    "  end = time.time()\n",
    "  print(end-start)\n",
    "  \n",
    "  print(\"Score {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1824796199798584\n",
      "(5, 12, 8, 16, 5, 13, 19, 7, 19, 3, 2, 4, 11, 19, 8, 14, 18, 2, 13, 17, 14, 18, 5, 1, 6, 9)\n",
      "Score 97.032\n"
     ]
    }
   ],
   "source": [
    "def make_new_beam():\n",
    "  fn = lambda : (NEG_INF, NEG_INF)\n",
    "  return collections.defaultdict(fn)\n",
    "\n",
    "def logsumexp(*args):\n",
    "  \"\"\"\n",
    "  Stable log sum exp.\n",
    "  (Optimized version)\n",
    "  \"\"\"\n",
    "  if not args:\n",
    "      return NEG_INF\n",
    "\n",
    "  # <-- 2. Optimized logsumexp\n",
    "  # Find max in one pass\n",
    "  a_max = args[0]\n",
    "  for a in args[1:]:\n",
    "      if a > a_max:\n",
    "          a_max = a\n",
    "\n",
    "  if a_max == NEG_INF:\n",
    "      return NEG_INF\n",
    "\n",
    "  # Calculate sum in a second pass\n",
    "  lsp_sum = 0.0\n",
    "  for a in args:\n",
    "      lsp_sum += math.exp(a - a_max)\n",
    "      \n",
    "  return a_max + math.log(lsp_sum)\n",
    "\n",
    "def decode(probs, beam_size=100, blank=0):\n",
    "  \"\"\"\n",
    "  Performs inference for the given output probabilities.\n",
    "  Arguments:\n",
    "      probs: The output probabilities (e.g. post-softmax) for each\n",
    "        time step. Should be an array of shape (time x output dim).\n",
    "      beam_size (int): Size of the beam to use during inference.\n",
    "      blank (int): Index of the CTC blank label.\n",
    "  Returns the output label sequence and the corresponding negative\n",
    "  log-likelihood estimated by the decoder.\n",
    "  \"\"\"\n",
    "  T, S = probs.shape\n",
    "  # Convert to log-probabilities once at the beginning\n",
    "  log_probs = np.log(probs)\n",
    "\n",
    "  # Elements in the beam are (prefix, (p_blank, p_no_blank))\n",
    "  # Initialize the beam with the empty sequence, a probability of\n",
    "  # 1 for ending in blank and zero for ending in non-blank\n",
    "  # (in log space).\n",
    "  beam = [(tuple(), (0.0, NEG_INF))]\n",
    "\n",
    "  for t in range(T): # Loop over time\n",
    "\n",
    "    # A default dictionary to store the next step candidates.\n",
    "    next_beam = make_new_beam()\n",
    "\n",
    "    # <-- 3. Pre-slice probabilities for this time step\n",
    "    log_probs_t = log_probs[t, :]\n",
    "\n",
    "    for s in range(S): # Loop over vocab\n",
    "      p = log_probs_t[s] # Use the pre-sliced array\n",
    "\n",
    "      # The variables p_b and p_nb are respectively the\n",
    "      # probabilities for the prefix given that it ends in a\n",
    "      # blank and does not end in a blank at this time step.\n",
    "      for prefix, (p_b, p_nb) in beam: # Loop over beam\n",
    "\n",
    "        # If we propose a blank the prefix doesn't change.\n",
    "        # Only the probability of ending in blank gets updated.\n",
    "        if s == blank:\n",
    "          n_p_b, n_p_nb = next_beam[prefix]\n",
    "          n_p_b = logsumexp(n_p_b, p_b + p, p_nb + p)\n",
    "          next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "          continue\n",
    "\n",
    "        # Extend the prefix by the new character s and add it to\n",
    "        # the beam. Only the probability of not ending in blank\n",
    "        # gets updated.\n",
    "        end_t = prefix[-1] if prefix else None\n",
    "        n_prefix = prefix + (s,)\n",
    "        n_p_b, n_p_nb = next_beam[n_prefix]\n",
    "        if s != end_t:\n",
    "          n_p_nb = logsumexp(n_p_nb, p_b + p, p_nb + p)\n",
    "        else:\n",
    "          # We don't include the previous probability of not ending\n",
    "          # in blank (p_nb) if s is repeated at the end. The CTC\n",
    "          # algorithm merges characters not separated by a blank.\n",
    "          n_p_nb = logsumexp(n_p_nb, p_b + p)\n",
    "          \n",
    "        next_beam[n_prefix] = (n_p_b, n_p_nb)\n",
    "\n",
    "        # If s is repeated at the end we also update the unchanged\n",
    "        # prefix. This is the merging case.\n",
    "        if s == end_t:\n",
    "          n_p_b, n_p_nb = next_beam[prefix]\n",
    "          n_p_nb = logsumexp(n_p_nb, p_nb + p)\n",
    "          next_beam[prefix] = (n_p_b, n_p_nb)\n",
    "\n",
    "    # <-- 4. MAJOR OPTIMIZATION: Use heapq.nlargest instead of full sort\n",
    "    # Original:\n",
    "    # beam = sorted(next_beam.items(),\n",
    "    #         key=lambda x : logsumexp(*x[1]),\n",
    "    #         reverse=True)\n",
    "    # beam = beam[:beam_size]\n",
    "    \n",
    "    # Optimized:\n",
    "    beam_items = next_beam.items()\n",
    "    beam = heapq.nlargest(beam_size,\n",
    "                          beam_items,\n",
    "                          key=lambda x: logsumexp(*x[1]))\n",
    "\n",
    "  # Final selection\n",
    "  best = beam[0]\n",
    "  return best[0], -logsumexp(*best[1])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  np.random.seed(3)\n",
    "\n",
    "  seq_len = 50\n",
    "  output_dim = 20\n",
    "\n",
    "  probs = np.random.rand(seq_len, output_dim)\n",
    "  probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "\n",
    "  start = time.time()\n",
    "  labels, score = decode(probs)\n",
    "  end = time.time()\n",
    "  print(end-start)\n",
    "  print(labels)\n",
    "  print(\"Score {:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
