{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcdde4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Union\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998edfe1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c440da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ChunkConfig:\n",
    "    \"\"\"Configuration for chunked attention.\n",
    "    \n",
    "    chunk_size: the number of patches (tokens) per chunk. If None, use full context.\n",
    "    context_chunks: the number of left context chunks to attend to. If None, attend to all previous chunks.\n",
    "    \n",
    "    \"\"\"\n",
    "    chunk_size: Optional[int]\n",
    "    context_chunks: Optional[int] \n",
    "\n",
    "    def is_full_context(self) -> bool:\n",
    "        return self.chunk_size is None\n",
    "    \n",
    "class ChunkConfigSampler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        chunk_size_range: Tuple[int, int],\n",
    "        context_sec_range: Tuple[float, float],\n",
    "        timestep_duration_sec: float,\n",
    "        chunkwise_prob: float = 1.0,\n",
    "        left_constrain_prob: float = 1.0,\n",
    "        seed: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        \n",
    "        chunk_size_min, chunk_size_max = chunk_size_range\n",
    "        if chunk_size_max < chunk_size_min:\n",
    "            raise ValueError(f\"Chunk size range fault: max size is {chunk_size_max} and min size is {chunk_size_min}\")\n",
    "        \n",
    "        context_sec_min, context_sec_max = context_sec_range\n",
    "        if context_sec_max < context_sec_min:\n",
    "            raise ValueError(f\"Context sec range fault: max size is {context_sec_max} and min size is {context_sec_min}\")\n",
    "        \n",
    "        self.chunk_size_range = chunk_size_range\n",
    "        self.context_sec_range = context_sec_range\n",
    "        \n",
    "        if timestep_duration_sec <= 0:\n",
    "            raise ValueError(\"timestep_duration_sec must be positive.\")\n",
    "        self.timestep_duration_sec = timestep_duration_sec\n",
    "\n",
    "        self.left_constrain_prob = max(0.0, min(1.0, float(left_constrain_prob)))\n",
    "        self.chunkwise_prob = max(0.0, min(1.0, float(chunkwise_prob)))\n",
    "        self._rng = random.Random(seed)\n",
    "\n",
    "    # --- MODIFICATION: Combined into one function ---\n",
    "    def _sample_range(self, range_values: Optional[Tuple[float, float]], dtype: str) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Samples a value from a given range, casting to the specified dtype.\n",
    "        dtype: 'int' or 'float'\n",
    "        \"\"\"\n",
    "        if range_values is None:\n",
    "            return None\n",
    "        low, high = range_values\n",
    "\n",
    "        if low == float('inf') or high == float('inf'):\n",
    "            return float('inf')\n",
    "\n",
    "        # Handle type-specific logic\n",
    "        if dtype == 'int':\n",
    "            low = max(0, int(low))\n",
    "            high = max(low, int(high))\n",
    "            if low == high:\n",
    "                return low\n",
    "            return self._rng.randint(low, high) # Sample int\n",
    "        \n",
    "        elif dtype == 'float':\n",
    "            low = max(0.0, float(low))\n",
    "            high = max(low, float(high))\n",
    "            if low == high:\n",
    "                return low\n",
    "            return self._rng.uniform(low, high) # Sample float\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dtype: {dtype}\")\n",
    "\n",
    "    # --- MODIFICATION: Updated sample() to use the new function ---\n",
    "    def sample(self) -> ChunkConfig:\n",
    "        if self._rng.random() > self.chunkwise_prob:\n",
    "            # Case for no chunking\n",
    "            return ChunkConfig(chunk_size=None, context_chunks=None)\n",
    "\n",
    "        # 1. Sample chunk size (as int)\n",
    "        chunk_size = self._sample_range(self.chunk_size_range, dtype='int')\n",
    "        \n",
    "        if chunk_size == float('inf'):\n",
    "            return ChunkConfig(chunk_size=None, context_chunks=None)\n",
    "        \n",
    "        chunk_size = max(1, int(chunk_size))\n",
    "\n",
    "        # 2. Determine context\n",
    "        if self.left_constrain_prob < 1.0 and self._rng.random() > self.left_constrain_prob:\n",
    "            # Case: No left-constrained context\n",
    "            context_chunks = None\n",
    "        else:\n",
    "            # Case: Left-constrained context\n",
    "            # Sample context_sec (as float)\n",
    "            context_sec = self._sample_range(self.context_sec_range, dtype='float')\n",
    "        \n",
    "            if context_sec is None or context_sec == float('inf'):\n",
    "                context_chunks = None\n",
    "            else:\n",
    "                # The key calculation\n",
    "                total_context_timesteps = context_sec / self.timestep_duration_sec\n",
    "                context_chunks = math.ceil(total_context_timesteps / chunk_size)\n",
    "        \n",
    "        return ChunkConfig(chunk_size=chunk_size, context_chunks=context_chunks)\n",
    "    \n",
    "    \n",
    "def create_dynamic_chunk_mask(seq_len: int, config: ChunkConfig, device=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    seq_len: sequence length T after padding\n",
    "    config: ChunkConfig object defining chunk_size and context_chunks   \n",
    "    \"\"\"\n",
    "    \n",
    "    if config.is_full_context():\n",
    "        return None\n",
    "\n",
    "    chunk_size = max(1, min(int(config.chunk_size), seq_len))\n",
    "    chunk_ids = torch.arange(seq_len, device=device) // chunk_size\n",
    "\n",
    "    query_chunk_ids = chunk_ids.unsqueeze(1)  # (T, 1)\n",
    "    key_chunk_ids = chunk_ids.unsqueeze(0)    # (1, T)\n",
    "    \n",
    "    if config.context_chunks is None:\n",
    "        lower_bound = torch.zeros_like(query_chunk_ids)\n",
    "        upper_bound = query_chunk_ids\n",
    "    else:\n",
    "        context_chunks = max(0, int(config.context_chunks))\n",
    "        lower_bound = (query_chunk_ids - context_chunks).clamp(min=0)\n",
    "        upper_bound = query_chunk_ids\n",
    "\n",
    "    mask = (key_chunk_ids >= lower_bound) & (key_chunk_ids <= upper_bound)\n",
    "    return mask.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b95d609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Config: ChunkConfig(chunk_size=2, context_chunks=2)\n",
      "Chunk Mask:\n",
      " tensor([[[[ True,  True, False, False, False, False, False, False, False, False],\n",
      "          [ True,  True, False, False, False, False, False, False, False, False],\n",
      "          [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "          [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "          [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
      "          [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
      "          [False, False,  True,  True,  True,  True,  True,  True, False, False],\n",
      "          [False, False,  True,  True,  True,  True,  True,  True, False, False],\n",
      "          [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "          [False, False, False, False,  True,  True,  True,  True,  True,  True]]]])\n"
     ]
    }
   ],
   "source": [
    "chunk_config_sampler = ChunkConfigSampler(\n",
    "       chunk_size_range=(1,3),\n",
    "         context_sec_range=(0.4,0.4),\n",
    "         timestep_duration_sec=0.1,\n",
    "         chunkwise_prob=1.0,\n",
    "         left_constrain_prob=1.0, seed=10)\n",
    "\n",
    "\n",
    "chunk_config = chunk_config_sampler.sample()\n",
    "\n",
    "mask = create_dynamic_chunk_mask(seq_len=10, config=chunk_config, device='cpu')\n",
    "print(\"Chunk Config:\", chunk_config)\n",
    "print(\"Chunk Mask:\\n\", mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1c1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
