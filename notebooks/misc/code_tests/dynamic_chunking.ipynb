{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcdde4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Union\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c440da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChunkConfig:\n",
    "    \"\"\"Configuration for chunked attention.\n",
    "    \n",
    "    chunk_size: the number of patches (tokens) per chunk. If None, use full context.\n",
    "    context_chunks: the number of left context chunks to attend to. If None, attend to all previous chunks.\n",
    "    \n",
    "    \"\"\"\n",
    "    chunk_size: Optional[Union[int, float]]\n",
    "    context_chunks: Optional[int] \n",
    "\n",
    "    def is_full_context(self) -> bool:\n",
    "        return self.chunk_size is None\n",
    "    \n",
    "    def is_causal_attention(self) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if this config is for full-context CAUSAL attention.\n",
    "        This is distinct from chunk_size=inf, which is full-context BIDIRECTIONAL.\n",
    "        \"\"\"\n",
    "        return self.chunk_size is None\n",
    "    \n",
    "class ChunkConfigSampler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        chunk_size_range: Tuple[Union[int, float], Union[int, float]],\n",
    "        context_chunks_range: Tuple[int, int],\n",
    "        chunkwise_prob: float = 1.0,\n",
    "        left_constrain_prob: float = 1.0,\n",
    "        seed: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        chunk_size_range: Tuple (min_chunk_size, max_chunk_size). Use float('inf') for infinite chunk size.\n",
    "        context_chunks_range: Tuple (min_context_chunks, max_context_chunks).\n",
    "        chunkwise_prob: Probability of using chunked attention. (0.0 = no chunking, 1.0 = always chunking)\n",
    "        left_constrain_prob: Probability of using left-constrained context. (0.0 = no left constraint, 1.0 = always left constraint)\n",
    "        seed: Optional random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        chunk_size_min, chunk_size_max = chunk_size_range\n",
    "        if chunk_size_max < chunk_size_min:\n",
    "            raise ValueError(f\"Chunk size range fault: max size is {chunk_size_max} and min size is {chunk_size_min}\")\n",
    "        context_chunks_min, context_chunks_max = context_chunks_range\n",
    "        if context_chunks_max < context_chunks_min:\n",
    "            raise ValueError(f\"Context chunks range fault: max size is {context_chunks_max} and min size is {context_chunks_min}\")\n",
    "        # Store the new chunk size range\n",
    "        self.chunk_size_range = chunk_size_range\n",
    "        # Store the new context range\n",
    "        self.context_chunks_range = context_chunks_range\n",
    "\n",
    "        self.left_constrain_prob = max(0.0, min(1.0, float(left_constrain_prob)))\n",
    "        self.chunkwise_prob = max(0.0, min(1.0, float(chunkwise_prob)))\n",
    "        self._rng = random.Random(seed)\n",
    "\n",
    "    def _sample_range(self, range_values: Optional[Tuple[int, int]]) -> Optional[int]:\n",
    "        if range_values is None:\n",
    "            return None\n",
    "        low, high = range_values\n",
    "        \n",
    "        # Handle the infinite case\n",
    "        if low == float('inf'):\n",
    "            # Assume if low is inf, high is also inf\n",
    "            return float('inf')\n",
    "\n",
    "        # Handle finite (int) case\n",
    "        low = max(0, int(low))\n",
    "        high = max(low, int(high))\n",
    "        if low == high:\n",
    "            return low\n",
    "\n",
    "        return self._rng.randint(low, high)\n",
    "\n",
    "    def sample(self) -> ChunkConfig:\n",
    "        if self.chunkwise_prob < 1.0 and self._rng.random() > self.chunkwise_prob:\n",
    "            # Case for no chunking. run in full context mode\n",
    "            return ChunkConfig(chunk_size=None, context_chunks=None)\n",
    "\n",
    "        # Sample the single chunk size value\n",
    "        chunk_size = self._sample_range(self.chunk_size_range)\n",
    "        # Sample the single context_chunks value\n",
    "        if self.left_constrain_prob < 1.0 and self._rng.random() > self.left_constrain_prob:\n",
    "            # Case 1: for no left-constrained chunking context\n",
    "            context_chunks = None   # “no limit” case\n",
    "        else:\n",
    "            # Case 2: for left-constrained chunking context\n",
    "            context_chunks = self._sample_range(self.context_chunks_range)\n",
    "        \n",
    "        return ChunkConfig(chunk_size=chunk_size, context_chunks=context_chunks)\n",
    "\n",
    "\n",
    "def create_dynamic_chunk_mask(seq_len: int, config: ChunkConfig, device=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    seq_len: sequence length T after padding\n",
    "    config: ChunkConfig object defining chunk_size and context_chunks   \n",
    "    \"\"\"\n",
    "    \n",
    "    if config.is_full_context():\n",
    "        return None\n",
    "\n",
    "    chunk_size = max(1, min(int(config.chunk_size), seq_len))\n",
    "    chunk_ids = torch.arange(seq_len, device=device) // chunk_size\n",
    "\n",
    "    query_chunk_ids = chunk_ids.unsqueeze(1)  # (T, 1)\n",
    "    key_chunk_ids = chunk_ids.unsqueeze(0)    # (1, T)\n",
    "    \n",
    "    if config.context_chunks is None:\n",
    "        lower_bound = torch.zeros_like(query_chunk_ids)\n",
    "        upper_bound = query_chunk_ids\n",
    "    else:\n",
    "        context_chunks = max(0, int(config.context_chunks))\n",
    "        lower_bound = (query_chunk_ids - context_chunks).clamp(min=0)\n",
    "        upper_bound = query_chunk_ids\n",
    "\n",
    "    mask = (key_chunk_ids >= lower_bound) & (key_chunk_ids <= upper_bound)\n",
    "    return mask.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b95d609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Config: ChunkConfig(chunk_size=1, context_chunks=2)\n",
      "Chunk Mask:\n",
      " tensor([[[[ True, False, False, False, False, False, False, False, False, False],\n",
      "          [ True,  True, False, False, False, False, False, False, False, False],\n",
      "          [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "          [False,  True,  True,  True, False, False, False, False, False, False],\n",
      "          [False, False,  True,  True,  True, False, False, False, False, False],\n",
      "          [False, False, False,  True,  True,  True, False, False, False, False],\n",
      "          [False, False, False, False,  True,  True,  True, False, False, False],\n",
      "          [False, False, False, False, False,  True,  True,  True, False, False],\n",
      "          [False, False, False, False, False, False,  True,  True,  True, False],\n",
      "          [False, False, False, False, False, False, False,  True,  True,  True]]]])\n"
     ]
    }
   ],
   "source": [
    "chunk_config_sampler = ChunkConfigSampler(\n",
    "       chunk_size_range=(1,3),\n",
    "         context_chunks_range=(1,2),\n",
    "            chunkwise_prob=1.0,\n",
    "            left_constrain_prob=1.0,\n",
    "            seed=None)\n",
    "\n",
    "\n",
    "chunk_config = chunk_config_sampler.sample()\n",
    "\n",
    "mask = create_dynamic_chunk_mask(seq_len=10, config=chunk_config, device='cpu')\n",
    "print(\"Chunk Config:\", chunk_config)\n",
    "print(\"Chunk Mask:\\n\", mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1c1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
