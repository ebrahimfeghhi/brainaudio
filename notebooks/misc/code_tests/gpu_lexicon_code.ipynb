{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853f54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import collections\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "@dataclass\n",
    "class CTCHypothesis:\n",
    "    \"\"\"Mimics torchaudio.models.decoder.CTCHypothesis\"\"\"\n",
    "    tokens: torch.Tensor\n",
    "    words: List[str]\n",
    "    score: float\n",
    "    timesteps: torch.Tensor\n",
    "\n",
    "class VectorizedLexicon:\n",
    "    def __init__(self, lexicon_path: str, tokens: List[str], blank_token=\"<blank>\"):\n",
    "        self.tokens = tokens\n",
    "        self.token_to_id = {t: i for i, t in enumerate(tokens)}\n",
    "        self.blank_id = self.token_to_id.get(blank_token, 0)  # Default to 0 if not found\n",
    "        \n",
    "        # 1. Build the Trie on CPU\n",
    "        self.trie = {'children': {}, 'is_word': False, 'id': 0}\n",
    "        self.node_count = 1\n",
    "        \n",
    "        with open(lexicon_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if not parts: continue\n",
    "                word = parts[0]\n",
    "                # Assuming lexicon format: \"word c a t\" or just \"word\" if simple\n",
    "                # We'll assume the word itself is the sequence of chars for simplicity,\n",
    "                # or that the file provides space-separated tokens.\n",
    "                spelling = parts[1:] if len(parts) > 1 else list(word)\n",
    "                \n",
    "                node = self.trie\n",
    "                for char in spelling:\n",
    "                    if char not in self.token_to_id:\n",
    "                        continue # Skip unknown tokens\n",
    "                    token_id = self.token_to_id[char]\n",
    "                    if token_id not in node['children']:\n",
    "                        node['children'][token_id] = {'children': {}, 'is_word': False, 'id': self.node_count}\n",
    "                        self.node_count += 1\n",
    "                    node = node['children'][token_id]\n",
    "                node['is_word'] = True\n",
    "\n",
    "        # 2. Flatten Trie to GPU Tensor: [Num_Nodes, Vocab_Size]\n",
    "        # transitions[i, k] = index of next node if we are at node i and see token k\n",
    "        # We initialize with -1 (invalid transition)\n",
    "        self.vocab_size = len(tokens)\n",
    "        self.transitions = torch.full((self.node_count, self.vocab_size), -1, dtype=torch.long)\n",
    "        self.is_word_node = torch.zeros(self.node_count, dtype=torch.bool)\n",
    "        \n",
    "        # Queue for BFS traversal to populate tensor\n",
    "        queue = [self.trie]\n",
    "        while queue:\n",
    "            node = queue.pop(0)\n",
    "            u = node['id']\n",
    "            if node['is_word']:\n",
    "                self.is_word_node[u] = True\n",
    "            \n",
    "            for token_id, child_node in node['children'].items():\n",
    "                v = child_node['id']\n",
    "                self.transitions[u, token_id] = v\n",
    "                queue.append(child_node)\n",
    "                \n",
    "        # Self-loops for blank token (stay at same node)\n",
    "        # NOTE: Standard CTC logic handles blanks separately, but for the generic \n",
    "        # transition matrix, we often mark blank as \"stay\". \n",
    "        # Here we will handle blanks in the decoder logic explicitly to adhere to CTC rules.\n",
    "\n",
    "    def to(self, device):\n",
    "        self.transitions = self.transitions.to(device)\n",
    "        self.is_word_node = self.is_word_node.to(device)\n",
    "        return self\n",
    "\n",
    "\n",
    "class GPUCTCDecoder:\n",
    "    def __init__(self, \n",
    "                 lexicon_path: str, \n",
    "                 tokens: List[str], \n",
    "                 beam_size: int = 10, \n",
    "                 blank_token: str = \"-\"):\n",
    "        \n",
    "        self.beam_size = beam_size\n",
    "        self.tokens = tokens\n",
    "        self.blank_id = tokens.index(blank_token) if blank_token in tokens else 0\n",
    "        \n",
    "        # Load and vectorized lexicon\n",
    "        self.lexicon = VectorizedLexicon(lexicon_path, tokens, blank_token)\n",
    "        \n",
    "    def __call__(self, emissions: torch.Tensor) -> List[List[CTCHypothesis]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emissions: (Batch, Time, Vocab) - Logits from acoustic model\n",
    "        Returns:\n",
    "            List of lists of hypotheses (Batch -> N-Best)\n",
    "        \"\"\"\n",
    "        device = emissions.device\n",
    "        if self.lexicon.transitions.device != device:\n",
    "            self.lexicon.to(device)\n",
    "\n",
    "        B, T, V = emissions.shape\n",
    "        \n",
    "        # Initialize Beams\n",
    "        # Scores: [B, Beam_Size]\n",
    "        beam_scores = torch.full((B, self.beam_size), -float('inf'), device=device)\n",
    "        beam_scores[:, 0] = 0.0  # Start with prob 1.0 (log prob 0.0)\n",
    "        \n",
    "        # History Tracking\n",
    "        # We need to store token history to reconstruct paths later\n",
    "        # For efficiency in this demo, we'll just store the current state indices\n",
    "        # In a real optimized version, you'd use a backpointer array.\n",
    "        \n",
    "        # Trie Node Indices: [B, Beam_Size]\n",
    "        # Start at root (0)\n",
    "        beam_trie_nodes = torch.zeros((B, self.beam_size), dtype=torch.long, device=device)\n",
    "        \n",
    "        # Last Token (for CTC repeat collapse): [B, Beam_Size]\n",
    "        beam_last_tokens = torch.full((B, self.beam_size), self.blank_id, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Store paths (inefficient in pure Python, but clear for logic)\n",
    "        # List of Lists of lists: Batch -> Beam -> Sequence\n",
    "        beam_paths = [[[] for _ in range(self.beam_size)] for _ in range(B)]\n",
    "\n",
    "        # --- DECODING LOOP ---\n",
    "        for t in range(T):\n",
    "            # 1. Get Logits for this step: [B, V]\n",
    "            log_probs = torch.nn.functional.log_softmax(emissions[:, t, :], dim=-1)\n",
    "            \n",
    "            # 2. Expand Beams: [B, Beam, V]\n",
    "            # Add current log_probs to existing beam scores\n",
    "            # Shape: [B, Beam, 1] + [B, 1, V] -> [B, Beam, V]\n",
    "            next_scores = beam_scores.unsqueeze(-1) + log_probs.unsqueeze(1)\n",
    "            next_scores = next_scores.view(B, -1) # Flatten to [B, Beam * V]\n",
    "            \n",
    "            # --- LEXICON CONSTRAINT IMPLEMENTATION ---\n",
    "            \n",
    "            # Logic: We need to know the valid 'next_node' for every possible expansion.\n",
    "            # Current nodes: beam_trie_nodes [B, Beam]\n",
    "            # We need to lookup transitions for all V tokens.\n",
    "            \n",
    "            # Expand current nodes to match the flattened structure\n",
    "            # [B, Beam] -> [B, Beam, 1] -> expand -> flatten -> [B, Beam*V]\n",
    "            current_nodes_expanded = beam_trie_nodes.unsqueeze(-1).expand(-1, -1, V).reshape(B, -1)\n",
    "            \n",
    "            # Get potential tokens for every candidate\n",
    "            # [1, 1, V] -> [B, Beam, V] -> flatten\n",
    "            candidate_tokens = torch.arange(V, device=device).reshape(1, 1, -1).expand(B, self.beam_size, -1).reshape(B, -1)\n",
    "            \n",
    "            # Calculate Next Node using Vectorized Lookup\n",
    "            # This is the \"Step\" logic for CTC:\n",
    "            # - If token is BLANK: Stay at current node\n",
    "            # - If token == Last Token: Stay at current node (Merge repeats)\n",
    "            # - If token is NEW: Look up transition table\n",
    "            \n",
    "            last_tokens_expanded = beam_last_tokens.unsqueeze(-1).expand(-1, -1, V).reshape(B, -1)\n",
    "            \n",
    "            # Default: Move to next node in Trie\n",
    "            # transitions[current_node, token]\n",
    "            # We use gather or advanced indexing. \n",
    "            # tensor[idx_batch, idx_vocab]\n",
    "            # Since we have a batch of current nodes, we flatten everything for lookup\n",
    "            \n",
    "            # Lookup: [Total_Candidates]\n",
    "            next_trie_nodes_lookup = self.lexicon.transitions[current_nodes_expanded, candidate_tokens]\n",
    "            \n",
    "            is_space = (candidate_tokens == self.space_id)\n",
    "            \n",
    "            # Check if the current node (before moving) is a valid end of a word\n",
    "            # We expand: [B, Beam] -> [B, Beam * V] to match flattened list\n",
    "            current_is_word = self.lexicon.is_word_node[current_nodes_expanded]\n",
    "            \n",
    "            # Logic: \n",
    "            # 1. If Token is Space AND Current is Word -> Go to Root (0)\n",
    "            # 2. If Token is Space AND Current is NOT Word -> Invalid (-1)\n",
    "            # 3. If Token is NOT Space -> Keep the standard lookup result\n",
    "            \n",
    "            # We build a \"Space Result\" tensor first\n",
    "            space_target = torch.where(current_is_word, \n",
    "                                       torch.tensor(0, device=device),  # Valid Space -> Root\n",
    "                                       torch.tensor(-1, device=device)) # Invalid Space -> Die\n",
    "            \n",
    "            # Apply it to our lookup\n",
    "            next_trie_nodes_lookup = torch.where(is_space, space_target, next_trie_nodes_lookup)\n",
    "            \n",
    "            # Apply CTC Logic\n",
    "            is_blank = (candidate_tokens == self.blank_id)\n",
    "            is_repeat = (candidate_tokens == last_tokens_expanded)\n",
    "            \n",
    "            # Logic:\n",
    "            # If blank OR repeat: Next Node = Current Node\n",
    "            # Else: Next Node = Lookup Result\n",
    "            \n",
    "            final_next_nodes = torch.where(\n",
    "                is_blank | is_repeat,\n",
    "                current_nodes_expanded,\n",
    "                next_trie_nodes_lookup\n",
    "            )\n",
    "            \n",
    "            \n",
    "            # MASKING:\n",
    "            # If final_next_nodes is -1, it means we tried to step off the Trie.\n",
    "            # Set score to -inf\n",
    "            valid_mask = (final_next_nodes != -1)\n",
    "            next_scores = torch.where(valid_mask, next_scores, torch.tensor(-float('inf'), device=device))\n",
    "\n",
    "            # ------------------------------------------\n",
    "            # [[ SLOT FOR YOUR LLM SCORING HERE ]]\n",
    "            # You have access to `next_scores` (Batch, Beam*V) \n",
    "            # and `final_next_nodes` before we top-k.\n",
    "            # ------------------------------------------\n",
    "\n",
    "            # 3. Prune (Top-K)\n",
    "            # Take top beam_size scores\n",
    "            top_scores, top_indices = torch.topk(next_scores, self.beam_size, dim=1)\n",
    "            \n",
    "            # 4. Update State\n",
    "            beam_scores = top_scores\n",
    "            \n",
    "            # Recover which beam and which token generated the top scores\n",
    "            # index = beam_idx * V + token_idx\n",
    "            prev_beam_indices = top_indices // V\n",
    "            new_token_indices = top_indices % V\n",
    "            \n",
    "            # Update Trie Nodes\n",
    "            # We need to gather the node IDs from the `final_next_nodes` array using the top_indices\n",
    "            beam_trie_nodes = torch.gather(final_next_nodes, 1, top_indices)\n",
    "            \n",
    "            # Update Last Tokens\n",
    "            beam_last_tokens = new_token_indices\n",
    "\n",
    "            # Update Paths (CPU-side bookkeeping for output)\n",
    "            # This part is hard to fully vectorize without heavy memory, so usually done on CPU or\n",
    "            # using a parent-pointer tensor. For simplicity/clarity:\n",
    "            new_paths = []\n",
    "            for b in range(B):\n",
    "                batch_paths = []\n",
    "                for k in range(self.beam_size):\n",
    "                    prev_k = prev_beam_indices[b, k].item()\n",
    "                    token = new_token_indices[b, k].item()\n",
    "                    \n",
    "                    # Copy previous path\n",
    "                    current_path = list(beam_paths[b][prev_k])\n",
    "                    current_path.append(token)\n",
    "                    batch_paths.append(current_path)\n",
    "                new_paths.append(batch_paths)\n",
    "            beam_paths = new_paths\n",
    "            \n",
    "        # --- Finalize Results ---\n",
    "        results = []\n",
    "        for b in range(B):\n",
    "            hyps = []\n",
    "            for k in range(self.beam_size):\n",
    "                # Filter blanks and repeats to get final \"words\"\n",
    "                # Note: In this specific logic, we only tracked Trie nodes.\n",
    "                # To get actual words, we'd usually process the raw token path.\n",
    "                raw_tokens = beam_paths[b][k]\n",
    "                \n",
    "                # CTC Collapse for output string\n",
    "                collapsed_indices = []\n",
    "                prev = -1\n",
    "                for t_id in raw_tokens:\n",
    "                    if t_id != self.blank_id and t_id != prev:\n",
    "                        collapsed_indices.append(t_id)\n",
    "                    prev = t_id\n",
    "                \n",
    "                # Simple word reconstruction (assuming chars)\n",
    "                words_str = \"\".join([self.tokens[i] for i in collapsed_indices])\n",
    "                \n",
    "                hyps.append(CTCHypothesis(\n",
    "                    tokens=torch.tensor(collapsed_indices),\n",
    "                    words=[words_str], # simplified\n",
    "                    score=beam_scores[b, k].item(),\n",
    "                    timesteps=torch.tensor([]) # Placeholder\n",
    "                ))\n",
    "            results.append(hyps)\n",
    "            \n",
    "        return results\n",
    "\n",
    "# --- USAGE EXAMPLE ---\n",
    "\n",
    "# 1. Create a dummy lexicon file\n",
    "with open(\"lexicon.txt\", \"w\") as f:\n",
    "    f.write(\"CAT c a t\\n\")\n",
    "    f.write(\"DOG d o g\\n\")\n",
    "    f.write(\"BAT b a t\\n\")\n",
    "\n",
    "# 2. Setup\n",
    "tokens = [\"-\", \"c\", \"a\", \"t\", \"d\", \"o\", \"g\", \"b\"] # - is blank\n",
    "decoder = GPUCTCDecoder(\"lexicon.txt\", tokens, beam_size=2)\n",
    "\n",
    "# 3. Fake Emissions [Batch=1, Time=5, Vocab=8]\n",
    "# Let's make \"d o g\" likely\n",
    "emissions = torch.randn(1, 5, 8).cuda() \n",
    "# ... (Set high logits for d, o, g to test) ...\n",
    "\n",
    "# 4. Run\n",
    "results = decoder(emissions)\n",
    "print(results[0][0].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a46a13f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "{}[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65959ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRIE STRUCTURE ---\n",
      "Node ID: 0\n",
      "  --(c)-->\n",
      "  Node ID: 1\n",
      "    --(a)-->\n",
      "    Node ID: 2\n",
      "      --(t)-->\n",
      "      Node ID: 3 [WORD]\n",
      "      --(r)-->\n",
      "      Node ID: 10 [WORD]\n",
      "  --(d)-->\n",
      "  Node ID: 4\n",
      "    --(o)-->\n",
      "    Node ID: 5\n",
      "      --(g)-->\n",
      "      Node ID: 6 [WORD]\n",
      "  --(b)-->\n",
      "  Node ID: 7\n",
      "    --(a)-->\n",
      "    Node ID: 8\n",
      "      --(t)-->\n",
      "      Node ID: 9 [WORD]\n",
      "      --(r)-->\n",
      "      Node ID: 11 [WORD]\n"
     ]
    }
   ],
   "source": [
    "def debug_print_trie(node, token_map_inv, indent=0):\n",
    "    \"\"\"\n",
    "    Recursive function to print the Trie structure.\n",
    "    node: The current dictionary node\n",
    "    token_map_inv: Dict mapping IDs back to characters {0: 'a', 1: 'b', ...}\n",
    "    \"\"\"\n",
    "    # Visual spacing based on depth\n",
    "    space = \"  \" * indent\n",
    "    \n",
    "    # Check if this node marks the end of a word\n",
    "    word_marker = \" [WORD]\" if node['is_word'] else \"\"\n",
    "    \n",
    "    # Print current node ID\n",
    "    print(f\"{space}Node ID: {node['id']}{word_marker}\")\n",
    "    \n",
    "    # Recursively print children\n",
    "    for token_id, child_node in node['children'].items():\n",
    "        char = token_map_inv.get(token_id, f\"id_{token_id}\")\n",
    "        print(f\"{space}  --({char})-->\")\n",
    "        debug_print_trie(child_node, token_map_inv, indent + 1)\n",
    "\n",
    "# --- usage with the previous example ---\n",
    "# Assuming you have the 'decoder' object from the previous code block:\n",
    "\n",
    "# 1. Create a dummy lexicon file\n",
    "with open(\"lexicon.txt\", \"w\") as f:\n",
    "    f.write(\"CAT c a t\\n\")\n",
    "    f.write(\"DOG d o g\\n\")\n",
    "    f.write(\"BAT b a t\\n\")\n",
    "    f.write(\"CAR c a r\\n\")\n",
    "    f.write(\"BAR b a r\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "# 2. Setup\n",
    "tokens = [\"-\", \"c\", \"a\", \"t\", \"d\", \"o\", \"g\", \"b\", \"r\"] # - is blank\n",
    "decoder = GPUCTCDecoder(\"lexicon.txt\", tokens, beam_size=2)\n",
    "\n",
    "# Create an inverse map to see characters instead of IDs\n",
    "id_to_token = {v: k for k, v in decoder.lexicon.token_to_id.items()}\n",
    "\n",
    "print(\"--- TRIE STRUCTURE ---\")\n",
    "debug_print_trie(decoder.lexicon.trie, id_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2fe4b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'key'}\n"
     ]
    }
   ],
   "source": [
    "l = {}\n",
    "\n",
    "l[0] = \"key\"\n",
    "\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7205a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
