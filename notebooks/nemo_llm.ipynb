{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bbb79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
      "No exporters were provided. This means that no telemetry data will be collected.\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "[NeMo W 2025-12-28 19:47:58 nemo_logging:364] /home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from brainaudio.inference.decoder.ngram_lm.ngram_lm_batched import NGramGPULanguageModel\n",
    "from brainaudio.inference.decoder import BatchedBeamCTCComputer\n",
    "from brainaudio.inference.decoder.beam_helpers import load_log_probs\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dab3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VOCAB = [\n",
    "    \"<sp>\",          # space token\n",
    "    \"!\", \",\", \".\", \"?\", \"'\",   # punctuation (incl. apostrophe)\n",
    "] + [chr(i) for i in range(ord('a'), ord('z') + 1)]  # 'a'..'z'\n",
    "\n",
    "# Build mappings\n",
    "_CHAR_TO_ID = {c: i for i, c in enumerate(CHAR_VOCAB)}\n",
    "_ID_TO_CHAR = {i: c for c, i in _CHAR_TO_ID.items()}\n",
    "\n",
    "# Convenience indices\n",
    "SPACE_ID = _CHAR_TO_ID[\"<sp>\"]\n",
    "\n",
    "def charToId(c: str) -> int:\n",
    "    \"\"\"Map raw input char to ID, normalizing space and lowercase.\"\"\"\n",
    "    if c == \" \":\n",
    "        c = \"<sp>\"\n",
    "    c = c.lower()\n",
    "    return _CHAR_TO_ID[c]\n",
    "\n",
    "def idToChar(i: int) -> str:\n",
    "    return _ID_TO_CHAR[i]\n",
    "\n",
    "\n",
    "\n",
    "language_model_path = \"/data2/brain2text/lm/char_lm/lm_dec19_char_huge_12gram.nemo\"\n",
    "vocab_size = 32\n",
    "lm = NGramGPULanguageModel.from_nemo(\n",
    "    lm_path=language_model_path,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "log_probs, log_probs_length = load_log_probs(\"/data2/brain2text/b2t_24/logits/tm_transformer_combined_lw_char/logits_val.npz\", [0,1,2,3,4,5,6,7,8,9,10], device=\"cuda\", blank_last_index=True)\n",
    "\n",
    "alpha = 1.5\n",
    "decoder = BatchedBeamCTCComputer(blank_index=32, beam_size=200, \n",
    "                                 return_best_hypothesis=True, fusion_models=[lm], \n",
    "                                fusion_models_alpha=[alpha], beam_threshold=20)\n",
    "\n",
    "idx = 0\n",
    "\n",
    "log_probs_one_sample = torch.unsqueeze(log_probs[idx], dim=0)\n",
    "log_probs_length_one_sample = torch.unsqueeze(log_probs_length[idx], dim=0)\n",
    "transcripts = decoder.batched_beam_search_torch(log_probs_one_sample, log_probs_length_one_sample)\n",
    "best_hyp_lm = transcripts.to_nbest_hyps_list()[0]\n",
    "for i in range(len(best_hyp_lm.n_best_hypotheses)):\n",
    "    decoded_lm = \"\".join(idToChar(idx) for idx in best_hyp_lm.n_best_hypotheses[i].y_sequence).replace('<sp>', ' ')\n",
    "    print(f'{i}: {decoded_lm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05adbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f5bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  ded \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e1978fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NBestHypotheses(n_best_hypotheses=[Hypothesis(score=-748.5164794921875, y_sequence=array([ 0,  9, 10,  9,  0]), text=None, dec_out=None, dec_state=None, timestamp=array([27, 43, 46, 50, 52]), alignments=None, frame_confidence=None, token_confidence=None, word_confidence=None, length=0, y=None, lm_state=None, lm_scores=None, ngram_lm_state=None, tokens=None, last_token=None, token_duration=None, last_frame=None)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyp_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e134c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainaudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
