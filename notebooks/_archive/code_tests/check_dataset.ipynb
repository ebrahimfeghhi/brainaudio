{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset = pd.read_pickle(\"/data2/brain2text/b2t_24/brain2text24_with_fa_char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset['train'][10][\"transcriptions\"][30]\n",
    "characters = char_dataset['train'][10][\"textC\"][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VOCAB = [\n",
    "    \"<sp>\",          # space token\n",
    "    \"!\", \",\", \".\", \"?\", \"'\",   # punctuation (incl. apostrophe)\n",
    "] + [chr(i) for i in range(ord('a'), ord('z') + 1)]  # 'a'..'z'\n",
    "\n",
    "# Build mappings\n",
    "_CHAR_TO_ID = {c: i for i, c in enumerate(CHAR_VOCAB)}\n",
    "_ID_TO_CHAR = {i: c for c, i in _CHAR_TO_ID.items()}\n",
    "\n",
    "# Convenience indices\n",
    "SPACE_ID = _CHAR_TO_ID[\"<sp>\"]\n",
    "\n",
    "print(CHAR_VOCAB)\n",
    "\n",
    "\n",
    "character_units = [\"-\"]\n",
    "for cv in CHAR_VOCAB:\n",
    "    \n",
    "    if cv == \"<sp>\":\n",
    "        character_units.append(\"|\")\n",
    "    else:\n",
    "        character_units.append(cv)\n",
    "        \n",
    "string = \"\"\n",
    "for c in characters:\n",
    "    if c > 1:\n",
    "        string += character_units[c]\n",
    "    if c == 1:\n",
    "        string += \" \"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = \"/data2/brain2text/b2t_24/saved_val_results/transformer_short_training_fixed.pkl\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['transformer_short_training_fixed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak VRAM during a full training step: 5250.92 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from brainaudio.models.transformer_chunking import TransformerModel\n",
    "import yaml\n",
    "\n",
    "config_path = \"tm_transformer_b2t24_log_dynchunk.yaml\"\n",
    "config_file = f\"../src/brainaudio/training/utils/custom_configs/{config_path}\"\n",
    "\n",
    "with open(config_file, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model_type = config['modelType']\n",
    "model_args = config['model'][model_type]\n",
    "\n",
    "model = TransformerModel(features_list=model_args['features_list'], samples_per_patch=model_args['samples_per_patch'], dim=model_args['d_model'], depth=model_args['depth'], heads=model_args['n_heads'], mlp_dim_ratio=model_args['mlp_dim_ratio'],  dim_head=model_args['dim_head'], \n",
    "                     dropout=config['dropout'], input_dropout=config['input_dropout'], nClasses=config['nClasses'], \n",
    "                     max_mask_pct=config['max_mask_pct'], num_masks=config['num_masks'], num_participants=len(model_args['features_list']), return_final_layer=False, \n",
    "                     chunked_attention=model_args[\"chunked_attention\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --- 1. Initialize the Optimizer FIRST ---\n",
    "# This ensures it's part of the memory measurement\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], \n",
    "                               weight_decay=config['l2_decay'], \n",
    "                               eps=config['eps'], \n",
    "                               betas=(config['beta1'], config['beta2']), \n",
    "                               fused=True)\n",
    "\n",
    "# --- 2. Check PEAK memory during a forward/backward/step pass ---\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# --- Create plausible dummy data for your Transformer ---\n",
    "batch_size = 64\n",
    "seq_len = 1500 \n",
    "d_model = model_args['d_model'] \n",
    "\n",
    "dummy_input = torch.randn(batch_size, seq_len, 256, device=device) \n",
    "dummy_X_len = torch.full((batch_size,), seq_len, dtype=torch.long, device=device)\n",
    "dummy_participant_idx = 0\n",
    "dummy_day_idx = 1\n",
    "\n",
    "# --- Simulate a FULL training step ---\n",
    "optimizer.zero_grad() # Clear old gradients\n",
    "output = model(dummy_input, dummy_X_len, dummy_participant_idx, dummy_day_idx)\n",
    "loss = output.sum() # Dummy loss\n",
    "loss.backward() # Calculate gradients\n",
    "optimizer.step() # Update weights AND allocate optimizer state\n",
    "# ---------------------------------\n",
    "\n",
    "# Get the peak memory\n",
    "peak_memory_bytes = torch.cuda.max_memory_allocated()\n",
    "peak_memory_mb = peak_memory_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Peak VRAM during a full training step: {peak_memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
