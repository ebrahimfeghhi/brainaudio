{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 23\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the Llama 3.2 tokenizer (requires Hugging Face login)\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "text = \"i was also speaking in longer blocks of words and that makes it difficult to correct anything to one hundred percent correct.\"\n",
    "\n",
    "# 1. Encode the text into token IDs\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "# 2. Count them\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "\n",
    "# Optional: See what the tokens actually are\n",
    "# print(tokenizer.convert_ids_to_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- KV Cache Memory Cost ---\n",
      "Config: 2000 Beams x 40 Tokens (fp16)\n",
      "Memory: 8.54 GB\n"
     ]
    }
   ],
   "source": [
    "def calculate_kv_memory_usage(num_beams=2000, max_tokens=50, precision=\"fp16\"):\n",
    "    \"\"\"\n",
    "    Calculates the VRAM usage for Llama 3.2 3B KV Cache.\n",
    "    \n",
    "    Architecture Reference (Llama 3.2 3B):\n",
    "    - Layers: 28\n",
    "    - KV Heads: 8 (Uses Grouped Query Attention)\n",
    "    - Head Dim: 128\n",
    "    \"\"\"\n",
    "    # 1. Model Configuration\n",
    "    NUM_LAYERS = 28\n",
    "    NUM_KV_HEADS = 8\n",
    "    HEAD_DIM = 128\n",
    "    \n",
    "    # 2. Precision\n",
    "    if precision == \"fp16\" or precision == \"bf16\":\n",
    "        bytes_per_param = 2\n",
    "    elif precision == \"fp32\":\n",
    "        bytes_per_param = 4\n",
    "    elif precision == \"int8\":\n",
    "        bytes_per_param = 1\n",
    "    else:\n",
    "        raise ValueError(\"Unknown precision\")\n",
    "\n",
    "    # 3. Calculate Size\n",
    "    # We store 2 matrices (Key + Value) per layer\n",
    "    # Size = (K + V) * Layers * Heads * Dim * Bytes\n",
    "    size_per_token = 2 * NUM_LAYERS * NUM_KV_HEADS * HEAD_DIM * bytes_per_param\n",
    "    \n",
    "    total_bytes = size_per_token * num_beams * max_tokens\n",
    "    total_gb = total_bytes / (1024**3)\n",
    "    \n",
    "    print(f\"--- KV Cache Memory Cost ---\")\n",
    "    print(f\"Config: {num_beams} Beams x {max_tokens} Tokens ({precision})\")\n",
    "    print(f\"Memory: {total_gb:.2f} GB\")\n",
    "    \n",
    "    return total_gb\n",
    "\n",
    "# Example Usage\n",
    "usage = calculate_kv_memory_usage(num_beams=2000, max_tokens=40, precision=\"fp16\")\n",
    "\n",
    "# Check if it fits on your GPU (e.g. 24GB)\n",
    "# Model Weights ~6GB + Cache ~10.7GB = ~16.7GB Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# --- Hardware Check ---\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"CUDA not detected. This script requires a GPU.\")\n",
    "    exit()\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "props = torch.cuda.get_device_properties(device)\n",
    "print(f\"Running on: {props.name}\")\n",
    "print(f\"Total VRAM: {props.total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# --- Configuration (Matches Llama 3.2 3B) ---\n",
    "NUM_BEAMS = 2000       # The massive beam count\n",
    "MAX_LEN = 50           # Max sequence length\n",
    "DTYPE = torch.float16  # Standard precision\n",
    "\n",
    "# Llama 3.2 3B Specs\n",
    "NUM_LAYERS = 28\n",
    "NUM_KV_HEADS = 8       # Grouped Query Attention (GQA)\n",
    "HEAD_DIM = 128         # Standard Llama head dim\n",
    "\n",
    "# --- 1. Allocate the \"Static KV Cache\" ---\n",
    "print(f\"\\nAllocating Static KV Cache for {NUM_BEAMS} beams x {MAX_LEN} tokens...\")\n",
    "\n",
    "# We calculate the exact shape.\n",
    "# Shape: [Layers, 2 (K+V), Beams, Heads, Max_Len, Head_Dim]\n",
    "# Note: In a real implementation, you might split 'Layers' into a list to avoid one contiguous 10GB block,\n",
    "# but for this test, a single block ensures we test the worst-case memory fragmentation.\n",
    "cache_shape = (NUM_LAYERS, 2, NUM_BEAMS, NUM_KV_HEADS, MAX_LEN, HEAD_DIM)\n",
    "\n",
    "try:\n",
    "    # Initialize with zeros (or random) in VRAM\n",
    "    kv_cache = torch.zeros(cache_shape, dtype=DTYPE, device=device)\n",
    "    \n",
    "    # Calculate size in GB\n",
    "    num_elements = kv_cache.numel()\n",
    "    size_gb = (num_elements * 2) / (1024**3) # 2 bytes for float16\n",
    "    print(f\"✅ Cache Allocated Successfully!\")\n",
    "    print(f\"   Size: {size_gb:.2f} GB\")\n",
    "    print(f\"   Shape: {cache_shape}\")\n",
    "    \n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(\"❌ OOM! The cache is too big for your GPU.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. The Simulation Loop ---\n",
    "print(f\"\\nStarting Beam Search Simulation ({MAX_LEN} steps)...\")\n",
    "\n",
    "# Timers\n",
    "total_reorder_time = 0\n",
    "start_benchmark = time.time()\n",
    "\n",
    "# Dummy \"New Token\" data (simulating output from one step of the model)\n",
    "# Shape per step: [Layers, 2, Beams, Heads, 1, Dim]\n",
    "new_token_update = torch.randn(\n",
    "    (NUM_LAYERS, 2, NUM_BEAMS, NUM_KV_HEADS, 1, HEAD_DIM), \n",
    "    dtype=DTYPE, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "for step in range(MAX_LEN):\n",
    "    # A. SIMULATE COMPUTE (Model Forward Pass)\n",
    "    # We pretend the model ran and gave us new KV data.\n",
    "    # We write this new data into the static cache at index 'step'.\n",
    "    \n",
    "    # Write to cache (No cost, just indexing)\n",
    "    kv_cache[:, :, :, :, step:step+1, :] = new_token_update\n",
    "\n",
    "    # B. SIMULATE BEAM SELECTION (The \"Shuffle\" Logic)\n",
    "    # Generate random \"parent indices\" to simulate beams splitting/dying.\n",
    "    # This represents: \"Beam 0 survived, Beam 1 died and was replaced by a copy of Beam 0\"\n",
    "    parent_indices = torch.randint(0, NUM_BEAMS, (NUM_BEAMS,), device=device)\n",
    "    \n",
    "    # C. REORDER CACHE (The Critical Benchmark)\n",
    "    # We must shuffle the history [0...step] to match the new parents.\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    reorder_start = time.time()\n",
    "    \n",
    "    # 1. Slice valid history (Layers, KV, Beams, Heads, 0:step+1, Dim)\n",
    "    # 2. Select along the BEAM dimension (Dim 2)\n",
    "    # 3. Write back in-place\n",
    "    \n",
    "    # Optimized View-based Select\n",
    "    # We gather everything up to the current step.\n",
    "    # Note: 'index_select' is often faster than fancy slicing for this specific shape.\n",
    "    \n",
    "    current_history = kv_cache[:, :, :, :, :step+1, :]\n",
    "    \n",
    "    # The heavy lift: Shuffling 10GB of data around\n",
    "    reordered_history = torch.index_select(current_history, 2, parent_indices)\n",
    "    \n",
    "    # Write back\n",
    "    kv_cache[:, :, :, :, :step+1, :] = reordered_history\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    reorder_end = time.time()\n",
    "    \n",
    "    total_reorder_time += (reorder_end - reorder_start)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}/{MAX_LEN} - Reorder Time: {(reorder_end - reorder_start)*1000:.2f} ms\")\n",
    "\n",
    "total_benchmark = time.time() - start_benchmark\n",
    "\n",
    "# --- 3. Results ---\n",
    "avg_reorder = (total_reorder_time / MAX_LEN) * 1000\n",
    "print(f\"\\n--- RTX 5090 Simulation Results ---\")\n",
    "print(f\"Total Cache Size:      {size_gb:.2f} GB\")\n",
    "print(f\"Avg Cache Reorder Time: {avg_reorder:.2f} ms per step\")\n",
    "print(f\"Total Time (50 steps):  {total_benchmark:.2f} s\")\n",
    "\n",
    "# Context for latency\n",
    "print(f\"\\n--- Latency Context ---\")\n",
    "print(f\"Typical Llama 3B Compute Time (per step): ~50.00 ms\")\n",
    "print(f\"Your Cache Overhead: {avg_reorder:.2f} ms\")\n",
    "print(f\"Overhead Percentage: {(avg_reorder / 50.0) * 100:.1f}%\")\n",
    "\n",
    "if avg_reorder < 5:\n",
    "    print(\"\\n✅ VERDICT: EXTREMELY FAST. The cache reordering is negligible.\")\n",
    "    print(\"   You can run 2000 beams without complex optimization.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ VERDICT: NOTICEABLE. You might want to look into PagedAttention.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta-llama/Llama-3.2-3B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTS ---\n",
      "Type of past_key_values: <class 'transformers.cache_utils.DynamicCache'>\n",
      "✅ contains 'reorder_cache' method (It is a Dynamic/Sink/Static Cache object)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Replace with your local path or model ID\n",
    "model_id = \"meta-llama/Llama-3.2-3B\" \n",
    "\n",
    "print(f\"Loading {model_id}...\")\n",
    "\n",
    "# Load model - mimicking your likely production setup\n",
    "# Note: behavior can change if you use attn_implementation=\"eager\" vs \"sdpa\" vs \"flash_attention_2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Prepare dummy input\n",
    "input_text = \"Testing the cache type\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Run forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, use_cache=True)\n",
    "\n",
    "# INSPECTION\n",
    "cache = outputs.past_key_values\n",
    "\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(f\"Type of past_key_values: {type(cache)}\")\n",
    "\n",
    "if hasattr(cache, \"reorder_cache\"):\n",
    "    print(\"✅ contains 'reorder_cache' method (It is a Dynamic/Sink/Static Cache object)\")\n",
    "else:\n",
    "    print(\"❌ NO 'reorder_cache' method (It is likely a Tuple)\")\n",
    "\n",
    "# If it's a tuple, let's print the structure\n",
    "if isinstance(cache, tuple):\n",
    "    print(f\"Tuple structure: {len(cache)} layers\")\n",
    "    if len(cache) > 0:\n",
    "        print(f\"Layer 0 type: {type(cache[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
