outputDir: /data2/brain2text/b2t_combined/outputs/
datasetPath:
- /data2/brain2text/b2t_25/brain2text25_with_fa
- /data2/brain2text/b2t_24/brain2text24_with_fa
device: cuda:1
modelName: tm_transformer_combined_unidir_chunked
modelType: transformer
optimizer: AdamW
learning_rate: 0.001
beta1: 0.9
beta2: 0.999
eps: 1.0e-08
learning_scheduler: cosine
n_epochs: 951
milestones:
- 400
gamma: None
learning_rate_min: 0.0001
learning_rate_decay_steps: 120000
learning_rate_warmup_steps: 1000
batchSize: 64
nClasses: 40
seed: 0
load_pretrained_model: ''
grad_norm_clip_value: 10
num_masks: 20
max_mask_pct: 0.075
whiteNoiseSD: 0.2
constantOffsetSD: 0.05
gaussianSmoothWidth: 2.0
smooth_kernel_size: 20
random_cut: 0
use_amp: true
l2_decay: 1.0e-05
input_dropout: 0.2
dropout: 0.35
model:
  gru:
    name: GRU
    year: '2024'
    nInputFeatures: 256
    nClasses: 40
    nUnits: 1024
    nLayers: 5
    bidirectional: true
    strideLen: 4
    kernelLen: 4
    nDays: 24
  transformer:
    name: Transformer
    features_list:
    - 512
    - 256
    samples_per_patch: 4
    d_model: 512
    n_heads: 8
    mlp_dim_ratio: 4
    dim_head: 64
    depth: 7
    bidirectional: false
    chunked_attention:
      enable_dynamic: true
      chunkwise_prob: 0.6
      chunk_size_min: 8
      chunk_size_max: 64
      context_chunks_min: 1
      context_chunks_max: 4
      seed: 1
      eval:
        chunk_size: null
        context_chunks: null
wandb:
  project: nejm-brain-to-text
  entity: lionelhu926-ucla



