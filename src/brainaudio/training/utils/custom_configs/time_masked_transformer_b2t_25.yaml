outputDir: /data2/brain2text/b2t_25/outputs/
datasetPath: /data2/brain2text/b2t_25/brain2text25_log.pkl
device: cuda:1
modelName: tm_transformer_b2t_25_with_24_hp
modelType: transformer
optimizer: AdamW
learning_rate: 0.001
n_epochs: 951
batchSize: 64
nClasses: 40
beta1: 0.9
beta2: 0.999
learning_scheduler: multistep
milestones:
- 571
gamma: 0.1
learning_rate_min: 0.0001
learning_rate_decay_steps: 120000
learning_rate_warmup_steps: 1000
seed: 0
load_pretrained_model: ''
grad_norm_clip_value: -1
num_masks: 20
max_mask_pct: 0.075
whiteNoiseSD: 0.2
constantOffsetSD: 0.05
gaussianSmoothWidth: 2.0
smooth_kernel_size: 20
random_cut: 0
l2_decay: 1.0e-5
input_dropout: 0.2
dropout: 0.35
use_amp: True
eps: 1.0e-8
model:
  gru:
    name: GRU
    nInputFeatures: 512
    nClasses: 40
    nUnits: 768
    nLayers: 5
    bidirectional: false
    strideLen: 4
    kernelLen: 14
    nDays: 45
  transformer:
    name: Transformer
    patch_size:
    - 4
    - 512
    d_model: 384
    n_heads: 6
    mlp_dim_ratio: 4
    dim_head: 64
    depth: 5
wandb:
  project: "nejm-brain-to-text" 
  entity: "lionelhu926-ucla"