{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100 grammar test pairs...\n",
      "Example Pair: Correct='The cat runs away.' vs Wrong='The cat run away.'\n",
      "\n",
      "--- Loading SmolLM2-135M ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 100 trials on SmolLM2-135M...\n",
      "Finished SmolLM2-135M. Cleaning up GPU...\n",
      "--- Loading RWKV7-0.1B ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/layers/rwkv7.py:142: UserWarning: According to Bo, you are using a potentially buggy FLA implementation of RWKV. If you plan to report any numbers based on this implementation, we strongly recommend cross-checking with the official repo: https://github.com/BlinkDL/RWKV-LM. Bo may disagree with results reported from this version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 100 trials on RWKV7-0.1B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/ops/rwkv7/fused_recurrent.py:301: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (5) < num_heads (12). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "  warnings.warn(\n",
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/ops/rwkv7/fused_recurrent.py:301: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (5) < num_heads (12). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "  warnings.warn(\n",
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/ops/rwkv7/fused_recurrent.py:301: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (4) < num_heads (12). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "  warnings.warn(\n",
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/ops/rwkv7/fused_recurrent.py:301: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (4) < num_heads (12). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "  warnings.warn(\n",
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/ops/rwkv7/fused_recurrent.py:301: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (5) < num_heads (12). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "  warnings.warn(\n",
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/ops/rwkv7/fused_recurrent.py:301: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (6) < num_heads (12). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished RWKV7-0.1B. Cleaning up GPU...\n",
      "\n",
      "==============================\n",
      "FINAL SCOREBOARD (Out of 100)\n",
      "==============================\n",
      "SmolLM2-135M: 94 / 100\n",
      "RWKV7-0.1B: 95 / 100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MODELS = [\n",
    "    {\n",
    "        \"name\": \"SmolLM2-135M\",\n",
    "        \"repo\": \"HuggingFaceTB/SmolLM2-135M\",\n",
    "        \"trust_remote_code\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RWKV7-0.1B\",\n",
    "        \"repo\": \"fla-hub/rwkv7-0.1B-g1\", \n",
    "        \"trust_remote_code\": True # Required for custom RWKV7 architecture\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- DATASET GENERATOR (100 Trials) ---\n",
    "def generate_grammar_pairs(num_pairs=100):\n",
    "    \"\"\"\n",
    "    Generates synthetic Subject-Verb agreement tests.\n",
    "    Correct: \"The cat runs.\"\n",
    "    Wrong:   \"The cat run.\"\n",
    "    \"\"\"\n",
    "    singular_subjects = [\"The cat\", \"A dog\", \"My friend\", \"The car\", \"He\", \"She\", \"The bird\", \"A computer\", \"The sun\", \"That man\"]\n",
    "    plural_subjects = [\"The cats\", \"Dogs\", \"My friends\", \"The cars\", \"They\", \"We\", \"The birds\", \"Computers\", \"The stars\", \"Those men\"]\n",
    "    \n",
    "    singular_verbs = [\"runs\", \"eats\", \"jumps\", \"sleeps\", \"works\", \"shines\", \"moves\", \"flies\", \"falls\", \"plays\"]\n",
    "    plural_verbs =   [\"run\", \"eat\", \"jump\", \"sleep\", \"work\", \"shine\", \"move\", \"fly\", \"fall\", \"play\"]\n",
    "    \n",
    "    objects = [\"fast\", \"food\", \"high\", \"well\", \"hard\", \"brightly\", \"quickly\", \"away\", \"down\", \"games\"]\n",
    "\n",
    "    pairs = []\n",
    "    for _ in range(num_pairs):\n",
    "        # 50% chance of generating a Singular Subject case\n",
    "        if random.random() > 0.5:\n",
    "            subj_idx = random.randint(0, len(singular_subjects)-1)\n",
    "            verb_idx = random.randint(0, len(singular_verbs)-1)\n",
    "            obj_idx = random.randint(0, len(objects)-1)\n",
    "            \n",
    "            good = f\"{singular_subjects[subj_idx]} {singular_verbs[verb_idx]} {objects[obj_idx]}.\"\n",
    "            bad =  f\"{singular_subjects[subj_idx]} {plural_verbs[verb_idx]} {objects[obj_idx]}.\"\n",
    "        else:\n",
    "            # Plural Subject case\n",
    "            subj_idx = random.randint(0, len(plural_subjects)-1)\n",
    "            verb_idx = random.randint(0, len(plural_verbs)-1)\n",
    "            obj_idx = random.randint(0, len(objects)-1)\n",
    "            \n",
    "            good = f\"{plural_subjects[subj_idx]} {plural_verbs[verb_idx]} {objects[obj_idx]}.\"\n",
    "            bad =  f\"{plural_subjects[subj_idx]} {singular_verbs[verb_idx]} {objects[obj_idx]}.\"\n",
    "            \n",
    "        pairs.append((good, bad))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# --- SCORING ENGINE ---\n",
    "def get_log_prob(model, tokenizer, sentence):\n",
    "    \"\"\"\n",
    "    Computes the total log probability of a sentence.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "        \n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Shift logits and labels for next-token prediction\n",
    "    # logits[0, :-1] predicts input_ids[0, 1:]\n",
    "    shift_logits = logits[0, :-1, :]\n",
    "    shift_labels = input_ids[0, 1:]\n",
    "    \n",
    "    # Calculate Cross Entropy (Loss = -Avg Log Prob)\n",
    "    # reduction='sum' gives total negative log prob\n",
    "    loss = F.cross_entropy(shift_logits, shift_labels, reduction='sum')\n",
    "    \n",
    "    # Return Log Prob (negative of loss)\n",
    "    return -loss.item()\n",
    "\n",
    "# --- MAIN BATTLE LOOP ---\n",
    "def run_battle():\n",
    "    # 1. Generate Data\n",
    "    print(f\"Generating 100 grammar test pairs...\")\n",
    "    test_data = generate_grammar_pairs(100)\n",
    "    print(f\"Example Pair: Correct='{test_data[0][0]}' vs Wrong='{test_data[0][1]}'\\n\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 2. Test Models\n",
    "    for config in MODELS:\n",
    "        name = config['name']\n",
    "        print(f\"--- Loading {name} ---\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(config['repo'], trust_remote_code=config['trust_remote_code'])\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                config['repo'], \n",
    "                trust_remote_code=config['trust_remote_code'],\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "            )\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "            \n",
    "            print(f\"Running 100 trials on {name}...\")\n",
    "            correct_count = 0\n",
    "            \n",
    "            for good, bad in test_data:\n",
    "                score_good = get_log_prob(model, tokenizer, good)\n",
    "                score_bad = get_log_prob(model, tokenizer, bad)\n",
    "                \n",
    "                if score_good > score_bad:\n",
    "                    correct_count += 1\n",
    "            \n",
    "            results[name] = correct_count\n",
    "            print(f\"Finished {name}. Cleaning up GPU...\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del model\n",
    "            del tokenizer\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"FAILED to load or run {name}: {e}\")\n",
    "            results[name] = \"Error\"\n",
    "\n",
    "    # --- FINAL SCOREBOARD ---\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(f\"FINAL SCOREBOARD (Out of 100)\")\n",
    "    print(f\"{'='*30}\")\n",
    "    for name, score in results.items():\n",
    "        print(f\"{name}: {score} / 100\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_battle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmarking SmolLM2-135M ---\n",
      "Warming up...\n",
      "Running 100 iterations...\n",
      "Results for SmolLM2-135M:\n",
      "  Avg Time: 9.9147 ms\n",
      "  Std Dev:  0.0210 ms\n",
      "  Throughput: 206563.01 tokens/sec\n",
      "\n",
      "--- Benchmarking RWKV7-0.1B ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/layers/rwkv7.py:142: UserWarning: According to Bo, you are using a potentially buggy FLA implementation of RWKV. If you plan to report any numbers based on this implementation, we strongly recommend cross-checking with the official repo: https://github.com/BlinkDL/RWKV-LM. Bo may disagree with results reported from this version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n",
      "Running 100 iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/ops/rwkv7/fused_recurrent.py:301: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (4) < num_heads (12). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for RWKV7-0.1B:\n",
      "  Avg Time: 7.8858 ms\n",
      "  Std Dev:  0.3649 ms\n",
      "  Throughput: 259708.59 tokens/sec\n",
      "\n",
      "==============================\n",
      "FINAL COMPARISON (Lower is Better)\n",
      "==============================\n",
      "SmolLM2-135M: 9.91 ms (+0.0%)\n",
      "RWKV7-0.1B: 7.89 ms (-20.5%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BATCH_SIZE = 512\n",
    "SEQ_LEN = 4      # Simulating a 4-token word (e.g., \"unbelievable\")\n",
    "NUM_ITERS = 100  # Number of trials for averaging\n",
    "WARMUP = 10      # Warmup steps to settle GPU clocks\n",
    "\n",
    "# Define Models\n",
    "MODELS = {\n",
    "    \"SmolLM2-135M\": \"HuggingFaceTB/SmolLM2-135M\",\n",
    "    \"RWKV7-0.1B\": \"fla-hub/rwkv7-0.1B-g1\"\n",
    "}\n",
    "\n",
    "def benchmark_model(name, repo_id):\n",
    "    print(f\"\\n--- Benchmarking {name} ---\")\n",
    "    try:\n",
    "        # Load Model\n",
    "        # trust_remote_code=True is essential for RWKV7 / FLA\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            repo_id, \n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16\n",
    "        ).cuda()\n",
    "        model.eval()\n",
    "\n",
    "        # Generate Random Inputs\n",
    "        # Shape: [Batch, Seq_Len]\n",
    "        input_ids = torch.randint(0, 1000, (BATCH_SIZE, SEQ_LEN)).cuda()\n",
    "\n",
    "        # Warmup (Get GPU clocks up, compile kernels)\n",
    "        print(\"Warming up...\")\n",
    "        for _ in range(WARMUP):\n",
    "            with torch.no_grad():\n",
    "                _ = model(input_ids)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Benchmark Loop\n",
    "        print(f\"Running {NUM_ITERS} iterations...\")\n",
    "        times = []\n",
    "        \n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(NUM_ITERS):\n",
    "                start_event.record()\n",
    "                \n",
    "                # The Critical Operation: Forward Pass\n",
    "                # For RWKV, this runs in \"Parallel/Flash\" mode for the chunk\n",
    "                _ = model(input_ids)\n",
    "                \n",
    "                end_event.record()\n",
    "                torch.cuda.synchronize() # Wait for GPU to finish\n",
    "                times.append(start_event.elapsed_time(end_event)) # Returns ms\n",
    "\n",
    "        avg_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        \n",
    "        print(f\"Results for {name}:\")\n",
    "        print(f\"  Avg Time: {avg_time:.4f} ms\")\n",
    "        print(f\"  Std Dev:  {std_time:.4f} ms\")\n",
    "        print(f\"  Throughput: {(BATCH_SIZE * SEQ_LEN) / (avg_time / 1000):.2f} tokens/sec\")\n",
    "\n",
    "        # Cleanup to free VRAM for next model\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return avg_time\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to benchmark {name}: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "# --- MAIN ---\n",
    "if __name__ == \"__main__\":\n",
    "    results = {}\n",
    "    for name, repo in MODELS.items():\n",
    "        results[name] = benchmark_model(name, repo)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"FINAL COMPARISON (Lower is Better)\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    baseline = results[\"SmolLM2-135M\"]\n",
    "    for name, t in results.items():\n",
    "        diff = ((t - baseline) / baseline) * 100\n",
    "        print(f\"{name}: {t:.2f} ms ({diff:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "RWKV-7 Memory Profiler\n",
      "Config: Batch=2000 | Seq=4 | Dtype=torch.float16\n",
      "========================================\n",
      "\n",
      "1. Empty GPU:         0.0 MB\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/layers/rwkv7.py:142: UserWarning: According to Bo, you are using a potentially buggy FLA implementation of RWKV. If you plan to report any numbers based on this implementation, we strongly recommend cross-checking with the official repo: https://github.com/BlinkDL/RWKV-LM. Bo may disagree with results reported from this version.\n",
      "  warnings.warn(\n",
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/ops/rwkv7/fused_recurrent.py:301: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (1) < num_heads (12). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "  warnings.warn(\n",
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/ops/rwkv7/fused_recurrent.py:301: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (1) < num_heads (12). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "  warnings.warn(\n",
      "/home/ebrahim/brainaudio/.venv/lib/python3.12/site-packages/fla/ops/rwkv7/fused_recurrent.py:301: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (1) < num_heads (12). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Model Weights:     379.5 MB (Static Cost)\n",
      "\n",
      "Allocating Giant State for 2000 beams...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Cache' object has no attribute 'repeat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     57\u001b[39m     seed_state = out.past_key_values \u001b[38;5;66;03m# Shape: [Layers, 1, ...]\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Expand to 2000 beams\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# RWKV states are Tensors, so we repeat them.\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# We detach to ensure it's treated as leaf storage, not part of a graph\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m giant_state = \u001b[43mseed_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepeat\u001b[49m(\u001b[32m1\u001b[39m, BATCH_SIZE, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m).detach().clone()\n\u001b[32m     64\u001b[39m torch.cuda.synchronize()\n\u001b[32m     65\u001b[39m curr, peak = get_mem_mb()\n",
      "\u001b[31mAttributeError\u001b[39m: 'Cache' object has no attribute 'repeat'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MODEL_ID = \"fla-hub/rwkv7-0.1B-g1\"\n",
    "BATCH_SIZE = 2000  # The massive number of beams\n",
    "SEQ_LEN = 4        # The chunk size (e.g., scoring a 4-token word)\n",
    "DTYPE = torch.float16\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "def get_mem_mb():\n",
    "    \"\"\"Returns current and peak memory usage in MB\"\"\"\n",
    "    current = torch.cuda.memory_allocated() / 1024**2\n",
    "    peak = torch.cuda.max_memory_allocated() / 1024**2\n",
    "    return current, peak\n",
    "\n",
    "def reset_peak():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"RWKV-7 Memory Profiler\")\n",
    "print(f\"Config: Batch={BATCH_SIZE} | Seq={SEQ_LEN} | Dtype={DTYPE}\")\n",
    "print(f\"{'='*40}\\n\")\n",
    "\n",
    "# 1. BASELINE (Empty GPU)\n",
    "torch.cuda.empty_cache()\n",
    "reset_peak()\n",
    "curr, peak = get_mem_mb()\n",
    "print(f\"1. Empty GPU:         {curr:.1f} MB\")\n",
    "\n",
    "# 2. LOAD MODEL (Weights)\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=DTYPE\n",
    ").to(DEVICE)\n",
    "\n",
    "# Run one dummy tiny step to initialize internal buffers\n",
    "model(torch.tensor([[0]], device=DEVICE)) \n",
    "torch.cuda.synchronize()\n",
    "reset_peak()\n",
    "\n",
    "curr, peak = get_mem_mb()\n",
    "model_size = curr\n",
    "print(f\"2. Model Weights:     {curr:.1f} MB (Static Cost)\")\n",
    "\n",
    "# 3. CREATE GIANT STATE (The 'Cache')\n",
    "# We simulate your exact setup: 2000 beams initialized from <BOS>\n",
    "print(\"\\nAllocating Giant State for 2000 beams...\")\n",
    "\n",
    "# Get shape from a single pass\n",
    "with torch.no_grad():\n",
    "    out = model(torch.tensor([[tokenizer.bos_token_id]], device=DEVICE), use_cache=True)\n",
    "    seed_state = out.past_key_values # Shape: [Layers, 1, ...]\n",
    "\n",
    "# Expand to 2000 beams\n",
    "# RWKV states are Tensors, so we repeat them.\n",
    "# We detach to ensure it's treated as leaf storage, not part of a graph\n",
    "giant_state = seed_state.repeat(1, BATCH_SIZE, 1, 1).detach().clone()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "curr, peak = get_mem_mb()\n",
    "state_size = curr - model_size\n",
    "print(f\"3. State Cache:       {state_size:.1f} MB (Persistent Cost)\")\n",
    "print(f\"   (This stays CONSTANT even if sequence length increases!)\")\n",
    "\n",
    "# 4. FORWARD PASS (Chunked Mode)\n",
    "# Simulating scoring a 4-token word for 2000 beams\n",
    "print(f\"\\nRunning Forward Pass (Batch={BATCH_SIZE}, Seq={SEQ_LEN})...\")\n",
    "input_ids = torch.randint(0, 1000, (BATCH_SIZE, SEQ_LEN), device=DEVICE)\n",
    "\n",
    "reset_peak() # Reset peak to capture ONLY this forward pass spike\n",
    "\n",
    "with torch.no_grad():\n",
    "    # We pass the giant state as 'past_key_values'\n",
    "    outputs = model(input_ids, past_key_values=giant_state, use_cache=True)\n",
    "    \n",
    "    # Force realization of logits and new states\n",
    "    logits = outputs.logits\n",
    "    new_states = outputs.past_key_values\n",
    "    \n",
    "torch.cuda.synchronize()\n",
    "_, run_peak = get_mem_mb()\n",
    "activation_cost = run_peak - (model_size + state_size)\n",
    "\n",
    "print(f\"4. Forward Spike:     {activation_cost:.1f} MB (Temporary Cost)\")\n",
    "\n",
    "# --- SUMMARY ---\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"FINAL REPORT for Batch={BATCH_SIZE}\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Model Weights:      {model_size:>7.1f} MB\")\n",
    "print(f\"KV/State Cache:     {state_size:>7.1f} MB\")\n",
    "print(f\"Activation Spike:   {activation_cost:>7.1f} MB\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"TOTAL VRAM REQUIRED: {run_peak:>7.1f} MB\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# Warning about Logits\n",
    "vocab_size = model.config.vocab_size\n",
    "logit_size_mb = (BATCH_SIZE * SEQ_LEN * vocab_size * 2) / 1024**2\n",
    "print(f\"\\nNOTE: The 'Activation Spike' is dominated by the Logits tensor.\")\n",
    "print(f\"Logits Shape: [{BATCH_SIZE}, {SEQ_LEN}, {vocab_size}] (FP16)\")\n",
    "print(f\"Theoretical Logit Size: ~{logit_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
